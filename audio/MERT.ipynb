{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONl9oj+O4Hlx+INLShMmFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/audio/MERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install -U yt-dlp"
      ],
      "metadata": {
        "id": "1HF9Htnua_Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPtZ7n-Fa06Z"
      },
      "outputs": [],
      "source": [
        "!yt-dlp -x --audio-format wav \"https://youtu.be/V-gxqhWEbxI\" -o \"%(title)s.%(ext)s\"  # vaundy\n",
        "!yt-dlp -x --audio-format wav \"https://youtu.be/6YZlFdTIdzM\" -o \"%(title)s.%(ext)s\"  # one ok rock\n",
        "!yt-dlp --audio-format wav \"https://youtu.be/hN5MBlGv2Ac\" -o \"%(title)s.%(ext)s\" # official\n",
        "!yt-dlp --audio-format wav \"https://youtu.be/oLrp9uTa9gw\" -o \"%(title)s.%(ext)s\"  # official 2\n",
        "!yt-dlp --audio-format wav \"https://youtu.be/lD-GY7WiTd4\" -o \"%(title)s.%(ext)s\"  # twice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = 30\n",
        "end = 60\n",
        "!ffmpeg -i \"/content/そんなbitterな話 ⧸ Vaundy：MUSIC VIDEO.wav\" -ss $start -t $end /content/out_vaundy.wav\n",
        "!ffmpeg -i \"/content/ONE OK ROCK - Clock Strikes [Official Music Video].wav\" -ss $start -t $end /content/out_oneok.wav\n",
        "!ffmpeg -i \"/content/Official髭男dism - Subtitle [Official Video].webm\" -ss $start -t $end /content/out_official.wav\n",
        "!ffmpeg -i \"/content/Official髭男dism - TATTOO [Official Video].webm\" -ss $start -t $end /content/out_official2.wav\n",
        "!ffmpeg -i \"/content/Bouquet.webm\" -ss $start -t $end /content/out_twice.wav"
      ],
      "metadata": {
        "id": "beSP2Co9j78U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "qGsYc1C4cSjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets\n",
        "!pip install nnAudio"
      ],
      "metadata": {
        "id": "cf3Y83vwcTrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "b-CIsj1wrTHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio.transforms as T\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# loading our model weights\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(device)\n",
        "# loading the corresponding preprocessor config\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "A1xhidtgccAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo inference"
      ],
      "metadata": {
        "id": "pTE9SkizlcKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "dataset = dataset.sort(\"id\")\n",
        "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "\n",
        "resample_rate = processor.sampling_rate\n",
        "# make sure the sample_rate aligned\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
        "    resampler = T.Resample(sampling_rate, resample_rate, dtype=torch.float64)\n",
        "else:\n",
        "    resampler = None\n",
        "\n",
        "# audio file is decoded on the fly\n",
        "if resampler is None:\n",
        "    input_audio = dataset[0][\"audio\"][\"array\"]\n",
        "else:\n",
        "  input_audio = resampler(torch.from_numpy(dataset[0][\"audio\"][\"array\"]))"
      ],
      "metadata": {
        "id": "QLfvozqectqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "ptsxMvRMrjiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the output shape, there are 13 layers of representation\n",
        "# each layer performs differently in different downstream tasks, you should choose empirically\n",
        "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
        "\n",
        "# for utterance level classification tasks, you can simply reduce the representation in time\n",
        "time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
        "print(time_reduced_hidden_states.shape) # [13, 768]\n",
        "\n",
        "# you can even use a learnable weighted average representation\n",
        "aggregator = nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1)\n",
        "weighted_avg_hidden_states = aggregator(time_reduced_hidden_states.unsqueeze(0).cpu()).squeeze()\n",
        "print(weighted_avg_hidden_states.shape) # [768]"
      ],
      "metadata": {
        "id": "JqqB_Dv6rloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Music Analysis"
      ],
      "metadata": {
        "id": "BKQBJVtyldrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "def calc_embedding_for_music(input_audio_path):\n",
        "  y, sr = librosa.load(input_audio_path)\n",
        "\n",
        "  resample_rate = processor.sampling_rate\n",
        "  # make sure the sample_rate aligned\n",
        "  if resample_rate != sampling_rate:\n",
        "      print(f'setting rate from {sr} to {resample_rate}')\n",
        "      resampler = T.Resample(sr, resample_rate, dtype=torch.float64)\n",
        "  else:\n",
        "      resampler = None\n",
        "\n",
        "  # audio file is decoded on the fly\n",
        "  if resampler is None:\n",
        "      input_audio = y\n",
        "  else:\n",
        "    input_audio = resampler(torch.Tensor(y).to(torch.float64))\n",
        "  \n",
        "  inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "  # need to finetuning\n",
        "  # aggregator = nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1)\n",
        "  # weighted_avg_hidden_states = aggregator(time_reduced_hidden_states.unsqueeze(0).cpu()).squeeze()\n",
        "  # print(weighted_avg_hidden_states.shape) # [768]\n",
        "  # return weighted_avg_hidden_states\n",
        "\n",
        "  all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "  # print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
        "  time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
        "  # print(time_reduced_hidden_states.shape) # [13, 768]\n",
        "  return time_reduced_hidden_states.view(-1).cpu()"
      ],
      "metadata": {
        "id": "jlKZF0HllwdM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot \n",
        "from numpy.linalg import norm \n",
        "def calc_cos_sim(a, b):\n",
        "  cos_sim = dot(a, b) / (norm(a) * norm(b)) \n",
        "  return cos_sim"
      ],
      "metadata": {
        "id": "hU_eReV0meS8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_vaundy = calc_embedding_for_music(\"/content/out_vaundy.wav\").detach().numpy()\n",
        "embed_oneok = calc_embedding_for_music(\"/content/out_oneok.wav\").detach().numpy()\n",
        "embed_official = calc_embedding_for_music(\"/content/out_official.wav\").detach().numpy()\n",
        "embed_official2 = calc_embedding_for_music(\"/content/out_official2.wav\").detach().numpy()\n",
        "embed_twice = calc_embedding_for_music(\"/content/out_twice.wav\").detach().numpy()"
      ],
      "metadata": {
        "id": "wyc09CS5mHYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_vaundy.shape)"
      ],
      "metadata": {
        "id": "JGROV1Z2omP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"official vs vaundy: \", calc_cos_sim(embed_official, embed_vaundy))\n",
        "print(\"official vs oneok: \", calc_cos_sim(embed_official, embed_oneok))\n",
        "print(\"official vs official2: \", calc_cos_sim(embed_official, embed_official2))\n",
        "print(\"official vs twice: \", calc_cos_sim(embed_official, embed_twice))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq1vBVXImlYR",
        "outputId": "ce23771f-2d54-45a3-b05d-5cd5e395967c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "official vs vaundy:  0.9231535\n",
            "official vs oneok:  0.9128771\n",
            "official vs official2:  0.9318339\n",
            "official vs twice:  0.907945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Inference"
      ],
      "metadata": {
        "id": "gSQP-zSSoEjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load librosa\n",
        "import librosa\n",
        "y, sr = librosa.load(\"/content/out_oneok.wav\")\n",
        "print(type(y), sr)\n",
        "\n",
        "resample_rate = processor.sampling_rate\n",
        "# make sure the sample_rate aligned\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'setting rate from {sr} to {resample_rate}')\n",
        "    resampler = T.Resample(sr, resample_rate, dtype=torch.float64)\n",
        "else:\n",
        "    resampler = None\n",
        "\n",
        "# audio file is decoded on the fly\n",
        "if resampler is None:\n",
        "    input_audio = y\n",
        "else:\n",
        "  input_audio = resampler(torch.Tensor(y).to(torch.float64))"
      ],
      "metadata": {
        "id": "pwjLchE9hDx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "CaWwUTvFfcf_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the output shape, there are 13 layers of representation\n",
        "# each layer performs differently in different downstream tasks, you should choose empirically\n",
        "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
        "\n",
        "# for utterance level classification tasks, you can simply reduce the representation in time\n",
        "time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
        "print(time_reduced_hidden_states.shape) # [13, 768]\n",
        "\n",
        "# you can even use a learnable weighted average representation\n",
        "aggregator = nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1)\n",
        "weighted_avg_hidden_states = aggregator(time_reduced_hidden_states.unsqueeze(0).cpu()).squeeze()\n",
        "print(weighted_avg_hidden_states.shape) # [768]"
      ],
      "metadata": {
        "id": "nXW57sTSfoWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}