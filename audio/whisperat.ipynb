{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNH9MYbLKIMhQXhMjY2+C0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/audio/whisperat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper-at"
      ],
      "metadata": {
        "id": "RsWHJsYVQ-Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download a sample audio\n",
        "!pip -q install wget\n",
        "import wget,IPython\n",
        "wget.download('https://www.dropbox.com/s/7eznyazmc1pmw9h/case_closed.wav?dl=1', '/content/sample_audio.flac')\n",
        "IPython.display.Audio('/content/sample_audio.flac')"
      ],
      "metadata": {
        "id": "bg5Auo5IRCJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note this is whisper\"_\"at not whisper-at\n",
        "import whisper_at as whisper\n",
        "\n",
        "# the only new thing in whisper-at\n",
        "# specify the temporal resolution for audio tagging, 10 means Whisper-AT predict audio event every 10 seconds (hop and window=10s).\n",
        "audio_tagging_time_resolution = 10\n",
        "\n",
        "model = whisper.load_model(\"large-v1\")\n",
        "# for large, medium, small models, we provide low-dim proj AT models to save compute.\n",
        "# model = whisper.load_model(\"large-v1\", at_low_compute=Ture)\n",
        "result = model.transcribe(\"/content/sample_audio.flac\", at_time_res=audio_tagging_time_resolution)\n",
        "for segment in result['segments']:\n",
        "  print(segment['start'], 's-', segment['end'], 's', segment['text'])"
      ],
      "metadata": {
        "id": "xJAR9RMcRJJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation task is also supported\n",
        "result = model.transcribe(\"/content/sample_audio.flac\", task='translate', at_time_res=audio_tagging_time_resolution)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "xMshbP7wRUFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the Audio Tagging"
      ],
      "metadata": {
        "id": "q5x-e-z5WPbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "audio, sr = torchaudio.load('/content/sample_audio.flac')\n",
        "audio_len = audio.shape[1] / sr\n",
        "audio_tag_result = whisper.parse_at_label(result, language='follow_asr', top_k=5, p_threshold=-2, include_class_list=list(range(527)))\n",
        "for segment in audio_tag_result:\n",
        "  print(\"time: \", segment['time'], 'Audio Tag Dict: ', segment['audio tags'])"
      ],
      "metadata": {
        "id": "MdWhb77aWRe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_tagging_time_resolution = 2\n",
        "result = model.transcribe(\"/content/sample_audio.flac\", at_time_res=audio_tagging_time_resolution)\n",
        "print('Audio length is {:.2f}, at time resolution is {:.1f}, Whisper-AT output in shape'.format(audio_len, audio_tagging_time_resolution), result['audio_tag'].shape)\n",
        "audio_tag_result = whisper.parse_at_label(result, language='follow_asr', top_k=5, p_threshold=-2, include_class_list=list(range(527)))\n",
        "for segment in audio_tag_result:\n",
        "  print(segment)\n",
        ""
      ],
      "metadata": {
        "id": "WELinSGYXCSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_tag_result = whisper.parse_at_label(result, language='ja', top_k=5, p_threshold=-2, include_class_list=list(range(527)))\n",
        "for segment in audio_tag_result:\n",
        "  print(segment)"
      ],
      "metadata": {
        "id": "ZzFeuZyeXTln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribe video"
      ],
      "metadata": {
        "id": "Bek1lI5SXoCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "# Replace this URL to play with your own video\n",
        "wget.download('https://www.dropbox.com/s/pzc72c59xtluuc0/case_closed.mp4?dl=1', '/content/sample_video.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Llf1hMXKXpmr",
        "outputId": "7f4deb8e-b090-4655-bb01-239230e5ef65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sample_video.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ffmpeg-python\n",
        "import os,ffmpeg,cv2\n",
        "\n",
        "def dubbing_video(video_path, out_video_path, text_info, font_size=0.5, font_v_pos=0.95, font_color=(0, 0, 255)):\n",
        "    extract_audio(video_path, './temp_audio.wav')\n",
        "\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    # Get video properties\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Create output video writer\n",
        "    output_video = cv2.VideoWriter('./temp_video.mp4', cv2.VideoWriter_fourcc(*\"mp4v\"), fps,\n",
        "                                   (frame_width, frame_height))\n",
        "\n",
        "    # Process each frame of the video\n",
        "    current_frame = 0\n",
        "    while video.isOpened():\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Calculate current time in seconds\n",
        "        current_time = current_frame / fps\n",
        "\n",
        "        # Iterate through text information and add text if within the time interval\n",
        "        for text_start, text_end, text in text_info:\n",
        "            if text_start <= current_time <= text_end:\n",
        "                text_position = (int(frame_width * 0.0), int(frame_height * font_v_pos))\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                font_scale = font_size\n",
        "                font_color = font_color\n",
        "                line_type = 1\n",
        "\n",
        "                cv2.putText(frame, text, text_position, font, font_scale, font_color, line_type, cv2.LINE_AA)\n",
        "\n",
        "        # Write the frame to the output video\n",
        "        output_video.write(frame)\n",
        "        current_frame += 1\n",
        "\n",
        "    # Release video resources\n",
        "    video.release()\n",
        "    output_video.release()\n",
        "\n",
        "    combine_audio_video('./temp_video.mp4', './temp_audio.wav', out_video_path)\n",
        "    os.remove('./temp_video.mp4')\n",
        "    os.remove('./temp_audio.wav')\n",
        "\n",
        "def combine_audio_video(video_path, audio_path, output_path):\n",
        "    video = ffmpeg.input(video_path)\n",
        "    audio = ffmpeg.input(audio_path)\n",
        "    output_file = ffmpeg.output(video, audio, output_path)\n",
        "    output_file.overwrite_output().run()\n",
        "\n",
        "def extract_audio(video_path, output_path):\n",
        "    video = ffmpeg.input(video_path)\n",
        "    audio = video.audio\n",
        "    output_file = ffmpeg.output(audio, output_path)\n",
        "    output_file.overwrite_output().run()\n",
        "\n",
        "extract_audio('/content/sample_video.mp4', '/content/sample_audio_from_video.wav')\n",
        "result = model.transcribe(\"/content/sample_audio_from_video.wav\", at_time_res=audio_tagging_time_resolution)\n",
        "\n",
        "# ASR Output\n",
        "text_segments = result['segments']\n",
        "text_annotation = [(x['start'], x['end'], x['text']) for x in text_segments]\n",
        "\n",
        "# Audio Tagging Output\n",
        "audio_tag_result = whisper.parse_at_label(result, language='follow_asr', top_k=5, p_threshold=-2, include_class_list=list(range(527)))\n",
        "\n",
        "all_seg = []\n",
        "for segment in audio_tag_result:\n",
        "    cur_start = segment['time']['start']\n",
        "    cur_end = segment['time']['end']\n",
        "    cur_tags = segment['audio tags']\n",
        "    cur_tags = [x[0] for x in cur_tags]\n",
        "    cur_tags = '; '.join(cur_tags)\n",
        "    all_seg.append((cur_start, cur_end, cur_tags))\n",
        "\n",
        "dubbing_video('/content/sample_video.mp4', '/content/sample_video_at.mp4', all_seg)\n",
        "dubbing_video('/content/sample_video_at.mp4', '/content/sample_video_at_text.mp4', text_annotation, font_color=(0,255,0), font_v_pos=0.85)\n",
        "os.remove('/content/sample_video.mp4')\n",
        "os.remove('/content/sample_video_at.mp4')\n",
        "\n",
        "mp4 = open('/content/sample_video_at_text.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()"
      ],
      "metadata": {
        "id": "4BUnkTczXycT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "DnIFyiTUYtG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}