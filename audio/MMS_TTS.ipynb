{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGnnbgtIZCx9cTzuZWAf+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/audio/MMS_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "gppeIo6SAt1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL5qEKENAozP"
      },
      "outputs": [],
      "source": [
        "%pwd\n",
        "!git clone https://github.com/jaywalnut310/vits.git\n",
        "!python --version\n",
        "%cd vits/\n",
        "\n",
        "!pip install Cython==0.29.21\n",
        "!pip install librosa==0.8.0\n",
        "!pip install phonemizer==2.2.1\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install Unidecode==1.1.1\n",
        "\n",
        "%cd monotonic_align/\n",
        "%mkdir monotonic_align\n",
        "!python3 setup.py build_ext --inplace\n",
        "%cd ../\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose language and download checkpoints"
      ],
      "metadata": {
        "id": "5zN8Fcb0A2qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/vits/"
      ],
      "metadata": {
        "id": "qMte0JWhClnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "def download(lang, tgt_dir=\"./\"):\n",
        "  lang_fn, lang_dir = os.path.join(tgt_dir, lang+'.tar.gz'), os.path.join(tgt_dir, lang)\n",
        "  cmd = \";\".join([\n",
        "        f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
        "        f\"tar zxvf {lang_fn}\"\n",
        "  ])\n",
        "  print(f\"Download model for language: {lang}\")\n",
        "  subprocess.check_output(cmd, shell=True)\n",
        "  print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
        "  return lang_dir\n",
        "\n",
        "LANG = \"eng\"  # \"jav\", \"eng\"\n",
        "ckpt_dir = download(LANG)"
      ],
      "metadata": {
        "id": "jBRIdCQtA2FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "eAoZ9nx6A_Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import tempfile\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import commons\n",
        "import utils\n",
        "import argparse\n",
        "import subprocess\n",
        "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
        "from models import SynthesizerTrn\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "class TextMapper(object):\n",
        "    def __init__(self, vocab_file):\n",
        "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
        "        self.SPACE_ID = self.symbols.index(\" \")\n",
        "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
        "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
        "\n",
        "    def text_to_sequence(self, text, cleaner_names):\n",
        "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
        "        Args:\n",
        "        text: string to convert to a sequence\n",
        "        cleaner_names: names of the cleaner functions to run the text through\n",
        "        Returns:\n",
        "        List of integers corresponding to the symbols in the text\n",
        "        '''\n",
        "        sequence = []\n",
        "        clean_text = text.strip()\n",
        "        for symbol in clean_text:\n",
        "            symbol_id = self._symbol_to_id[symbol]\n",
        "            sequence += [symbol_id]\n",
        "        return sequence\n",
        "\n",
        "    def uromanize(self, text, uroman_pl):\n",
        "        iso = \"xxx\"\n",
        "        with tempfile.NamedTemporaryFile() as tf, \\\n",
        "             tempfile.NamedTemporaryFile() as tf2:\n",
        "            with open(tf.name, \"w\") as f:\n",
        "                f.write(\"\\n\".join([text]))\n",
        "            cmd = f\"perl \" + uroman_pl\n",
        "            cmd += f\" -l {iso} \"\n",
        "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
        "            os.system(cmd)\n",
        "            outtexts = []\n",
        "            with open(tf2.name) as f:\n",
        "                for line in f:\n",
        "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
        "                    outtexts.append(line)\n",
        "            outtext = outtexts[0]\n",
        "        return outtext\n",
        "\n",
        "    def get_text(self, text, hps):\n",
        "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
        "        if hps.data.add_blank:\n",
        "            text_norm = commons.intersperse(text_norm, 0)\n",
        "        text_norm = torch.LongTensor(text_norm)\n",
        "        return text_norm\n",
        "\n",
        "    def filter_oov(self, text):\n",
        "        val_chars = self._symbol_to_id\n",
        "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
        "        print(f\"text after filtering OOV: {txt_filt}\")\n",
        "        return txt_filt\n",
        "\n",
        "def preprocess_text(txt, text_mapper, hps, uroman_dir=None):\n",
        "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
        "    if is_uroman:\n",
        "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "            if uroman_dir is None:\n",
        "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
        "                print(cmd)\n",
        "                subprocess.check_output(cmd, shell=True)\n",
        "                uroman_dir = tmp_dir\n",
        "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
        "            print(f\"uromanize\")\n",
        "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
        "            print(f\"uroman text: {txt}\")\n",
        "    txt = txt.lower()\n",
        "    txt = text_mapper.filter_oov(txt)\n",
        "    return txt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Run inference with {device}\")\n",
        "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
        "config_file = f\"{ckpt_dir}/config.json\"\n",
        "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
        "hps = utils.get_hparams_from_file(config_file)\n",
        "text_mapper = TextMapper(vocab_file)\n",
        "net_g = SynthesizerTrn(\n",
        "    len(text_mapper.symbols),\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    **hps.model)\n",
        "net_g.to(device)\n",
        "_ = net_g.eval()\n",
        "\n",
        "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
        "print(f\"load {g_pth}\")\n",
        "\n",
        "_ = utils.load_checkpoint(g_pth, net_g, None)"
      ],
      "metadata": {
        "id": "D2s24ZA4BAhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "84mUrFLKBDVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hello, I am greate thanks.\"\n",
        "print(f\"text: {txt}\")\n",
        "txt = preprocess_text(txt, text_mapper, hps)\n",
        "stn_tst = text_mapper.get_text(txt, hps)\n",
        "with torch.no_grad():\n",
        "    x_tst = stn_tst.unsqueeze(0).to(device)\n",
        "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
        "    hyp = net_g.infer(\n",
        "        x_tst, x_tst_lengths, noise_scale=.667,\n",
        "        noise_scale_w=0.8, length_scale=1.0\n",
        "    )[0][0,0].cpu().float().numpy()\n",
        "\n",
        "print(f\"Generated audio\") \n",
        "Audio(hyp, rate=hps.data.sampling_rate)"
      ],
      "metadata": {
        "id": "Bf21g1IkBEMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}