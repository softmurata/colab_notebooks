{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj1WB2sWOQhSQ0ljPtUNKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/audio/riffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkri9Y_qvv6t"
      },
      "outputs": [],
      "source": [
        "# https://zenn.dev/ganariya/articles/google-colaboratory-python39\n",
        "!sudo add-apt-repository -y ppa:deadsnakes/ppa\n",
        "!sudo apt-get -y update\n",
        "!sudo apt-get -y install python3.9\n",
        "!sudo apt-get -y install python3.9-dev\n",
        "!sudo apt-get -y install python3-pip\n",
        "!sudo apt-get -y install python3.9-distutils\n",
        "!python3.9 -m pip install --upgrade setuptools\n",
        "!python3.9 -m pip install --upgrade pip\n",
        "!python3.9 -m pip install --upgrade distlib\n",
        "\n",
        "!sudo update-alternatives --set python /usr/bin/python3.9\n",
        "!sudo ln -sf /usr/bin/python /usr/local/bin/python\n",
        "\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/riffusion/riffusion.git"
      ],
      "metadata": {
        "id": "T_ssR4uaxEze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/riffusion')\n",
        "!python3.9 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "0YcHKGRDxbn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "!python3.9 -m riffusion.cli -h"
      ],
      "metadata": {
        "id": "G4rwV9dY070P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create mel spectrogram image from audio file\n",
        "!python3.9 -m riffusion.cli audio-to-image --audio examples/gamesound.wav --image examples/gamesound_spectgram.png"
      ],
      "metadata": {
        "id": "1lyT4OmRyNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download desired artist from youtube\n",
        "!sudo curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp\n",
        "!sudo chmod a+rx /usr/local/bin/yt-dlp\n",
        "!yt-dlp --version\n",
        "!pip install google-api-python-client"
      ],
      "metadata": {
        "id": "AD14Gtd0Xwy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Youtube Data APi Settings\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n",
        "SEARCH_TEXT = \"one ok rock\" #@param {type: \"string\"}\n",
        "YOUTUBE_DATA_API_KEY = \"AIzaSyC-Oru_2Dx_wWV7v7xMxd89XyuAS_4qaW8\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "Xoe60wCMaeZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "from apiclient.discovery import build\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Èü≥Â£∞„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÅÆCOMMAND\n",
        "# !yt-dlp -x --audio-format mp3 'https://youtu.be/XPT91f5vU-k' -o sample001.mp3\n",
        "\n",
        "# create youtube data api instance\n",
        "youtube = build(\n",
        "    YOUTUBE_API_SERVICE_NAME,\n",
        "    YOUTUBE_API_VERSION,\n",
        "    developerKey=YOUTUBE_DATA_API_KEY\n",
        ")\n",
        "\n",
        "response = youtube.search().list(q=SEARCH_TEXT, part='id,snippet', maxResults=25).execute()\n",
        "\n",
        "# Get channel id\n",
        "channel_result = []\n",
        "\n",
        "for item in response.get('items', []):\n",
        "    if item['id']['kind'] != 'youtube#channel':\n",
        "        continue\n",
        "    channel_result.append(json.dumps(item, indent=2, ensure_ascii=False))\n",
        "\n",
        "channel_id = json.loads(channel_result[0])[\"id\"][\"channelId\"]\n",
        "print('*' * 10)\n",
        "print(channel_id)\n",
        "print('*' * 10)\n",
        "\n",
        "# Get video list\n",
        "response = youtube.search().list(part=\"snippet\", channelId=channel_id, maxResults=20, order=\"date\").execute()\n",
        "\n",
        "video_list = []\n",
        "for item in response.get(\"items\", []):\n",
        "    if item[\"id\"][\"kind\"] != \"youtube#video\":\n",
        "        continue\n",
        "    video_list.append(json.loads(json.dumps(item, indent=2, ensure_ascii=False))[\"id\"][\"videoId\"])\n",
        "\n",
        "print('*' * 10)\n",
        "print(video_list)\n",
        "print(\"*\" * 10)\n",
        "\n",
        "# download mp3 file\n",
        "artistname = SEARCH_TEXT.replace(\" \", \"_\")\n",
        "sample_dir = f\"./musics/{artistname}/\"\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "for idx, videoId in enumerate(video_list):\n",
        "    url = f\"https://youtu.be/{videoId}\"\n",
        "    filename = sample_dir + f\"{idx}\".zfill(3) + \".mp3\"\n",
        "    print(filename)\n",
        "    cmd = f\"yt-dlp -x --audio-format mp3 {url} -o {filename}\"\n",
        "    !{cmd}\n"
      ],
      "metadata": {
        "id": "O9UBmBqoajFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ByMYdd1ugZ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title create riffusion spectrogram\n",
        "# create mel spectrogram image from audio file\n",
        "# !python3.9 -m riffusion.cli audio-to-image --audio examples/gamesound.wav --image examples/gamesound_spectgram.png\n",
        "\n",
        "import glob\n",
        "\n",
        "audio_image_dir = f\"audio_images/{artistname}/\"\n",
        "os.makedirs(audio_image_dir, exist_ok=True)\n",
        "\n",
        "audio_dir = f'musics/{artistname}/'\n",
        "audio_files = sorted(glob.glob(audio_dir + \"*.mp3\"))\n",
        "for audio_file in audio_files:\n",
        "     filename = audio_file.split(\"/\")[-1].split(\".\")[0]\n",
        "     cmd = f\"python3.9 -m riffusion.cli audio-to-image --audio {audio_file} --image {audio_image_dir}/{filename}.png\"\n",
        "     !{cmd}\n",
        "\n"
      ],
      "metadata": {
        "id": "a6Ie0WqEfLnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create 512 *512 image from audio images\n",
        "import cv2\n",
        "\n",
        "for idx in range(20):\n",
        "    image_path = \"./data/special/{}.png\".format(str(idx).zfill(3))\n",
        "    img = cv2.imread(image_path)\n",
        "    div = img.shape[1] // 512\n",
        "    print(div)"
      ],
      "metadata": {
        "id": "rgGX7Jf7M2kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training method\n",
        "# 1. create mel spectrogram from some audio files(20Êûö„Åè„Çâ„ÅÑ)\n",
        "# 2. train with dreambooth with riffusion model(huggingface)\n",
        "# 3. test with riffusion web app "
      ],
      "metadata": {
        "id": "Bl5uUGBK0g9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title riffusion dream booth\n",
        "!pip install diffusers scipy\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.15.0 transformers ftfy bitsandbytes gradio natsort"
      ],
      "metadata": {
        "id": "TOnFH_fLVr1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/riffusion/riffusion.git"
      ],
      "metadata": {
        "id": "EXxZzb5FooGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to HuggingFace ü§ó\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ü§ó Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ],
      "metadata": {
        "id": "cRpkVXonVzNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.15.0 transformers ftfy bitsandbytes gradio natsort"
      ],
      "metadata": {
        "id": "Y9ArPfvD_hmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4.\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgXBMgeIV6EP",
        "outputId": "357ea161-53a9-4adf-e8b2-56fd661a55ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"riffusion/riffusion-model-v1\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"riffusion_weights/special\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9n55hleV8TC",
        "outputId": "8dc13e9c-27ba-4bd6-9855-a559fbad5102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/riffusion_weights/special\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of special pop\",\n",
        "        \"class_prompt\":         \"photo of a pop\",\n",
        "        \"instance_data_dir\":    \"/content/data/special\",\n",
        "        \"class_data_dir\":       \"/content/data/pop\"\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "SuIsezypWO_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# change 512 x 512?\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ],
      "metadata": {
        "id": "Kzs7Rp0cWe6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "iYuWhtg9q88a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training with accelerate\n",
        "# „Åì„Çå„ÅØStable diffusion„ÅÆ„Åü„ÇÅ„Å´‰ΩúÊàê„Åï„Çå„Åüfine tuningÁî®„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÄÅÂà•„ÅÆPipeline„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„Åä„Åù„Çâ„Åètrain_dreambooth.py„ÇíÊõ∏„ÅçÊèõ„Åà„Å™„ÅÑ„Å®„ÅÑ„Åë„Å™„ÅÑ„ÄÇ„ÄÇ„ÄÇ\n",
        "# stable diffusion„Å´„ÅØ„Å™„ÅÑcheckpoint„Å®„Åó„Å¶„ÄÅsafe_checker, feature_extractor„ÅåÂ≠òÂú®„Åô„Çã„ÄÇ\n",
        "# Question: stable diffusion„ÅÆÊñáËÑà„Åßfine tuning„Åó„ÅüÂæå„Å´„Åù„Åì„Åã„Çâtext_encoder, vae, unet„Åø„Åü„ÅÑ„Å™Â≠¶Áøí„Åó„Åü„ÅÆ„Çí„Åù„Åì„Åã„Çâ‰Ωø„Å£„Å¶safe_checker, feature_extractor„ÅÇ„Åü„Çä„ÅØriffusion/riffusion-mode-v1„ÅÆ„Çí‰Ωø„Å£„Å¶Riffusion Pipeline„ÇíÁ´ã„Å°‰∏ä„Åí„Çå„Å∞„ÅÑ„ÅÑÔºü\n",
        "os.chdir(\"/content\")\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=5 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"photo of special pop\" \\\n",
        "  --concepts_list=\"concepts_list.json\""
      ],
      "metadata": {
        "id": "GIo5NaOwWi_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = \"/content/riffusion_weights/special/5/\"             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "g_cuda = None\n",
        "\n",
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KTLPzo3FeEI",
        "outputId": "be204920-5614-4dfe-9a09-169f50505f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.9.0\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": false,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": false,\n",
            "  \"steps_offset\": 0,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  warnings.warn(warning + message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efe6449adf0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of japanese special pop\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "AlAJ0sUuFm8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux"
      ],
      "metadata": {
        "id": "FVlXQixjSwrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==9.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXlgnEPvaiK6",
        "outputId": "653e6d43-396a-4b6e-be06-191b012d9d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow==9.1.0 in /usr/local/lib/python3.8/dist-packages (9.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/riffusion\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "BQ13yafxI59N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Riffusion Inference code\n",
        "from __future__ import annotations\n",
        "from riffusion.util import torch_util\n",
        "# from diffusers.pipeline_utils import DiffusionPipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers.schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "import typing as T\n",
        "import dataclasses\n",
        "import functools\n",
        "import inspect\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from riffusion.datatypes import InferenceInput\n",
        "from riffusion.external.prompt_weighting import get_weighted_text_embeddings\n",
        "\n",
        "class InferRiffusionPipeline(object):\n",
        "    def __init__(\n",
        "        self\n",
        "    ):\n",
        "        super().__init__()\n",
        "    @classmethod\n",
        "    def load_checkpoint(\n",
        "        cls,\n",
        "        checkpoint: str,\n",
        "        use_traced_unet: bool = True,\n",
        "        channels_last: bool = False,\n",
        "        dtype: torch.dtype = torch.float16,\n",
        "        device: str = \"cuda\",\n",
        "        local_files_only: bool = False,\n",
        "        low_cpu_mem_usage: bool = False,\n",
        "        cache_dir: T.Optional[str] = None,\n",
        "    ) -> StableDiffusionPipeline:\n",
        "        \"\"\"\n",
        "        Load the riffusion model pipeline.\n",
        "        Args:\n",
        "            checkpoint: Model checkpoint on disk in diffusers format\n",
        "            use_traced_unet: Whether to use the traced unet for speedups\n",
        "            device: Device to load the model on\n",
        "            channels_last: Whether to use channels_last memory format\n",
        "            local_files_only: Don't download, only use local files\n",
        "            low_cpu_mem_usage: Attempt to use less memory on CPU\n",
        "        \"\"\"\n",
        "        device = torch_util.check_device(device)\n",
        "\n",
        "        if device == \"cpu\" or device.lower().startswith(\"mps\"):\n",
        "            print(f\"WARNING: Falling back to float32 on {device}, float16 is unsupported\")\n",
        "            dtype = torch.float32\n",
        "\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            checkpoint,\n",
        "            revision=\"main\",\n",
        "            torch_dtype=dtype,\n",
        "            # Disable the NSFW filter, causes incorrect false positives\n",
        "            # TODO(hayk): Disable the \"you have passed a non-standard module\" warning from this.\n",
        "            safety_checker=None,\n",
        "            low_cpu_mem_usage=low_cpu_mem_usage,\n",
        "            local_files_only=local_files_only,\n",
        "            cache_dir=cache_dir,\n",
        "        ).to(device)\n",
        "\n",
        "        if channels_last:\n",
        "            pipeline.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "        # Optionally load a traced unet\n",
        "        if checkpoint == \"riffusion/riffusion-model-v1\" and use_traced_unet:\n",
        "            traced_unet = cls.load_traced_unet(\n",
        "                checkpoint=checkpoint,\n",
        "                subfolder=\"unet_traced\",\n",
        "                filename=\"unet_traced.pt\",\n",
        "                in_channels=pipeline.unet.in_channels,\n",
        "                dtype=dtype,\n",
        "                device=device,\n",
        "                local_files_only=local_files_only,\n",
        "                cache_dir=cache_dir,\n",
        "            )\n",
        "\n",
        "            if traced_unet is not None:\n",
        "                pipeline.unet = traced_unet\n",
        "\n",
        "        model = pipeline.to(device)\n",
        "\n",
        "        return model\n",
        "    @staticmethod\n",
        "    def load_traced_unet(\n",
        "        checkpoint: str,\n",
        "        subfolder: str,\n",
        "        filename: str,\n",
        "        in_channels: int,\n",
        "        dtype: torch.dtype,\n",
        "        device: str = \"cuda\",\n",
        "        local_files_only=False,\n",
        "        cache_dir: T.Optional[str] = None,\n",
        "    ) -> T.Optional[torch.nn.Module]:\n",
        "        \"\"\"\n",
        "        Load a traced unet from the huggingface hub. This can improve performance.\n",
        "        \"\"\"\n",
        "        if device == \"cpu\" or device.lower().startswith(\"mps\"):\n",
        "            print(\"WARNING: Traced UNet only available for CUDA, skipping\")\n",
        "            return None\n",
        "\n",
        "        # Download and load the traced unet\n",
        "        unet_file = hf_hub_download(\n",
        "            checkpoint,\n",
        "            subfolder=subfolder,\n",
        "            filename=filename,\n",
        "            local_files_only=local_files_only,\n",
        "            cache_dir=cache_dir,\n",
        "        )\n",
        "        unet_traced = torch.jit.load(unet_file)\n",
        "\n",
        "        # Wrap it in a torch module\n",
        "        class TracedUNet(torch.nn.Module):\n",
        "            @dataclasses.dataclass\n",
        "            class UNet2DConditionOutput:\n",
        "                sample: torch.FloatTensor\n",
        "\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.in_channels = device\n",
        "                self.device = device\n",
        "                self.dtype = dtype\n",
        "\n",
        "            def forward(self, latent_model_input, t, encoder_hidden_states):\n",
        "                sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n",
        "                return self.UNet2DConditionOutput(sample=sample)\n",
        "\n",
        "        return TracedUNet()\n",
        "\n",
        "    @property\n",
        "    def device(self) -> str:\n",
        "        return \"cuda\"\n",
        "\n",
        "    @functools.lru_cache()\n",
        "    def embed_text(self, text, diffusion_pipeline) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Takes in text and turns it into text embeddings.\n",
        "        \"\"\"\n",
        "        text_input = diffusion_pipeline.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            max_length=diffusion_pipeline.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            embed = diffusion_pipeline.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "        return embed\n",
        "\n",
        "    @functools.lru_cache()\n",
        "    def embed_text_weighted(self, text, diffusion_pipeline) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Get text embedding with weights.\n",
        "        \"\"\"\n",
        "        return get_weighted_text_embeddings(\n",
        "            pipe=diffusion_pipeline,\n",
        "            prompt=text,\n",
        "            uncond_prompt=None,\n",
        "            max_embeddings_multiples=3,\n",
        "            no_boseos_middle=False,\n",
        "            skip_parsing=False,\n",
        "            skip_weighting=False,\n",
        "        )[0]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def riffuse(\n",
        "        self,\n",
        "        diffusion_pipeline: StableDiffusionPipeline,\n",
        "        inputs: InferenceInput,\n",
        "        init_image: Image.Image,\n",
        "        mask_image: T.Optional[Image.Image] = None,\n",
        "        use_reweighting: bool = True,\n",
        "    ) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Runs inference using interpolation with both img2img and text conditioning.\n",
        "        Args:\n",
        "            inputs: Parameter dataclass\n",
        "            init_image: Image used for conditioning\n",
        "            mask_image: White pixels in the mask will be replaced by noise and therefore repainted,\n",
        "                        while black pixels will be preserved. It will be converted to a single\n",
        "                        channel (luminance) before use.\n",
        "            use_reweighting: Use prompt reweighting\n",
        "        \"\"\"\n",
        "        alpha = inputs.alpha\n",
        "        start = inputs.start\n",
        "        end = inputs.end\n",
        "\n",
        "        guidance_scale = start.guidance * (1.0 - alpha) + end.guidance * alpha\n",
        "\n",
        "        # TODO(hayk): Always generate the seed on CPU?\n",
        "        if self.device.lower().startswith(\"mps\"):\n",
        "            generator_start = torch.Generator(device=\"cpu\").manual_seed(start.seed)\n",
        "            generator_end = torch.Generator(device=\"cpu\").manual_seed(end.seed)\n",
        "        else:\n",
        "            generator_start = torch.Generator(device=self.device).manual_seed(start.seed)\n",
        "            generator_end = torch.Generator(device=self.device).manual_seed(end.seed)\n",
        "\n",
        "        # Text encodings\n",
        "        if use_reweighting:\n",
        "            embed_start = self.embed_text_weighted(start.prompt, diffusion_pipeline)\n",
        "            embed_end = self.embed_text_weighted(end.prompt, diffusion_pipeline)\n",
        "        else:\n",
        "            embed_start = self.embed_text(start.prompt, diffusion_pipeline)\n",
        "            embed_end = self.embed_text(end.prompt, diffusion_pipeline)\n",
        "\n",
        "        text_embedding = embed_start + alpha * (embed_end - embed_start)\n",
        "\n",
        "        # Image latents\n",
        "        init_image_torch = preprocess_image(init_image).to(\n",
        "            device=self.device, dtype=embed_start.dtype\n",
        "        )\n",
        "        init_latent_dist = diffusion_pipeline.vae.encode(init_image_torch).latent_dist\n",
        "        # TODO(hayk): Probably this seed should just be 0 always? Make it 100% symmetric. The\n",
        "        # result is so close no matter the seed that it doesn't really add variety.\n",
        "        if self.device.lower().startswith(\"mps\"):\n",
        "            generator = torch.Generator(device=\"cpu\").manual_seed(start.seed)\n",
        "        else:\n",
        "            generator = torch.Generator(device=self.device).manual_seed(start.seed)\n",
        "\n",
        "        init_latents = init_latent_dist.sample(generator=generator)\n",
        "        init_latents = 0.18215 * init_latents\n",
        "\n",
        "        # Prepare mask latent\n",
        "        mask: T.Optional[torch.Tensor] = None\n",
        "        if mask_image:\n",
        "            vae_scale_factor = 2 ** (len(diffusion_pipeline.vae.config.block_out_channels) - 1)\n",
        "            mask = preprocess_mask(mask_image, scale_factor=vae_scale_factor).to(\n",
        "                device=self.device, dtype=embed_start.dtype\n",
        "            )\n",
        "\n",
        "        outputs = self.interpolate_img2img(\n",
        "            diffusion_pipeline=diffusion_pipeline,\n",
        "            text_embeddings=text_embedding,\n",
        "            init_latents=init_latents,\n",
        "            mask=mask,\n",
        "            generator_a=generator_start,\n",
        "            generator_b=generator_end,\n",
        "            interpolate_alpha=alpha,\n",
        "            strength_a=start.denoising,\n",
        "            strength_b=end.denoising,\n",
        "            num_inference_steps=inputs.num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "        )\n",
        "\n",
        "        return outputs[\"images\"][0]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def interpolate_img2img(\n",
        "        self,\n",
        "        diffusion_pipeline: StableDiffusionPipeline,\n",
        "        text_embeddings: torch.Tensor,\n",
        "        init_latents: torch.Tensor,\n",
        "        generator_a: torch.Generator,\n",
        "        generator_b: torch.Generator,\n",
        "        interpolate_alpha: float,\n",
        "        mask: T.Optional[torch.Tensor] = None,\n",
        "        strength_a: float = 0.8,\n",
        "        strength_b: float = 0.8,\n",
        "        num_inference_steps: int = 50,\n",
        "        guidance_scale: float = 7.5,\n",
        "        negative_prompt: T.Optional[T.Union[str, T.List[str]]] = None,\n",
        "        num_images_per_prompt: int = 1,\n",
        "        eta: T.Optional[float] = 0.0,\n",
        "        output_type: T.Optional[str] = \"pil\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TODO\n",
        "        \"\"\"\n",
        "        batch_size = text_embeddings.shape[0]\n",
        "\n",
        "        # set timesteps\n",
        "        diffusion_pipeline.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "        bs_embed, seq_len, _ = text_embeddings.shape\n",
        "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
        "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            if negative_prompt is None:\n",
        "                uncond_tokens = [\"\"]\n",
        "            elif isinstance(negative_prompt, str):\n",
        "                uncond_tokens = [negative_prompt]\n",
        "            elif batch_size != len(negative_prompt):\n",
        "                raise ValueError(\"The length of `negative_prompt` should be equal to batch_size.\")\n",
        "            else:\n",
        "                uncond_tokens = negative_prompt\n",
        "\n",
        "            # max_length = text_input_ids.shape[-1]\n",
        "            uncond_input = diffusion_pipeline.tokenizer(\n",
        "                uncond_tokens,\n",
        "                padding=\"max_length\",\n",
        "                max_length=diffusion_pipeline.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            uncond_embeddings = diffusion_pipeline.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # duplicate unconditional embeddings for each generation per prompt\n",
        "            uncond_embeddings = uncond_embeddings.repeat_interleave(\n",
        "                batch_size * num_images_per_prompt, dim=0\n",
        "            )\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        latents_dtype = text_embeddings.dtype\n",
        "\n",
        "        strength = (1 - interpolate_alpha) * strength_a + interpolate_alpha * strength_b\n",
        "\n",
        "        # get the original timestep using init_timestep\n",
        "        offset = diffusion_pipeline.scheduler.config.get(\"steps_offset\", 0)\n",
        "        init_timestep = int(num_inference_steps * strength) + offset\n",
        "        init_timestep = min(init_timestep, num_inference_steps)\n",
        "\n",
        "        timesteps = diffusion_pipeline.scheduler.timesteps[-init_timestep]\n",
        "        timesteps = torch.tensor(\n",
        "            [timesteps] * batch_size * num_images_per_prompt, device=self.device\n",
        "        )\n",
        "\n",
        "        # add noise to latents using the timesteps\n",
        "        noise_a = torch.randn(\n",
        "            init_latents.shape, generator=generator_a, device=self.device, dtype=latents_dtype\n",
        "        )\n",
        "        noise_b = torch.randn(\n",
        "            init_latents.shape, generator=generator_b, device=self.device, dtype=latents_dtype\n",
        "        )\n",
        "        noise = torch_util.slerp(interpolate_alpha, noise_a, noise_b)\n",
        "        init_latents_orig = init_latents\n",
        "        init_latents = diffusion_pipeline.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same args\n",
        "        # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(diffusion_pipeline.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        latents = init_latents.clone()\n",
        "\n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "\n",
        "        # Some schedulers like PNDM have timesteps as arrays\n",
        "        # It's more optimized to move all timesteps to correct device beforehand\n",
        "        timesteps = diffusion_pipeline.scheduler.timesteps[t_start:].to(self.device)\n",
        "\n",
        "        for i, t in enumerate(diffusion_pipeline.progress_bar(timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = (\n",
        "                torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            )\n",
        "            latent_model_input = diffusion_pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = diffusion_pipeline.unet(\n",
        "                latent_model_input, t, encoder_hidden_states=text_embeddings\n",
        "            ).sample\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (\n",
        "                    noise_pred_text - noise_pred_uncond\n",
        "                )\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = diffusion_pipeline.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "            if mask is not None:\n",
        "                init_latents_proper = diffusion_pipeline.scheduler.add_noise(\n",
        "                    init_latents_orig, noise, torch.tensor([t])\n",
        "                )\n",
        "                # import ipdb; ipdb.set_trace()\n",
        "                latents = (init_latents_proper * mask) + (latents * (1 - mask))\n",
        "\n",
        "        latents = 1.0 / 0.18215 * latents\n",
        "        image = diffusion_pipeline.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = diffusion_pipeline.numpy_to_pil(image)\n",
        "\n",
        "        return dict(images=image, latents=latents, nsfw_content_detected=False)\n",
        "\n",
        "\n",
        "def preprocess_image(image: Image.Image) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Preprocess an image for the model.\n",
        "    \"\"\"\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
        "\n",
        "    image_np = np.array(image).astype(np.float32) / 255.0\n",
        "    image_np = image_np[None].transpose(0, 3, 1, 2)\n",
        "\n",
        "    image_torch = torch.from_numpy(image_np)\n",
        "\n",
        "    return 2.0 * image_torch - 1.0\n",
        "\n",
        "\n",
        "def preprocess_mask(mask: Image.Image, scale_factor: int = 8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Preprocess a mask for the model.\n",
        "    \"\"\"\n",
        "    # Convert to grayscale\n",
        "    mask = mask.convert(\"L\")\n",
        "\n",
        "    # Resize to integer multiple of 32\n",
        "    w, h = mask.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))\n",
        "    mask = mask.resize((w // scale_factor, h // scale_factor), resample=Image.NEAREST)\n",
        "\n",
        "    # Convert to numpy array and rescale\n",
        "    mask_np = np.array(mask).astype(np.float32) / 255.0\n",
        "\n",
        "    # Tile and transpose\n",
        "    mask_np = np.tile(mask_np, (4, 1, 1))\n",
        "    mask_np = mask_np[None].transpose(0, 1, 2, 3)  # what does this step do?\n",
        "\n",
        "    # Invert to repaint white and keep black\n",
        "    mask_np = 1 - mask_np  # repaint white, keep black\n",
        "\n",
        "    return torch.from_numpy(mask_np)\n",
        "      "
      ],
      "metadata": {
        "id": "egWSBwgKGSdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from riffusion.datatypes import InferenceInput, PromptInput\n",
        "\n",
        "start = PromptInput(prompt=\"special pop\", seed=45326)\n",
        "end = PromptInput(prompt=\"special pop\", seed=45326)\n",
        "alpha = 0.5\n",
        "inputs = InferenceInput(start, end, alpha)\n",
        "\n",
        "init_image_path = \"/content/riffusion/seed_images/agile.png\"\n",
        "init_image = Image.open(str(init_image_path)).convert(\"RGB\")\n",
        "mask_image: T.Optional[Image.Image] = None"
      ],
      "metadata": {
        "id": "eeMejG0LQDS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory„ÅåË∂≥„Çâ„Å™„Åè„Å¶Ê≠ª„Å¨...\n",
        "checkpoint: str = \"/content/riffusion_weights/special/5/\"\n",
        "no_traced_unet: bool = False,\n",
        "device: str = \"cuda\"\n",
        "\n",
        "diffusion_pipeline = InferRiffusionPipeline.load_checkpoint(\n",
        "    checkpoint,\n",
        "    use_traced_unet= not no_traced_unet,\n",
        "    device=device,\n",
        ")\n"
      ],
      "metadata": {
        "id": "sBqEqyehHYqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = InferRiffusionPipeline().riffuse(\n",
        "        diffusion_pipeline=diffusion_pipeline,\n",
        "        inputs=inputs,\n",
        "        init_image=init_image,\n",
        "        mask_image=mask_image,\n",
        ")"
      ],
      "metadata": {
        "id": "-gF5K-z1WvRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert image to audio\n",
        "import io\n",
        "from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "from riffusion.spectrogram_params import SpectrogramParams\n",
        "from riffusion.datatypes import InferenceOutput\n",
        "from riffusion.util import base64_util\n",
        "import json"
      ],
      "metadata": {
        "id": "HdblC5Y2YWni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = SpectrogramParams(\n",
        "    min_frequency=0,\n",
        "    max_frequency=10000,\n",
        ")\n",
        "converter = SpectrogramImageConverter(params=params, device=str(diffusion_pipeline.device))\n",
        "segment = converter.audio_from_spectrogram_image(\n",
        "        image,\n",
        "        apply_filters=True,\n",
        ")\n",
        "\n",
        "# Export audio to MP3 bytes\n",
        "mp3_bytes = io.BytesIO()\n",
        "segment.export(mp3_bytes, format=\"mp3\")\n",
        "mp3_bytes.seek(0)\n",
        "\n",
        "# Export image to JPEG bytes\n",
        "image_bytes = io.BytesIO()\n",
        "image.save(image_bytes, exif=image.getexif(), format=\"JPEG\")\n",
        "image_bytes.seek(0)\n",
        "\n",
        "# Assemble the output dataclass\n",
        "output = InferenceOutput(\n",
        "        image=\"data:image/jpeg;base64,\" + base64_util.encode(image_bytes),\n",
        "        audio=\"data:audio/mpeg;base64,\" + base64_util.encode(mp3_bytes),\n",
        "        duration_s=segment.duration_seconds,\n",
        ")\n",
        "\n",
        "oo = json.dumps(dataclasses.asdict(output))\n",
        "print(json.loads(oo)[\"duration_s\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tfUxsdwY-tu",
        "outputId": "bc64a315-72f4-4bf1-f4bc-1e7f808641a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title text-image training method(Reference)\n",
        "\n",
        "!pip install diffusers scipy\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio natsort"
      ],
      "metadata": {
        "id": "CcwMJmXRKAH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to HuggingFace ü§ó\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ü§ó Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ],
      "metadata": {
        "id": "qdzcyy7ZKRgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4.\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cpJujvKKY72",
        "outputId": "adb5128c-65cb-4a29-f815-6f10c173d632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"stabilityai/stable-diffusion-2-base\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqqGQ6_fK9g6",
        "outputId": "6222cbbc-7d8e-46b1-8a00-2e81906c9062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/stable_diffusion_weights/zwx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of zwx dog\",\n",
        "        \"class_prompt\":         \"photo of a dog\",\n",
        "        \"instance_data_dir\":    \"/content/data/zwx\",\n",
        "        \"class_data_dir\":       \"/content/data/dog\"\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "gNyRouyyLK1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ],
      "metadata": {
        "id": "oTak-DY4MfiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=800 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"photo of zwx dog\" \\\n",
        "  --concepts_list=\"concepts_list.json\""
      ],
      "metadata": {
        "id": "ku9seNvSOxcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0uEYpL_TX51",
        "outputId": "37b50d0f-0e98-44e1-e45a-e3e2dc6eaf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] WEIGHTS_DIR=/content/stable_diffusion_weights/zwx/800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ],
      "metadata": {
        "id": "H3QCduVVTiw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title inference\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "g_cuda = None"
      ],
      "metadata": {
        "id": "J3Dh01CVU7g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "1HgSilWSVA9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of zwx dog in a bucket\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "gZRyKmQwVDt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}