{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHbGfiienfPtr2URp+X3XF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/computervision/RelTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "-GcM-FH2Tma5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stf0ITgLTfLL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/yrcong/RelTR.git\n",
        "%cd RelTR/\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VG Labels"
      ],
      "metadata": {
        "id": "DIBJeG-gTtul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [ 'N/A', 'airplane', 'animal', 'arm', 'bag', 'banana', 'basket', 'beach', 'bear', 'bed', 'bench', 'bike',\n",
        "                'bird', 'board', 'boat', 'book', 'boot', 'bottle', 'bowl', 'box', 'boy', 'branch', 'building',\n",
        "                'bus', 'cabinet', 'cap', 'car', 'cat', 'chair', 'child', 'clock', 'coat', 'counter', 'cow', 'cup',\n",
        "                'curtain', 'desk', 'dog', 'door', 'drawer', 'ear', 'elephant', 'engine', 'eye', 'face', 'fence',\n",
        "                'finger', 'flag', 'flower', 'food', 'fork', 'fruit', 'giraffe', 'girl', 'glass', 'glove', 'guy',\n",
        "                'hair', 'hand', 'handle', 'hat', 'head', 'helmet', 'hill', 'horse', 'house', 'jacket', 'jean',\n",
        "                'kid', 'kite', 'lady', 'lamp', 'laptop', 'leaf', 'leg', 'letter', 'light', 'logo', 'man', 'men',\n",
        "                'motorcycle', 'mountain', 'mouth', 'neck', 'nose', 'number', 'orange', 'pant', 'paper', 'paw',\n",
        "                'people', 'person', 'phone', 'pillow', 'pizza', 'plane', 'plant', 'plate', 'player', 'pole', 'post',\n",
        "                'pot', 'racket', 'railing', 'rock', 'roof', 'room', 'screen', 'seat', 'sheep', 'shelf', 'shirt',\n",
        "                'shoe', 'short', 'sidewalk', 'sign', 'sink', 'skateboard', 'ski', 'skier', 'sneaker', 'snow',\n",
        "                'sock', 'stand', 'street', 'surfboard', 'table', 'tail', 'tie', 'tile', 'tire', 'toilet', 'towel',\n",
        "                'tower', 'track', 'train', 'tree', 'truck', 'trunk', 'umbrella', 'vase', 'vegetable', 'vehicle',\n",
        "                'wave', 'wheel', 'window', 'windshield', 'wing', 'wire', 'woman', 'zebra']\n",
        "\n",
        "REL_CLASSES = ['__background__', 'above', 'across', 'against', 'along', 'and', 'at', 'attached to', 'behind',\n",
        "                'belonging to', 'between', 'carrying', 'covered in', 'covering', 'eating', 'flying in', 'for',\n",
        "                'from', 'growing on', 'hanging from', 'has', 'holding', 'in', 'in front of', 'laying on',\n",
        "                'looking at', 'lying on', 'made of', 'mounted on', 'near', 'of', 'on', 'on back of', 'over',\n",
        "                'painted on', 'parked on', 'part of', 'playing', 'riding', 'says', 'sitting on', 'standing on',\n",
        "                'to', 'under', 'using', 'walking in', 'walking on', 'watching', 'wearing', 'wears', 'with']"
      ],
      "metadata": {
        "id": "5E82xPFhTusR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load model"
      ],
      "metadata": {
        "id": "Fxj05d53TyYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.backbone import Backbone, Joiner\n",
        "from models.position_encoding import PositionEmbeddingSine\n",
        "from models.transformer import Transformer\n",
        "from models.reltr import RelTR\n",
        "\n",
        "position_embedding = PositionEmbeddingSine(128, normalize=True)\n",
        "backbone = Backbone('resnet50', False, False, False)\n",
        "backbone = Joiner(backbone, position_embedding)\n",
        "backbone.num_channels = 2048\n",
        "\n",
        "transformer = Transformer(d_model=256, dropout=0.1, nhead=8,\n",
        "                          dim_feedforward=2048,\n",
        "                          num_encoder_layers=6,\n",
        "                          num_decoder_layers=6,\n",
        "                          normalize_before=False,\n",
        "                          return_intermediate_dec=True)\n",
        "\n",
        "model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n",
        "              num_entities=100, num_triplets=200)\n",
        "\n",
        "# The checkpoint is pretrained on Visual Genome\n",
        "ckpt = torch.hub.load_state_dict_from_url(\n",
        "    url='https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth',\n",
        "    map_location='cpu', check_hash=True)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "M9Yt4JAoTzuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils function"
      ],
      "metadata": {
        "id": "q4-94-XIUFSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some transformation functions\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b"
      ],
      "metadata": {
        "id": "yuYYjueaUA63"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "wuFNmAW5USTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.zurich.co.jp/-/Media/jpz/zrh/car/useful/guide/cc-whatis-arterial-road/img_re/cc-whatis-arterial-road_img_001.jpg' # road\n",
        "# url = \"https://c.files.bbci.co.uk/2270/production/_127861880_gettyimages-1245282739.jpg\" # mitoma\n",
        "# url = \"https://www.footballchannel.jp/wordpress/assets/2022/12/1202Mitoma_getty.jpg\"  # mitoma 2\n",
        "# url = \"https://assets.renoveru.jp/journal/wp-content/uploads/2021/11/10125410/16187_01.jpg\"  # room\n",
        "im = Image.open(requests.get(url, stream=True).raw)\n",
        "plt.imshow(im)\n",
        "img = transform(im).unsqueeze(0)"
      ],
      "metadata": {
        "id": "0BcPEX_rUTMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(img)\n",
        "\n",
        "# keep only predictions with >0.3 confidence\n",
        "probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n",
        "probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n",
        "probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = torch.logical_and(probas.max(-1).values > 0.3, torch.logical_and(probas_sub.max(-1).values > 0.3,\n",
        "                                                                        probas_obj.max(-1).values > 0.3))"
      ],
      "metadata": {
        "id": "rG_aH6VNUngP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert boxes from [0; 1] to image scales\n",
        "sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n",
        "obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n",
        "\n",
        "topk = 10 # display up to 10 images\n",
        "keep_queries = torch.nonzero(keep, as_tuple=True)[0]\n",
        "indices = torch.argsort(-probas[keep_queries].max(-1)[0] * probas_sub[keep_queries].max(-1)[0] * probas_obj[keep_queries].max(-1)[0])[:topk]\n",
        "keep_queries = keep_queries[indices]"
      ],
      "metadata": {
        "id": "7O9gwOvIUsEH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the attention weights\n",
        "conv_features, dec_attn_weights_sub, dec_attn_weights_obj = [], [], []\n",
        "hooks = [\n",
        "    model.backbone[-2].register_forward_hook(\n",
        "        lambda self, input, output: conv_features.append(output)\n",
        "    ),\n",
        "    model.transformer.decoder.layers[-1].cross_attn_sub.register_forward_hook(\n",
        "        lambda self, input, output: dec_attn_weights_sub.append(output[1])\n",
        "    ),\n",
        "    model.transformer.decoder.layers[-1].cross_attn_obj.register_forward_hook(\n",
        "        lambda self, input, output: dec_attn_weights_obj.append(output[1])\n",
        "    )]"
      ],
      "metadata": {
        "id": "QjX-e58eUul1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "nRmewZuOUyeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # propagate through the model\n",
        "    outputs = model(img)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # don't need the list anymore\n",
        "    conv_features = conv_features[0]\n",
        "    dec_attn_weights_sub = dec_attn_weights_sub[0]\n",
        "    dec_attn_weights_obj = dec_attn_weights_obj[0]\n",
        "\n",
        "    # get the feature map shape\n",
        "    h, w = conv_features['0'].tensors.shape[-2:]\n",
        "    im_w, im_h = im.size\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(indices), nrows=3, figsize=(22, 7))\n",
        "    for idx, ax_i, (sxmin, symin, sxmax, symax), (oxmin, oymin, oxmax, oymax) in \\\n",
        "            zip(keep_queries, axs.T, sub_bboxes_scaled[indices], obj_bboxes_scaled[indices]):\n",
        "        ax = ax_i[0]\n",
        "        ax.imshow(dec_attn_weights_sub[0, idx].view(h, w))\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'query id: {idx.item()}')\n",
        "        ax = ax_i[1]\n",
        "        ax.imshow(dec_attn_weights_obj[0, idx].view(h, w))\n",
        "        ax.axis('off')\n",
        "        ax = ax_i[2]\n",
        "        ax.imshow(im)\n",
        "        ax.add_patch(plt.Rectangle((sxmin, symin), sxmax - sxmin, symax - symin,\n",
        "                                    fill=False, color='blue', linewidth=2.5))\n",
        "        ax.add_patch(plt.Rectangle((oxmin, oymin), oxmax - oxmin, oymax - oymin,\n",
        "                                    fill=False, color='orange', linewidth=2.5))\n",
        "\n",
        "        ax.axis('off')\n",
        "        ax.set_title(CLASSES[probas_sub[idx].argmax()]+' '+REL_CLASSES[probas[idx].argmax()]+' '+CLASSES[probas_obj[idx].argmax()], fontsize=10)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show() # show the output\n"
      ],
      "metadata": {
        "id": "tVbWJNPUUvJw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}