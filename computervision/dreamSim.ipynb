{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPC/YKMdJLcdkXH38GS4Ubz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/computervision/dreamSim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "Rh-s2-ZM7EyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4XTMSga63i7"
      },
      "outputs": [],
      "source": [
        "!pip install dreamsim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download weights"
      ],
      "metadata": {
        "id": "R2H9ixhG7LlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models/\n",
        "!wget -O models/open_clip_vitb32_pretrain.pth.tar https://github.com/ssundaram21/dreamsim/releases/download/v0.1.0/open_clip_vitb32_pretrain.pth.tar"
      ],
      "metadata": {
        "id": "syWhageg7Ar1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo Images"
      ],
      "metadata": {
        "id": "rzW9MEHr7M04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/images\n",
        "!wget https://github.com/ssundaram21/dreamsim/releases/download/v0.1.0/sample_images.zip -O images/sample_images.zip\n",
        "!wget https://github.com/ssundaram21/dreamsim/releases/download/v0.1.0/retrieval_images.zip -O images/retrieval_images.zip\n",
        "!unzip images/sample_images.zip\n",
        "!unzip images/retrieval_images.zip"
      ],
      "metadata": {
        "id": "fsySqAJM7QIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "UfCYiNHu7WM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from dreamsim import dreamsim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = dreamsim(pretrained=True)"
      ],
      "metadata": {
        "id": "axCYuwNg7XH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils function"
      ],
      "metadata": {
        "id": "AEEW4Shj7bUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_imgs(ims, captions=None):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=len(ims), figsize=(10, 5))\n",
        "    for i in range(len(ims)):\n",
        "        ax[i].imshow(ims[i])\n",
        "        ax[i].axis('off')\n",
        "        if captions is not None:\n",
        "          ax[i].set_title(captions[i], fontweight=\"bold\")"
      ],
      "metadata": {
        "id": "IMKfZVoM7YOz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Search"
      ],
      "metadata": {
        "id": "nQTeWVjL7hDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AMY-7Lyr7gJS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_pil = Image.open(\"sample_images/ref_1.png\")\n",
        "img_a_pil = Image.open(\"sample_images/img_a_1.png\")\n",
        "img_b_pil = Image.open(\"sample_images/img_b_1.png\")\n",
        "\n",
        "show_imgs(\n",
        "    ims=[img_a_pil, ref_pil, img_b_pil],\n",
        "    captions=[\"A\", \"Reference\", \"B\"])"
      ],
      "metadata": {
        "id": "aXL5kvrW7l23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate similarity score\n",
        "ref = preprocess(ref_pil).to(device)\n",
        "img_a = preprocess(img_a_pil).to(device)\n",
        "img_b = preprocess(img_b_pil).to(device)\n",
        "\n",
        "dist_a = model(ref, img_a)\n",
        "dist_b = model(ref, img_b)\n",
        "\n",
        "show_imgs(\n",
        "    ims=[img_a_pil, ref_pil, img_b_pil],\n",
        "    captions=[f\"A, Score: {round(float(dist_a.cpu()), 3)}\",\n",
        "              \"Reference\",\n",
        "              f\"B, Score: {round(float(dist_b.cpu()), 3)}\"])"
      ],
      "metadata": {
        "id": "-GKeg6mE7yic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference own data"
      ],
      "metadata": {
        "id": "1F17BNcq73LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Required:\n",
        "ref_path = \"sample_images/ref_2.png\" #@param {type:\"string\"}\n",
        "img_a_path = \"sample_images/img_a_2.png\" #@param {type:\"string\"}\n",
        "#@markdown Optional:\n",
        "img_b_path = \"sample_images/img_b_2.png\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "Jpe8hAzh75Ny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_pil = Image.open(ref_path)\n",
        "img_a_pil = Image.open(img_a_path)\n",
        "ref = preprocess(ref_pil).to(device)\n",
        "img_a = preprocess(img_a_pil).to(device)\n",
        "dist_a = model(ref, img_a)\n",
        "\n",
        "if len(img_b_path) > 0:\n",
        "  img_b_pil = Image.open(img_b_path)\n",
        "  img_b = preprocess(img_b_pil).to(device)\n",
        "  dist_b = model(ref, img_b)\n",
        "  ims = [img_a_pil, ref_pil, img_b_pil]\n",
        "  captions = [f\"A, Score: {round(float(dist_a.cpu()), 3)}\", \"Reference\",\n",
        "              f\"B, Score: {round(float(dist_b.cpu()), 3)}\"]\n",
        "else:\n",
        "  ims = [ref_pil, img_a_pil]\n",
        "  captions = [\"Reference\", f\"Score: {round(float(dist_a.cpu()), 3)}\"]\n",
        "\n",
        "show_imgs(\n",
        "    ims=ims,\n",
        "    captions=captions)"
      ],
      "metadata": {
        "id": "Y10n_urW76Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNLUHHA98Gj6",
        "outputId": "193f868e-2c59-4587-bbdd-4d4d19c11e46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24670"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Retrieval"
      ],
      "metadata": {
        "id": "Db94hGzj8Hyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "root = \"retrieval_images/\"\n",
        "images = []\n",
        "for path in os.listdir(root):\n",
        "  try:\n",
        "    images.append(Image.open(root + path))\n",
        "  except:\n",
        "    pass\n",
        "query, images = images[0], images[1:]"
      ],
      "metadata": {
        "id": "VK8xzNb48Jg_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparison with 3 models\n",
        "from dreamsim import PerceptualModel\n",
        "\n",
        "dreamsim_model = model\n",
        "dino_model = PerceptualModel(feat_type='cls', model_type='dino_vitb16', stride='16', baseline=True, device=\"cuda\")\n",
        "open_clip_model = PerceptualModel(feat_type='embedding', model_type='open_clip_vitb32', stride='32', baseline=True, device=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc90rhHS8T4i",
        "outputId": "466079d0-f2f2-4efb-cbaa-acc7360202b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in ./models/facebookresearch_dino_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for embedding\n",
        "def get_embeddings(model, name, images):\n",
        "  embeddings = []\n",
        "  for img in tqdm(images):\n",
        "    img = preprocess(img).to(device)\n",
        "    embeddings.append(model.embed(img).detach().cpu())\n",
        "  with open(f\"images/{name}_embeds.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embeddings, f)\n",
        "\n",
        "get_embeddings(dreamsim_model, \"dreamsim\", images)\n",
        "get_embeddings(dino_model, \"dino\", images)\n",
        "get_embeddings(open_clip_model, \"open_clip\", images)"
      ],
      "metadata": {
        "id": "gvF9wRsz8afq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(embeddings, query_index):\n",
        "    query_embed = embeddings[query_index]\n",
        "    dists = {}\n",
        "\n",
        "    # Compute the (cosine) distance between the query embedding\n",
        "    # and each search image embedding\n",
        "    for i, im in enumerate(embeddings):\n",
        "      if i == query_index:\n",
        "        continue\n",
        "      dists[i] = (1 - F.cosine_similarity(query_embed, embeddings[i],\n",
        "                                          dim=-1)).item()\n",
        "\n",
        "    # Return results sorted by distance\n",
        "    df = pd.DataFrame({\"ids\": list(dists.keys()), \"dists\": list(dists.values())})\n",
        "    df = df.sort_values(by=\"dists\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "MUvIxmC88dif"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title query\n",
        "query_index = 15 #@param {type:\"number\"}\n",
        "n = 3\n",
        "display_width = 11\n",
        "display_height = 4\n",
        "\n",
        "## Load embeddings for each metric and compute nearest neighbors to the query_index-th image\n",
        "nn_dfs = {}\n",
        "for metric_name in [\"dreamsim\", \"open_clip\", \"dino\"]:\n",
        "    with open(f\"images/{metric_name}_embeds.pkl\", \"rb\") as f:\n",
        "      embeddings = pickle.load(f)\n",
        "    nn_dfs[metric_name] = nearest_neighbors(embeddings, query_index)\n",
        "\n",
        "## Plot results\n",
        "f, ax = plt.subplots(4, n+2, figsize=(14,7), gridspec_kw={\"height_ratios\":[0.005,1,1,1]})\n",
        "ax[0,0].axis('off')\n",
        "for col in range(1, n+2):\n",
        "    title = \"Query\" if col == 1 else f\"n{col-1}\"\n",
        "    ax[0, col].set_title(title, fontweight=\"bold\", fontsize=15)\n",
        "    ax[0, col].axis('off')\n",
        "\n",
        "for i, name in enumerate([\"dreamsim\", \"open_clip\", \"dino\"]):\n",
        "    ax[i+1, 0].text(0.5, 0.5, name, fontsize=13)\n",
        "    ax[i+1, 0].axis('off')\n",
        "\n",
        "    ax[i+1, 1].imshow(images[query_index])\n",
        "    ax[i+1, 1].axis(\"off\")\n",
        "\n",
        "    for j in range(n):\n",
        "        im_idx = nn_dfs[name]['ids'].iloc[j]\n",
        "        ax[i + 1, j + 2].imshow(images[im_idx])\n",
        "        ax[i + 1, j + 2].axis('off')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "ZFU4HovI8iQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptual Loss"
      ],
      "metadata": {
        "id": "pOa-9FG19xVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_img_path = \"sample_images/ref_1.png\""
      ],
      "metadata": {
        "id": "dDKjBKLs9y7H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "NJPVwafA92IW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop(img, sizex, sizey, crop_margin_x, crop_margin_y):\n",
        "  startx = np.random.randint(crop_margin_x)\n",
        "  starty = np.random.randint(crop_margin_y)\n",
        "  endx = startx + sizex\n",
        "  endy = starty + sizey\n",
        "  return img[:, :, startx:endx, starty:endy]"
      ],
      "metadata": {
        "id": "vYwEwfEm94KR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizex, sizey, crop_margin_x, crop_margin_y = 224, 224, 32, 32\n",
        "ref = preprocess(Image.open(ref_img_path)).to(device)\n",
        "pred = torch.rand([1, 3, sizex+crop_margin_x, sizey+crop_margin_y])\n",
        "pred = Variable(pred.cuda(), requires_grad=True)\n",
        "\n",
        "# use model as the loss for dreamsim, and loss_fn as the loss for lpips\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = torch.optim.Adam([pred], lr=1e-2)\n",
        "plt.ion()\n",
        "fig = plt.figure(1)\n",
        "ax = fig.add_subplot(131)\n",
        "ax.imshow(ref[0].permute(1,2,0).detach().cpu( ))\n",
        "ax.axis('off')\n",
        "ax.set_title('Reference')\n",
        "ax = fig.add_subplot(132)\n",
        "ax.imshow(pred[0].permute(1, 2, 0).detach().cpu())\n",
        "ax.axis('off')\n",
        "ax.set_title('Initialization')\n",
        "\n",
        "for i in tqdm(list(range(2500))):\n",
        "    optimizer.zero_grad()\n",
        "    pred_inpt = crop(pred, sizex, sizey, crop_margin_x, crop_margin_y)\n",
        "    dist = model(pred_inpt, ref)\n",
        "    dist.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "ax = fig.add_subplot(133)\n",
        "ax.imshow(pred[0].permute(1, 2, 0).detach().cpu())\n",
        "ax.axis('off')\n",
        "ax.set_title('Output')"
      ],
      "metadata": {
        "id": "PcjOgIFp96mQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}