{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxi4+dAjYIZgH4GAeXEG6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/generative/dreambooth_multiconcepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJSomkuf-wQB"
      },
      "outputs": [],
      "source": [
        "!pip install ninja bitsandbytes xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShivamShrirao/diffusers.git\n",
        "%cd diffusers/examples/dreambooth\n",
        "!pip install git+https://github.com/ShivamShrirao/diffusers.git\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "pa9m0f40-_Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCESS_TOKEN -> \"\""
      ],
      "metadata": {
        "id": "VZNrw0b7_6AG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "CT9WBTGo_4Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDC3_9sbAF7K",
        "outputId": "4d829730-fe63-45d3-d1ae-10a614481fa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/stable_diffusion_weights/zwx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "# a style\n",
        "# face\n",
        "\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"a ->apriljapan style\",\n",
        "        \"class_prompt\":         \"a style\",\n",
        "        \"instance_data_dir\":    \"/content/data/apriljapan\",\n",
        "        \"class_data_dir\":       \"/content/data/style\"\n",
        "    },\n",
        "    {\n",
        "          \"instance_prompt\":      \"3d ->anya face\",\n",
        "          \"class_prompt\":         \"3d face\",\n",
        "         \"instance_data_dir\":    \"/content/data/anya\",\n",
        "         \"class_data_dir\":       \"/content/data/3d-face\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "MDyRTXz2AxNq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=800 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"a ->apriljapan style, 3d ->anya face\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ],
      "metadata": {
        "id": "TcSNZounDvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4yzSv4NIG4G",
        "outputId": "4c14bbd6-03bb-4a54-ca27-15196583b28e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] WEIGHTS_DIR=/content/stable_diffusion_weights/zwx/800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzWGKm4XIXUy",
        "outputId": "3c604e7a-c13d-4841-b9ac-475b58f0a340"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled.\n",
            "Error caught was: No module named 'triton'\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNLwv4E9Iwmn",
        "outputId": "94735537-e7e2-428e-c88c-1930232189dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7aaf868250>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"3d ->anya face \" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 1 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "7qFUfDYyI0FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dves8Mw0Ma5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title stable diffusion inpaiting with dreambooth"
      ],
      "metadata": {
        "id": "dFMdsUVLMbbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/dreambooth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuPU38SyXXKW",
        "outputId": "79620211-16fa-40af-c8f2-5f93ce2905c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers/examples/dreambooth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-inpainting\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V6C3vwfXTDk",
        "outputId": "644464ab-6e7e-46ab-e61a-a11122cb114c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/stable_diffusion_weights/zwx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "# a style\n",
        "# face\n",
        "\n",
        "\n",
        "concepts_list = [\n",
        "    # {\n",
        "    #    \"instance_prompt\":      \"a ->apriljapan style\",\n",
        "    #    \"class_prompt\":         \"a style\",\n",
        "    #    \"instance_data_dir\":    \"/content/data/apriljapan\",\n",
        "    #    \"class_data_dir\":       \"/content/data/style\"\n",
        "    # },\n",
        "    {\n",
        "          \"instance_prompt\":      \"3d ->anya face\",\n",
        "          \"class_prompt\":         \"3d face\",\n",
        "         \"instance_data_dir\":    \"/content/data/anya\",\n",
        "         \"class_data_dir\":       \"/content/data/3d-face\"\n",
        "    },\n",
        "    {\n",
        "          \"instance_prompt\":      \"3d ->pamo pokemon\",\n",
        "          \"class_prompt\":         \"3d pokemon\",\n",
        "         \"instance_data_dir\":    \"/content/data/pamo\",\n",
        "         \"class_data_dir\":       \"/content/data/3d-pokemon\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "5DazZcaiXeh0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_inpainting_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=2 \\\n",
        "  --train_text_encoder \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=300 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=1200 \\\n",
        "  --n_save_sample=0 \\\n",
        "  --save_infer_steps=35 \\\n",
        "  --not_cache_latents \\\n",
        "  --hflip \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ],
      "metadata": {
        "id": "d9s5TNzYXp0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title inference\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-inpainting\",\n",
        "    revision=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n",
        "#image and mask_image should be PIL images.\n",
        "#The mask structure is white for inpainting and black for keeping as is\n",
        "image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\n",
        "image.save(\"./yellow_cat_on_park_bench.png\")"
      ],
      "metadata": {
        "id": "9cR2obliZ0sJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}