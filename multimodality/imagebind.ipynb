{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSls4Jxmjvxm+kd62NGrc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/multimodality/imagebind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "xZJxeJXTGXSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIlD7T_4EMoV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/ImageBind.git\n",
        "!pip install git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d timm==0.6.7 ftfy regex einops fvcore decord==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Checker"
      ],
      "metadata": {
        "id": "4FBGJt9RGZvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ImageBind"
      ],
      "metadata": {
        "id": "MK2lPqWxGcpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from PIL import Image\n",
        "\n",
        "text = 'bird'\n",
        "display(Image.open(f\".assets/{text}_image.jpg\"))\n",
        "display(Image.open(f\".assets/{text}_depth.jpg\"))\n",
        "IPython.display.Audio(f\".assets/{text}_audio.wav\")"
      ],
      "metadata": {
        "id": "StqoOy2SGbFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "EnTUMNkuGYxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ImageBind"
      ],
      "metadata": {
        "id": "ZZvxFsirEzDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import data\n",
        "import torch\n",
        "from models import imagebind_model\n",
        "from models.imagebind_model import ModalityType\n",
        "\n",
        "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
        "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
        "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate model\n",
        "model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Load data\n",
        "inputs = {\n",
        "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
        "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
        "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = model(inputs)\n",
        "\n",
        "print(\n",
        "    \"Vision x Text: \",\n",
        "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Audio x Text: \",\n",
        "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Vision x Audio: \",\n",
        "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
        ")\n"
      ],
      "metadata": {
        "id": "KJ7QyU7wExje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Application plan"
      ],
      "metadata": {
        "id": "k8UHFj_JJ5eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HlVkx-OIKZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# depth estimator\n",
        "# https://huggingface.co/spaces/nielsr/dpt-depth-estimation/blob/main/app.py\n",
        "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "mvyGAgAFJ8yI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
        "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")"
      ],
      "metadata": {
        "id": "ldj-WPw6KhNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"bird\"\n",
        "image = Image.open(f\"/content/ImageBind/.assets/{text}_image.jpg\")"
      ],
      "metadata": {
        "id": "rsfYQYlFK015"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = feature_extractor(image, return_tensors=\"pt\")\n",
        "    \n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(**encoding)\n",
        "  predicted_depth = outputs.predicted_depth\n",
        "    \n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "                        predicted_depth.unsqueeze(1),\n",
        "                        size=image.size[::-1],\n",
        "                        mode=\"bicubic\",\n",
        "                        align_corners=False,\n",
        "    ).squeeze()\n",
        "output = prediction.cpu().numpy()\n",
        "formatted = (output * 255 / np.max(output)).astype('uint8')\n",
        "img = Image.fromarray(formatted)"
      ],
      "metadata": {
        "id": "9vp5Xf5XKrat"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(img)\n",
        "img.save(f\"/content/ImageBind/.assets/{text}_depth.jpg\")"
      ],
      "metadata": {
        "id": "lRfkCGT-Kzoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross modality"
      ],
      "metadata": {
        "id": "KThYRt5TL8Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ImageBind"
      ],
      "metadata": {
        "id": "_m1GGTKMMA9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "def load_and_transform_depth_data(depth_paths, device):\n",
        "    if depth_paths is None:\n",
        "        return None\n",
        "\n",
        "    depth_ouputs = []\n",
        "    for depth_path in depth_paths:\n",
        "        data_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(\n",
        "                    224, interpolation=transforms.InterpolationMode.BICUBIC\n",
        "                ),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize((0.5, ), (0.5, ))\n",
        "            ]\n",
        "        )\n",
        "        with open(depth_path, \"rb\") as fopen:\n",
        "            image = Image.open(fopen).convert(\"L\")\n",
        "\n",
        "        image = data_transform(image).to(device)\n",
        "        depth_ouputs.append(image)\n",
        "    return torch.stack(depth_ouputs, dim=0)"
      ],
      "metadata": {
        "id": "OCe6ElLOP46v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import data\n",
        "import torch\n",
        "from models import imagebind_model\n",
        "from models.imagebind_model import ModalityType\n",
        "\n",
        "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
        "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
        "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
        "depth_paths = [\".assets/dog_depth.jpg\", \".assets/car_depth.jpg\", \".assets/bird_depth.jpg\"]\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate model\n",
        "model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Load data\n",
        "inputs = {\n",
        "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
        "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
        "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
        "    ModalityType.DEPTH: load_and_transform_depth_data(depth_paths, device),\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = model(inputs)\n",
        "\n",
        "print(\n",
        "    \"Vision x Depth: \",\n",
        "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.DEPTH].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Text x Depth: \",\n",
        "    torch.softmax(embeddings[ModalityType.TEXT] @ embeddings[ModalityType.DEPTH].T, dim=-1),\n",
        ")\n",
        "print(\n",
        "    \"Depth x Audio: \",\n",
        "    torch.softmax(embeddings[ModalityType.DEPTH] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qymSFi8gL9VF",
        "outputId": "1d96714a-c6b4-4a21-c148-a411d0e4f371"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vision x Depth:  tensor([[0.3444, 0.3040, 0.3516],\n",
            "        [0.3451, 0.2363, 0.4186],\n",
            "        [0.3517, 0.3634, 0.2849]], device='cuda:0')\n",
            "Text x Depth:  tensor([[9.5571e-01, 4.4270e-02, 1.5210e-05],\n",
            "        [5.6266e-01, 4.3734e-01, 9.7014e-10],\n",
            "        [4.6230e-06, 1.0000e+00, 7.2704e-15]], device='cuda:0')\n",
            "Depth x Audio:  tensor([[1.9618e-01, 1.4769e-02, 7.8905e-01],\n",
            "        [1.5248e-02, 4.6171e-03, 9.8014e-01],\n",
            "        [1.5896e-04, 1.8075e-02, 9.8177e-01]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}