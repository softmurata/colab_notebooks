{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDEim/5WBN4ACl+GfDHXoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/multimodality/perception_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "aON4FcAyBly-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chex\n",
        "!pip3 install imageio==2.4.1\n",
        "!pip3 install ml_collections\n",
        "!pip install -q mediapy"
      ],
      "metadata": {
        "id": "t1cS2i6uBljE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import library"
      ],
      "metadata": {
        "id": "Ssw8AatFBohV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vvGstTBi_tWh"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from absl import logging\n",
        "import chex\n",
        "import colorsys\n",
        "import copy\n",
        "import cv2\n",
        "import imageio\n",
        "import io\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import PIL\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ml_collections import config_dict\n",
        "from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple, Type, TypeVar, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load demo TFRecord data from GCP\n",
        "\n",
        "dataset = \"base_oss\"  # @param [\"base_oss\", \"cup_games_oss\", \"grounded_questions_oss\", \"points_oss\"]\n",
        "split = \"train\"  # @param [\"train\", \"test\"]\n",
        "\n",
        "def load_a_sequence_example(dataset, split):\n",
        "  print(f\"Loading dataset {dataset}; split {split}\")\n",
        "  tfrecord_uri = f\"gs://dm-perception-test/tfrecords/v1/{dataset}/{split}/pt_{split}-*-of-*.tfrecord\"\n",
        "\n",
        "  filenames = tf.io.matching_files(tfrecord_uri, name=None)\n",
        "  filenames = [tf.compat.as_str_any(tensor.numpy()) for tensor in filenames]\n",
        "  filenames.sort()\n",
        "  print(f\"Files {filenames}\")\n",
        "  \n",
        "  ds = tf.data.TFRecordDataset(filenames)\n",
        "  # Pick first two and shuffle.\n",
        "  ds = ds.shuffle(2)\n",
        "  ds_iter = ds.as_numpy_iterator()\n",
        "  serialised_example = ds_iter.next()\n",
        "  return tf.train.SequenceExample.FromString(serialised_example)\n",
        "\n",
        "sequence_example = load_a_sequence_example(dataset, split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UTgQPtK_6io",
        "outputId": "b55dbaa9-eeca-4d16-f632-ecf004ab3ec6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset base_oss; split train\n",
            "Files ['gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00000-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00001-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00002-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00003-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00004-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00005-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00006-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00007-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00008-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00009-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00010-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00011-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00012-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00013-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00014-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00015-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00016-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00017-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00018-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00019-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00020-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00021-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00022-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00023-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00024-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00025-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00026-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00027-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00028-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00029-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00030-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00031-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00032-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00033-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00034-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00035-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00036-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00037-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00038-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00039-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00040-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00041-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00042-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00043-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00044-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00045-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00046-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00047-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00048-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00049-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00050-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00051-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00052-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00053-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00054-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00055-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00056-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00057-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00058-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00059-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00060-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00061-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00062-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00063-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00064-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00065-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00066-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00067-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00068-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00069-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00070-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00071-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00072-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00073-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00074-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00075-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00076-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00077-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00078-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00079-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00080-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00081-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00082-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00083-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00084-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00085-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00086-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00087-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00088-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00089-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00090-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00091-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00092-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00093-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00094-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00095-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00096-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00097-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00098-of-00100.tfrecord', 'gs://dm-perception-test/tfrecords/v1/base_oss/train/pt_train-00099-of-00100.tfrecord']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek into SequenceExample\n",
        "show_raw_example = False  #@param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "FT = TypeVar(\"FT\")\n",
        "_FEATURE_ACCESS = {\n",
        "    bytes: lambda x: x.bytes_list.value,\n",
        "    str: lambda x: [s.decode(\"utf-8\") for s in x.bytes_list.value],\n",
        "    int: lambda x: x.int64_list.value,\n",
        "    float: lambda x: x.float_list.value,\n",
        "}\n",
        "\n",
        "\n",
        "def get_features(\n",
        "    example: tf.train.SequenceExample, dtype: Type[FT], feature: str\n",
        ") -> List[List[FT]]:\n",
        "  read_feature = _FEATURE_ACCESS[dtype]\n",
        "  feature_list = example.feature_lists.feature_list.get(feature, None)\n",
        "  if feature_list is None:\n",
        "    return []\n",
        "  return [read_feature(feature) for feature in feature_list.feature]\n",
        "\n",
        "\n",
        "if show_raw_example:\n",
        "  def list_features(feature, f_type):\n",
        "    print(f\"\\n{feature}\")\n",
        "    data = get_features(sequence_example, f_type, feature)\n",
        "    for d in data:\n",
        "      print(data)\n",
        "\n",
        "  print(\"SequenceExample Features\")\n",
        "  for key in sequence_example.feature_lists.feature_list.keys():\n",
        "    print(key)\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"objects/track_id\")\n",
        "  labels = get_features(sequence_example, str, \"objects/label\")\n",
        "  print(f\"\\nNumber of tracked objects: {len(ids)}\")\n",
        "  for i, (id, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"points/track_id\")\n",
        "  labels = get_features(sequence_example, str, \"points/label\")\n",
        "  print(f\"\\nNumber of tracked points: {len(ids)}\")\n",
        "  for i, (id, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  ids = get_features(sequence_example, int, \"questions/type\")\n",
        "  print(f\"\\nNumber of tracked points: {len(ids)}\")\n",
        "  for i, (typeid, label) in enumerate(zip(ids, labels)):\n",
        "    print(f\"id: {id[0]:02} - {label[0]:14} (track {i:02})\")\n",
        "\n",
        "  FEATURE_LIST = [\n",
        "      (\"questions/type\", str),\n",
        "      (\"questions/multi_answer/answer_ids\", int),\n",
        "      (\"questions/subcategory\", str),\n",
        "      (\"questions/domain\", str),\n",
        "      (\"questions/reasoning\", str),\n",
        "      (\"questions/task_id\", str),\n",
        "      (\"questions/multi_choice/answer_id\", int),\n",
        "      (\"questions/tags\", str),\n",
        "      (\"questions\", str),\n",
        "      (\"actions/track_id\", int),\n",
        "  ]\n",
        "\n",
        "  for feature, data_type in FEATURE_LIST:\n",
        "    list_features(feature, data_type)"
      ],
      "metadata": {
        "id": "UVTnPUiS_-Kq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ParseExample Classes\n",
        "\n",
        "@chex.dataclass\n",
        "class ExampleMetadata:\n",
        "  \"\"\"Global data about this Perception Test example.\"\"\"\n",
        "  original_audio_sample_rate: float\n",
        "  original_audio_start_time: float\n",
        "  original_audio_num_samples: int\n",
        "\n",
        "  original_video_frame_rate: float\n",
        "  original_video_frames: int\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -> \"ExampleMetadata\":\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    audio_dims = example.context.feature[\"WAVEFORM/feature/dimensions\"]\n",
        "    [num_audio_samples] = audio_dims.int64_list.value\n",
        "    sample_rate_feat = example.context.feature[\"WAVEFORM/feature/sample_rate\"]\n",
        "    [audio_sample_rate] = sample_rate_feat.float_list.value\n",
        "    [audio_start_us] = get_features(\n",
        "        example, int, \"WAVEFORM/feature/timestamp\")\n",
        "    audio_start_us = audio_start_us[0]\n",
        "    num_video_frames = len(get_features(example, int, \"image/timestamp\"))\n",
        "    frame_rate_feat = example.context.feature[\"image/frame_rate\"]\n",
        "    [video_frame_rate] = frame_rate_feat.float_list.value\n",
        "    return cls(\n",
        "        original_audio_sample_rate=audio_sample_rate,\n",
        "        original_audio_start_time=audio_start_us / 1e6,\n",
        "        original_audio_num_samples=num_audio_samples,\n",
        "        original_video_frame_rate=video_frame_rate,\n",
        "        original_video_frames=num_video_frames)\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class PointTrack:\n",
        "  \"\"\"Single point tracked across a video.\"\"\"\n",
        "\n",
        "  points: chex.Array           # [frames, 2] -- y, x\n",
        "  frames: chex.Array           # [frames]\n",
        "  human_annotated: chex.Array  # [frames]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -> List[\"PointTrack\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    ys = get_features(example, float, \"points/points/y\")\n",
        "    xs = get_features(example, float, \"points/points/x\")\n",
        "    is_human = get_features(example, int, \"points/points/is_human_label\")\n",
        "    frames = get_features(example, int, \"points/points/frame\")\n",
        "    num_points = len(ys)\n",
        "    if not num_points:\n",
        "      return []\n",
        "    for f in [ys, xs, is_human, frames]:\n",
        "      if len(f) != num_points:\n",
        "        raise ValueError(\"Invalid number of point features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(points=np.stack([ys[b], xs[b]],\n",
        "                            axis=-1).clip(0, 1).astype(np.float32),\n",
        "            frames=np.asarray(frames[b], dtype=np.int32),\n",
        "            human_annotated=np.asarray(is_human[b], dtype=bool))\n",
        "        for b in range(num_points)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class BoxTrack:\n",
        "  \"\"\"Single object bounding boxes across a video.\"\"\"\n",
        "\n",
        "  track_id: int\n",
        "  label: str\n",
        "  boxes: chex.Array            # [frames, 4] -- y1, x1, y2, x2\n",
        "  frames: chex.Array           # [frames]\n",
        "  human_annotated: chex.Array  # [frames]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -> List[\"BoxTrack\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    track_ids = get_features(example, int, \"objects/track_id\")\n",
        "    track_labels = get_features(example, str, \"objects/label\")\n",
        "    y1s = get_features(example, float, \"objects/bounding_boxes/top_left_y\")\n",
        "    x1s = get_features(example, float, \"objects/bounding_boxes/top_left_x\")\n",
        "    y2s = get_features(example, float, \"objects/bounding_boxes/bottom_right_y\")\n",
        "    x2s = get_features(example, float, \"objects/bounding_boxes/bottom_right_x\")\n",
        "    is_human = get_features(\n",
        "        example, int, \"objects/bounding_boxes/is_human_label\")\n",
        "    frames = get_features(example, int, \"objects/bounding_boxes/frame\")\n",
        "    num_boxes = len(y1s)\n",
        "    if not num_boxes:\n",
        "      return []\n",
        "    for f in [y1s, x1s, y2s, x2s, is_human, frames, track_ids, track_labels]:\n",
        "      if len(f) != num_boxes:\n",
        "        raise ValueError(\"Invalid number of box features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(track_id=track_ids[b][0],\n",
        "            label=track_labels[b][0],\n",
        "            boxes=np.stack([y1s[b], x1s[b],\n",
        "                            y2s[b], x2s[b]], axis=-1\n",
        "                           ).clip(0, 1).astype(np.float32),\n",
        "            frames=np.asarray(frames[b], dtype=np.int32),\n",
        "            human_annotated=np.asarray(is_human[b], dtype=bool))\n",
        "        for b in range(num_boxes)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class AudioBox:\n",
        "  \"\"\"The start and end position of a sound within a video.\"\"\"\n",
        "\n",
        "  start_time: chex.Numeric  # Time since the start of the video in seconds.\n",
        "  end_time: chex.Numeric\n",
        "  audio_label: str  # E.g. \"Human:Speech\"\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -> List[\"AudioBox\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    starts = get_features(example, int, \"sounds/start_timestamp\")\n",
        "    ends = get_features(example, int, \"sounds/end_timestamp\")\n",
        "    labels = get_features(example, str, \"sounds/label\")\n",
        "    num_sounds = len(starts)\n",
        "    if not num_sounds:\n",
        "      return []\n",
        "    for f in [starts, ends, labels]:\n",
        "      if len(f) != num_sounds:\n",
        "        raise ValueError(\"Invalid number of audio features.\")\n",
        "    return [  # pylint: disable=g-complex-comprehension\n",
        "        cls(start_time=start[0] / 1e6, end_time=end[0] / 1e6,\n",
        "            audio_label=label[0].strip())\n",
        "        for start, end, label in zip(starts, ends, labels)\n",
        "    ]\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class MultipleChoiceQuestion:\n",
        "  \"\"\"Multiple choice questions.\"\"\"\n",
        "\n",
        "  text: str\n",
        "  options: List[str]\n",
        "  answer_id: int\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls,\n",
        "            example: tf.train.SequenceExample\n",
        "            ) -> List[\"MultipleChoiceQuestion\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    questions = get_features(example, str, \"questions/question\")\n",
        "    qtypes = get_features(example, str, \"questions/type\")\n",
        "    options = get_features(example, str, \"questions/multi_choice/options\")\n",
        "    answer_ids = get_features(example, int, \"questions/multi_choice/answer_id\")\n",
        "    if not questions or not options or not answer_ids:\n",
        "      return []\n",
        "    if not len(questions) == len(options) == len(answer_ids) == len(qtypes):\n",
        "      raise ValueError(\n",
        "          f\"Invalid example with #q={len(questions)}, \"\n",
        "          f\"#o={len(options)}, #a={len(answer_ids)}, #t={len(qtypes)}.\")\n",
        "    ret = []\n",
        "    for i in range(len(questions)):\n",
        "      [qtype] = qtypes[i]\n",
        "      if qtype != \"LANGUAGE\":\n",
        "        continue\n",
        "      [question] = questions[i]\n",
        "      if not options[i]:\n",
        "        continue\n",
        "      [answer_id] = answer_ids[i]\n",
        "      ret.append(cls(text=question, options=options[i], answer_id=answer_id))\n",
        "    return ret\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class GroundedObjectQuestion:\n",
        "  \"\"\"Grounded object detection questions.\"\"\"\n",
        "\n",
        "  text: str\n",
        "  box_track_ids: List[int]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls,\n",
        "            example: tf.train.SequenceExample,\n",
        "            available_box_tracks: Set[int],\n",
        "            ) -> List[\"GroundedObjectQuestion\"]:\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    questions = get_features(example, str, \"questions/question\")\n",
        "    qtypes = get_features(example, str, \"questions/type\")\n",
        "    answer_ids = get_features(example, int, \"questions/multi_answer/answer_ids\")\n",
        "    if not questions or not answer_ids:\n",
        "      return []\n",
        "    if not len(questions) == len(answer_ids) == len(qtypes):\n",
        "      raise ValueError(\n",
        "          f\"Invalid example with #q={len(questions)}, \"\n",
        "          f\"#a={len(answer_ids)}, #t={len(qtypes)}.\")\n",
        "    ret = []\n",
        "    for i in range(len(questions)):\n",
        "      [qtype] = qtypes[i]\n",
        "      if qtype != \"BOX\":\n",
        "        continue\n",
        "      [question] = questions[i]\n",
        "      if not answer_ids[i]:\n",
        "        continue\n",
        "      box_track_ids = [track_id for track_id in answer_ids[i]\n",
        "                       if track_id in available_box_tracks]\n",
        "      if not box_track_ids:\n",
        "        continue\n",
        "      ret.append(cls(text=question, box_track_ids=box_track_ids))\n",
        "    return ret\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class VideoAction:\n",
        "  \"\"\"The data class for the storing actions in video.\"\"\"\n",
        "  track_id: int\n",
        "  start_frame: int\n",
        "  end_frame: int\n",
        "  start_time: int\n",
        "  end_time: int\n",
        "  label: str\n",
        "\n",
        "  @classmethod\n",
        "  def parse(\n",
        "      cls,\n",
        "      example: tf.train.SequenceExample,\n",
        "  ) -> List[\"VideoAction\"]:\n",
        "    \"\"\"Parses input data for the temporal action localization task.\n",
        "\n",
        "    Args:\n",
        "      example: Initial tf.train.SequenceExample from which data is extracted.\n",
        "\n",
        "    Returns:\n",
        "      List of VideoAction objects. One per sample.\n",
        "    \"\"\"\n",
        "    track_ids = get_features(example, int, \"actions/track_id\")\n",
        "    start_frames = get_features(example, int, \"actions/start_frame\")\n",
        "    start_timestamps = get_features(example, int, \"actions/start_timestamp\")\n",
        "    end_frames = get_features(example, int, \"actions/end_frame\")\n",
        "    end_timestamps = get_features(example, int, \"actions/end_timestamp\")\n",
        "    labels = get_features(example, str, \"actions/label\")\n",
        "    result = []\n",
        "    data_iterator = zip(\n",
        "        track_ids,\n",
        "        start_frames,\n",
        "        end_frames,\n",
        "        start_timestamps,\n",
        "        end_timestamps,\n",
        "        labels,\n",
        "    )\n",
        "    for sample in data_iterator:\n",
        "      track_id, start_frame, end_frame, start_time, end_time, label = sample\n",
        "      result.append(\n",
        "          cls(\n",
        "              track_id=track_id,\n",
        "              start_frame=start_frame,\n",
        "              end_frame=end_frame,\n",
        "              start_time=start_time,\n",
        "              end_time=end_time,\n",
        "              label=label\n",
        "          ))\n",
        "    return result\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class ParsedExample:\n",
        "  \"\"\"Parsed Perception Test example.\"\"\"\n",
        "  metadata: ExampleMetadata\n",
        "  video_frames: Optional[chex.Array]  # [num_frames, h, w, c]\n",
        "  video_features: Optional[chex.Array]\n",
        "  video_actions: Optional[Sequence[VideoAction]]\n",
        "  audio_wav: Optional[chex.Array]     # [num_samples]\n",
        "  point_tracks: List[PointTrack]\n",
        "  box_tracks: List[BoxTrack]\n",
        "  audio_boxes: List[AudioBox]\n",
        "  grounded_object_questions: List[GroundedObjectQuestion]\n",
        "  multiple_choice_questions: List[MultipleChoiceQuestion]\n",
        "\n",
        "  @classmethod\n",
        "  def parse(cls, example: tf.train.SequenceExample) -> \"ParsedExample\":\n",
        "    \"\"\"Parses a tf.train.SequenceExample.\"\"\"\n",
        "    metadata = ExampleMetadata.parse(example)\n",
        "    audio_wav = load_audio(example)\n",
        "    video_frames = load_video(example)\n",
        "    point_tracks = PointTrack.parse(example)\n",
        "    box_tracks = BoxTrack.parse(example)\n",
        "    audio_boxes = AudioBox.parse(example)\n",
        "    # Sometimes box/point tracks refer to frames outside the video.\n",
        "    max_frame = metadata.original_video_frames - 1\n",
        "    for track in box_tracks + point_tracks:\n",
        "      max_frame = max(max_frame, track.frames[-1])\n",
        "    if max_frame >= metadata.original_video_frames:\n",
        "      logging.info(\"Video had %d frames, but annotation referenced frame %d.\",\n",
        "                   metadata.original_video_frames, max_frame + 1)\n",
        "      metadata.original_video_frames = max_frame + 1\n",
        "    grounded_object_questions = GroundedObjectQuestion.parse(\n",
        "        example,\n",
        "        available_box_tracks=set([b.track_id for b in box_tracks]))\n",
        "    multiple_choice_questions = MultipleChoiceQuestion.parse(example)\n",
        "    video_actions = VideoAction.parse(example)\n",
        "    video_features = get_features(example, float, \"action/features\")\n",
        "    return cls(metadata=metadata,\n",
        "               video_frames=video_frames,\n",
        "               video_features=video_features,\n",
        "               audio_wav=audio_wav,\n",
        "               point_tracks=point_tracks,\n",
        "               box_tracks=box_tracks,\n",
        "               audio_boxes=audio_boxes,\n",
        "               grounded_object_questions=grounded_object_questions,\n",
        "               multiple_choice_questions=multiple_choice_questions,\n",
        "               video_actions=video_actions)"
      ],
      "metadata": {
        "id": "6HL-NRwKADxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data processing functions\n",
        "\n",
        "def load_audio(\n",
        "    example: tf.train.SequenceExample\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Returns the audio sample.\"\"\"\n",
        "  pad_left = 0\n",
        "  [audio_start_us] = get_features(example, int, \"WAVEFORM/feature/timestamp\")\n",
        "  audio_start_us = audio_start_us[0]\n",
        "  if audio_start_us > 0:\n",
        "    sample_rate_feat = example.context.feature[\"WAVEFORM/feature/sample_rate\"]\n",
        "    [audio_sample_rate] = sample_rate_feat.float_list.value\n",
        "    pad_left = round(audio_sample_rate * audio_start_us * 1e-6)\n",
        "  [samples] = get_features(example, float, \"WAVEFORM/feature/floats\")\n",
        "  ret = np.empty(pad_left + len(samples), dtype=np.float32)\n",
        "  ret[:pad_left] = 0\n",
        "  ret[pad_left:] = samples\n",
        "  return ret\n",
        "\n",
        "\n",
        "def load_video(\n",
        "    example: tf.train.SequenceExample\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Returns the video from a given example.\"\"\"\n",
        "  frame_features = get_features(example, bytes, \"image/encoded\")\n",
        "  num_frames = len(frame_features)\n",
        "\n",
        "  for t, frame_index in enumerate(range(num_frames)):\n",
        "    with io.BytesIO(frame_features[frame_index][0]) as f:\n",
        "      input_frame = PIL.Image.open(f)\n",
        "      if t == 0:\n",
        "        out_height = input_frame.height\n",
        "        out_width = input_frame.width\n",
        "        output_frames = np.empty(\n",
        "            (num_frames, out_height, out_width, 3), dtype=np.uint8)\n",
        "      output_frames[t] = np.frombuffer(\n",
        "          input_frame.tobytes(), dtype=np.uint8\n",
        "      ).reshape((out_height, out_width, 3))\n",
        "  return output_frames\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "cBneloyBAO7g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Drawing and display utilities\n",
        "\n",
        "def display_video(frames, fps=30):\n",
        "  # Create and display temporary video from numpy array frames\n",
        "  # format (num_frames, height, width, channels)\n",
        "  imageio.mimwrite('tmp_video_display.mp4', frames, fps=fps); \n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(frame):\n",
        "  # Display a frame, converting from RGB to BGR for cv2.\n",
        "  cv2_imshow(frame[:, :, ::-1])\n",
        "\n",
        "\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "  # Generate random colormaps for visualizing different objects and points.\n",
        "  colors = []\n",
        "  for i in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = i / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "COLORS = get_colors(num_colors=100)\n",
        "\n",
        "\n",
        "def try_different_dataset_notice(current, missing, suggestion):\n",
        "  msg = f\"\\n*** The current example from the `{current}` dataset does not \" \\\n",
        "        f\"contain any {missing} data.\\n\\n\" \\\n",
        "        f\"*** Try loading the `{suggestion}` dataset to \" \\\n",
        "        f\"visualise {missing} data.\" \n",
        "  print(f\"\\x1b[31m{msg}\\x1b[0m\")\n",
        "\n",
        "\n",
        "def paint_box(video: List[np.ndarray],\n",
        "              track: BoxTrack,\n",
        "              color: Tuple[int, int, int] = (255, 0, 0)):\n",
        "  num_frames, height, width, _ = video.shape\n",
        "  for box, frame_idx, human in zip(\n",
        "      track.boxes, track.frames, track.human_annotated):\n",
        "    if human:\n",
        "      label = f\"{track.label}*\"\n",
        "    else:\n",
        "      label = track.label\n",
        "    name = f'{track.track_id} : {label}'\n",
        "    frame = np.array(video[frame_idx])\n",
        "    y1 = int(round(box[0] * height))\n",
        "    x1 = int(round(box[1] * width))\n",
        "    y2 = int(round(box[2] * height))\n",
        "    x2 = int(round(box[3] * width))\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2), color=color, thickness=2)\n",
        "    frame = cv2.putText(frame, name, (x1, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.75, color, 2)\n",
        "    video[frame_idx] = frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_boxes(video, tracks: List[BoxTrack]):\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_box(video, track, COLORS[i])\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_point(\n",
        "    video: List[np.ndarray],\n",
        "    track: PointTrack,\n",
        "    color: Tuple[int, int, int] = (255, 0, 0),\n",
        "):\n",
        "  num_frames, height, width, _ = video.shape\n",
        "  for p, frame_idx, human in zip(\n",
        "      track.points, track.frames, track.human_annotated):\n",
        "    frame = video[frame_idx]\n",
        "    x = int(round(p[1] * width))\n",
        "    y = int(round(p[0] * height))\n",
        "    frame = cv2.circle(frame, (x, y), radius=10, color=color, thickness=-1)\n",
        "    video[frame_idx] = frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_points(video, tracks: List[PointTrack]):\n",
        "  for i, track in enumerate(tracks):\n",
        "    video = paint_point(video, track, COLORS[i])\n",
        "  return video\n"
      ],
      "metadata": {
        "id": "I8GWr-INAWN5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Parse an example to ParsedExample\n",
        "example = ParsedExample.parse(sequence_example)"
      ],
      "metadata": {
        "id": "uq4fW7hTAYyv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_original_video = True  #@param {type: \"boolean\"}\n",
        "if show_original_video:\n",
        "  display_video(example.video_frames)"
      ],
      "metadata": {
        "id": "3dTsITNjAc7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box Tracks"
      ],
      "metadata": {
        "id": "guZL3-m8CfYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Draw annotated bounding boxes on video\n",
        "\n",
        "tmp_vid = example.video_frames.copy()\n",
        "show_all_tracks = True  #@param {type: \"boolean\"}\n",
        "show_track = 0  #@param {type: \"integer\"}\n",
        "if show_all_tracks:\n",
        "  _ = paint_boxes(tmp_vid, example.box_tracks)\n",
        "else:\n",
        "  _ = paint_box(tmp_vid, example.box_tracks[show_track], \n",
        "                COLORS[show_track])\n",
        "display_video(tmp_vid)"
      ],
      "metadata": {
        "id": "DvmstqjyAf6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point Tracks"
      ],
      "metadata": {
        "id": "GtPwUevJCuuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Show video with points overlayed.\n",
        "\n",
        "tmp_vid = example.video_frames.copy()\n",
        "show_all_tracks = True  #@param {type: \"boolean\"}\n",
        "show_track = 0  #@param {type: \"integer\"}\n",
        "\n",
        "num_point_tracks = len(example.point_tracks)\n",
        "if num_point_tracks:\n",
        "  if show_all_tracks:\n",
        "    _ = paint_points(tmp_vid, example.point_tracks)\n",
        "  else:\n",
        "    _ = paint_point(tmp_vid, example.point_tracks[show_track], \n",
        "                  COLORS[show_track])\n",
        "  display_video(tmp_vid)\n",
        "else:\n",
        "  try_different_dataset_notice(dataset, \"point tracking\", \"points_oss\")\n"
      ],
      "metadata": {
        "id": "ZUbUSfVuBaY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actions"
      ],
      "metadata": {
        "id": "Vi6PMvvoFI-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown List video actions\n",
        "human_readable_text = True # @param {type: \"boolean\"}\n",
        "\n",
        "action_labels = []\n",
        "action_start_times = []\n",
        "action_end_times = []\n",
        "print(f\"Track id\\tstart_frame\\tend_frame\\tstart_time\\t\\tend_time\\tLabel\")\n",
        "if example.video_actions:\n",
        "  if human_readable_text:\n",
        "    for va in example.video_actions:\n",
        "      action_labels.append(str(va.label))\n",
        "      action_start_times.append(va.start_time)\n",
        "      action_end_times.append(va.end_time)\n",
        "      print(f\"{va.track_id[0]}\\t\\t{va.start_frame[0]}\\t\\t{va.end_frame[0]}\"\n",
        "            f\"\\t\\t{va.start_time[0]:8}\\t\\t{va.end_time[0]:8}\\t{va.label[0]}\")\n",
        "else:\n",
        "  try_different_dataset_notice(dataset, \"video actions\", \"base_oss\")\n",
        "\n",
        "action_start_times = np.array(action_start_times).squeeze()\n",
        "action_end_times = np.array(action_end_times).squeeze()"
      ],
      "metadata": {
        "id": "ENweEjiUFKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sounds"
      ],
      "metadata": {
        "id": "UwuTw02_F0yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Audio\n",
        "#markdown Extract the audio for the video.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from scipy.io.wavfile import write\n",
        "sample_rate = int(example.metadata.original_audio_sample_rate)\n",
        "\n",
        "wav = (example.audio_wav * 2**15).astype(int)\n",
        "write('test.wav', sample_rate, wav)\n",
        "Audio(wav, rate=sample_rate)"
      ],
      "metadata": {
        "id": "GCISnCu9F1s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sounds events\n",
        "#@markdown Print the sound events for a sample video.\n",
        "\n",
        "audio_labels = []\n",
        "audio_start_times = []\n",
        "audio_end_times = []\n",
        "print(f\"start\\tend\\tlabel\")\n",
        "if example.audio_boxes:\n",
        "  for audio in example.audio_boxes:\n",
        "    audio_labels.append(str(audio.audio_label))\n",
        "    audio_start_times.append(audio.start_time)\n",
        "    audio_end_times.append(audio.end_time)\n",
        "    print(f\"{audio.start_time:02.4f}\\t{audio.end_time:02.4f}\"\n",
        "          f\"\\t{audio.audio_label}\")\n",
        "\n",
        "audio_start_times = np.array(audio_start_times)\n",
        "audio_end_times = np.array(audio_end_times)"
      ],
      "metadata": {
        "id": "0qS7I-B7F5lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise timeline of events"
      ],
      "metadata": {
        "id": "e3Dza3pKF_Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Plot a timeline with the audio, sound events, action events and some frames.\n",
        "plt.figure(figsize=(14, 15))\n",
        "\n",
        "# Plot WAV\n",
        "plt.subplot(4,1,1)\n",
        "plt.title(\"Audio\")\n",
        "librosa.display.waveshow(example.audio_wav, sr=sample_rate)\n",
        "\n",
        "# Strip of frames\n",
        "plt.subplot(4,1,2)\n",
        "plt.title(\"Video Frames\")\n",
        "f_size = example.video_frames[0].shape\n",
        "small = tuple(reversed((np.array(f_size[:2]) / 4).astype(int)))\n",
        "strip = None\n",
        "num_frames = example.metadata.original_video_frames\n",
        "for i in range(0, num_frames, int(num_frames/4)):\n",
        "  frame = cv2.resize(example.video_frames[i], small)\n",
        "  if strip is None:\n",
        "    strip = np.array(frame)\n",
        "  else:\n",
        "    strip = np.concatenate([strip, frame], axis=1)\n",
        "plt.imshow(strip)\n",
        "\n",
        "# Plot audio events\n",
        "plt.subplot(4,1,3)\n",
        "plt.title(\"Audio Events\")\n",
        "plt.barh(range(len(audio_start_times)),\n",
        "         audio_end_times-audio_start_times,\n",
        "         left=audio_start_times)\n",
        "plt.yticks(range(len(audio_start_times)), audio_labels)\n",
        "\n",
        "# Plot video events\n",
        "plt.subplot(4,1,4)\n",
        "plt.title(\"Action Events\")\n",
        "plt.barh(range(len(action_start_times)),\n",
        "         action_end_times-action_start_times,\n",
        "         left=action_start_times)\n",
        "plt.yticks(range(len(action_start_times)), action_labels)\n",
        "\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "K32UPHBfF_20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple choice VQA"
      ],
      "metadata": {
        "id": "3PtTrNBkHPxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Load and print an example from the `multiple choice questions` dataset. \n",
        "human_readable = True # @param {type: \"boolean\"}\n",
        "if example.multiple_choice_questions:\n",
        "  for mcq in example.multiple_choice_questions:\n",
        "    if human_readable:\n",
        "      print(mcq.text)\n",
        "      for i, o in enumerate(mcq.options):\n",
        "        if i == mcq.answer_id:\n",
        "          answer = \" <---- ANSWER\"\n",
        "        else:\n",
        "          answer = \"\"\n",
        "        print(f\"  {o}{answer}\")\n",
        "      print(\"\")\n",
        "    else:\n",
        "      print(mcq)\n",
        "else:\n",
        "  try_different_dataset_notice(dataset,\n",
        "                               \"multiple choice visual question and answer\",\n",
        "                               \"base_oss\")"
      ],
      "metadata": {
        "id": "0CRtnxnOHO9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}