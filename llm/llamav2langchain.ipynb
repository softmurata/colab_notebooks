{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMAWb4siLGIi3iJ7XvF3pk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/llamav2langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "H5I1XkCoulC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MrvntnqozF_"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install langchain\n",
        "!pip install faiss-gpu\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get Weights"
      ],
      "metadata": {
        "id": "6moXP9oqumDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin -P /content"
      ],
      "metadata": {
        "id": "fIa-LunBpKZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "# ログレベルの設定\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)\n",
        "\n",
        "# ドキュメントの読み込み\n",
        "with open(\"soccer.txt\") as f:\n",
        "    test_all = f.read()\n",
        "\n",
        "# チャンクの分割\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,  # チャンクの最大文字数\n",
        "    chunk_overlap=20,  # オーバーラップの最大文字数\n",
        ")\n",
        "texts = text_splitter.split_text(test_all)\n",
        "\n",
        "# チャンクの確認\n",
        "print(len(texts))\n",
        "for text in texts:\n",
        "    print(text[:10].replace(\"\\n\", \"\\\\n\"), \":\", len(text))\n",
        "\n",
        "\n",
        "# インデックスの作成\n",
        "index = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\"),\n",
        ")\n",
        "index.save_local(\"storage\")\n",
        "\n",
        "# インデックスの読み込み\n",
        "# index = FAISS.load_local(\n",
        "#    \"storage\", HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "# )\n",
        "\n",
        "# LLMの準備\n",
        "# llm = OpenAI(temperature=0, verbose=True)\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
        "    input={\n",
        "        \"max_tokens\": 32,\n",
        "        \"stop\": [\"System:\", \"User:\", \"Assistant:\", \"\\n\"],\n",
        "    },\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# 質問応答チェーンの作成\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.as_retriever(search_kwargs={\"k\": 4}),\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "6Yp9TZkzpMZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run QA Chain"
      ],
      "metadata": {
        "id": "FNTFR17XvcWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"A1:\", qa_chain.run(\"Tell us about the changes in the Japanese national team's play style.\"))"
      ],
      "metadata": {
        "id": "qNGvzPT_p8KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"A2:\", qa_chain.run(\"I want you to tell me the results of the Japan national soccer team at the World Cup\"))"
      ],
      "metadata": {
        "id": "2VdFUujytu32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "翻訳"
      ],
      "metadata": {
        "id": "mjsQHzDvqxlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-1.3B\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-1.3B\")\n",
        "\n",
        "article = \"UN Chief says there is no military solution in Syria\"\n",
        "inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"jpn_Jpan\"], max_length=30\n",
        ")\n",
        "ret = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(ret)"
      ],
      "metadata": {
        "id": "QNVuqXf6qxPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eng_Latn 英語に変換するには\n",
        "prompt = \"おはようございます。今日はいい天気ですね。\"\n",
        "inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"], max_length=30\n",
        ")\n",
        "ret = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(ret)"
      ],
      "metadata": {
        "id": "xnMpdIzVrndO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}