{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO73TkJexHfDtxp5YEfhQmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/langchain_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "Kbb_93bW8eV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdmXd8vs78a1"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n",
        "!pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
        "!pip install -qq -U accelerate bitsandbytes xformers einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "YVQJUyZ-8mQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "import langchain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
        "\n",
        "print(langchain.__version__)\n",
        "\n",
        "### Multi-document retriever\n",
        "from langchain.vectorstores import Chroma, FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA, VectorDBQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "2KjUCEqo8Pi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and create pipeline"
      ],
      "metadata": {
        "id": "AelbAhUdI5EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google colab freeではllamaしか動かないと思います！\n",
        "class CFG:\n",
        "    model_name = 'llama' # wizardlm, llama, bloom, falcon\n",
        "\n",
        "def get_model(model = CFG.model_name):\n",
        "\n",
        "    print('\\nDownloading model: ', model, '\\n\\n')\n",
        "\n",
        "    if CFG.model_name == 'wizardlm':\n",
        "        tokenizer = AutoTokenizer.from_pretrained('TheBloke/wizardLM-7B-HF')\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained('TheBloke/wizardLM-7B-HF',\n",
        "                                                     load_in_8bit=True,\n",
        "                                                     device_map='auto',\n",
        "                                                     torch_dtype=torch.float16,\n",
        "                                                     low_cpu_mem_usage=True\n",
        "                                                    )\n",
        "        max_len = 1024\n",
        "        task = \"text-generation\"\n",
        "        T = 0\n",
        "\n",
        "    elif CFG.model_name == 'llama':\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(\"aleksickx/llama-7b-hf\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"aleksickx/llama-7b-hf\", unk_token=\"<unk>\",\n",
        "                                                    bos_token=\"<s>\",\n",
        "                                                    eos_token=\"</s>\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"aleksickx/llama-7b-hf\",\n",
        "                                                     load_in_8bit=True,\n",
        "                                                     device_map='auto',\n",
        "                                                     torch_dtype=torch.float16,\n",
        "                                                     low_cpu_mem_usage=True,\n",
        "                                                    )\n",
        "        max_len = 1024\n",
        "        task = \"text-generation\"\n",
        "        T = 0.1\n",
        "\n",
        "    elif CFG.model_name == 'bloom':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-7b1\",\n",
        "                                                     load_in_8bit=True,\n",
        "                                                     device_map='auto',\n",
        "                                                     torch_dtype=torch.float16,\n",
        "                                                     low_cpu_mem_usage=True,\n",
        "                                                    )\n",
        "        max_len = 1024\n",
        "        task = \"text-generation\"\n",
        "        T = 0\n",
        "\n",
        "    elif CFG.model_name == 'falcon':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\",\n",
        "                                                     load_in_8bit=True,\n",
        "                                                     device_map='auto',\n",
        "                                                     torch_dtype=torch.float16,\n",
        "                                                     low_cpu_mem_usage=True,\n",
        "                                                     trust_remote_code=True\n",
        "                                                    )\n",
        "        max_len = 1024\n",
        "        task = \"text-generation\"\n",
        "        T = 0\n",
        "\n",
        "    else:\n",
        "        print(\"Not implemented model (tokenizer and backbone)\")\n",
        "\n",
        "    return tokenizer, model, max_len, task, T"
      ],
      "metadata": {
        "id": "YpwQe9wc81lF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model, max_len, task, T = get_model(CFG.model_name)"
      ],
      "metadata": {
        "id": "nSjXNNYH85FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    task=task,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=max_len,\n",
        "    temperature=T,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "_1hvdAkz9APX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load documents and create db"
      ],
      "metadata": {
        "id": "0GuLN7NMI8VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docsというフォルダを作成して適当な論文をそのフォルダ内に追加してください\n",
        "loader = DirectoryLoader('/content/docs',\n",
        "                         glob=\"./*.pdf\",\n",
        "                         loader_cls=PyPDFLoader,\n",
        "                         show_progress=True,\n",
        "                         use_multithreading=True)\n",
        "\n",
        "documents = loader.load()\n",
        "# clean document\n",
        "for i in range(len(documents)):\n",
        "    documents[i].page_content = documents[i].page_content.replace('\\t', ' ')\\\n",
        "                                                         .replace('\\n', ' ')\\\n",
        "                                                         .replace('       ', ' ')\\\n",
        "                                                         .replace('      ', ' ')\\\n",
        "                                                         .replace('     ', ' ')\\\n",
        "                                                         .replace('    ', ' ')\\\n",
        "                                                         .replace('   ', ' ')\\\n",
        "                                                         .replace('  ', ' ')"
      ],
      "metadata": {
        "id": "jd_r043OCYqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1].page_content"
      ],
      "metadata": {
        "id": "qMXJy5S3DCw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitter"
      ],
      "metadata": {
        "id": "Rqbw_fNZDLnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G_IqZ9jDMcn",
        "outputId": "fb7f4962-dca3-4dcf-ef69-fb9bcc0a6715"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create vector database"
      ],
      "metadata": {
        "id": "gVDxU4yHDUHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'arxiv-vectordb-chroma'\n",
        "\n",
        "### download embeddings model\n",
        "instruct_ml_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # hkunlp/instructor-xl, cuda\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=instruct_ml_name,\n",
        "                                                      model_kwargs={\"device\": \"cpu\"})\n",
        "\n",
        "### create embeddings and DB\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=instructor_embeddings,\n",
        "                                 persist_directory=persist_directory,\n",
        "                                 collection_name='hp_books')\n",
        "\n",
        "\n",
        "\n",
        "### persist Chroma database\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "3394pbOZDP4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QA Retriever"
      ],
      "metadata": {
        "id": "IcLsxbVgFqnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3, \"search_type\" : \"similarity\"})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       return_source_documents=True,\n",
        "                                       verbose=False)"
      ],
      "metadata": {
        "id": "TGnUjCSzFrsh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post process output"
      ],
      "metadata": {
        "id": "Y_DmUuKhF9Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "def llm_ans(query):\n",
        "    llm_response = qa_chain(query)\n",
        "    ans = process_llm_response(llm_response)\n",
        "    return ans"
      ],
      "metadata": {
        "id": "A9IAda66F_II"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QA RUN"
      ],
      "metadata": {
        "id": "U-yM74aSJuEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why ShapeNet is better than other AI models?\"\n",
        "llm_ans(query)"
      ],
      "metadata": {
        "id": "fndZWn1zGE9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}