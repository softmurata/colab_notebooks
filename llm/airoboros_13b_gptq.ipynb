{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTHqykMSwfIkn80vdIMM+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/airoboros_13b_gptq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKwBnkd-aKlv"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "GC4R6rTPaX-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import argparse\n",
        "\n",
        "model_name_or_path = \"TheBloke/airoboros-13b-gpt4-GPTQ\"\n",
        "model_basename = \"airoboros-13b-gpt4-GPTQ-4bit-128g.no-act.order\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "73FVzYa-aTuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference with model generate"
      ],
      "metadata": {
        "id": "3P9qkS3qaZH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "USER: I want you to act as a programmer. I will provide the description of an image, you should output the corresponding layout of this image. Each object in the image is one rectangle or square box in the layout and size of boxes should be as large as possible comapre to the image size. The size of the image is 512 * 512 You should return each object and the corresponding coordinate of its boxes.\n",
        "the prompt: \\\"three cats in the field\\\", \n",
        "ASSISTANT :\n",
        "cat: (51, 82, 399, 279)\n",
        "cat: (288, 128, 472, 299)\n",
        "cat: (27, 355, 418, 494)\n",
        "\n",
        "USER:\n",
        "the prompt: \\\"a cat on the left of a dog on the road\\\"\n",
        "ASSISTANT:\n",
        "cat: (63, 196, 223, 394)\n",
        "dog: (289, 131, 466, 360)\n",
        "\n",
        "USER:\n",
        "the prompt: \\\"four balls in the room\\\"\n",
        "ASSISTANT:\n",
        "ball: (72, 81, 254, 243)\n",
        "ball: (316, 44, 483, 218)\n",
        "ball: (287, 295, 453, 462)\n",
        "ball: (50, 323, 196, 484)\n",
        "\n",
        "USER:\n",
        "the prompt: \\\"A donut to the right of a toilet\\\"\n",
        "ASSISTANT:\n",
        "donut: (287, 140, 467, 335)\n",
        "toilet: (31, 97, 216, 286)\n",
        "USER:\n",
        "the prompt: \\\"A cat sitting on the top of a car\\\"\n",
        "ASSISTANT:\n",
        "car: (94, 236, 414, 407)\n",
        "cat: (124, 139, 273, 252)\n",
        "\n",
        "USER:\n",
        "the prompt: \\\"A dog underneath a tree\\\"\n",
        "ASSISTANT:\n",
        "dog: (133, 232, 308, 445)\n",
        "tree: (121, 29, 324, 258)\n",
        "\n",
        "USER:\n",
        "the prompt: \\\"a small ball is put on the top of a box on the table. there is a red vase on the right of the box on the table\\\"\n",
        "ASSISTANT:\n",
        "small ball: (92, 30, 165, 134)\n",
        "box: (93, 132 , 205, 324, 310)\n",
        "red vase: (214, 164, 297, 301)\n",
        "table: (36, 259, 418, 463)\n",
        "USER:\n",
        "the prompt: \\\"a table at the center of room with four chairs. There is a sofa on the right side of the room\\\"\n",
        "\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt_template):])"
      ],
      "metadata": {
        "id": "Gt6KrG2BabPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation pipeline"
      ],
      "metadata": {
        "id": "6-NxOO2yad2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me about AI\"\n",
        "prompt_template=f'''USER: {prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'][len(prompt_template):])"
      ],
      "metadata": {
        "id": "KVpNtKveab29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}