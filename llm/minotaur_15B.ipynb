{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuAlRPucaLE/BhfwiTab04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/minotaur_15B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "wrza6UoSZoW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR-aJQtwYFAh"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "1doHrBKYaGC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "Ko97z9BHYXl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "model_name_or_path = \"TheBloke/minotaur-15B-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=False,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "NkSqq28ZYYgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "vQBim7ZgYZu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: check the prompt template is correct for this model.\n",
        "prompt = \"Please create the 20 minute presentation about AI.\"\n",
        "prompt_template=f'''USER: {prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=2048)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "2hzcBe73Yc7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vujzs7ImZ92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization"
      ],
      "metadata": {
        "id": "IrjKAyQuak5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Please summarize the following context within 100 words. Context: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers,\n",
        "benefiting both academia and society. While existing approaches have focused\n",
        "on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs\n",
        "with limited resources. In this work, we propose a new optimizer, LOw-Memory\n",
        "Optimization (LOMO), which fuses the gradient computation and the parameter\n",
        "update in one step to reduce memory usage. By integrating LOMO with existing\n",
        "memory saving techniques, we reduce memory usage to 10.8% compared to the\n",
        "standard approach (DeepSpeed solution). Consequently, our approach enables the\n",
        "full parameter fine-tuning of a 65B model on a single machine with 8×RTX 3090,\n",
        "each with 24GB memory.1\n",
        "1 INTRODUCTION\n",
        "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), demonstrating remarkable abilities such as emergence and grokking (Wei et al., 2022), pushing model size\n",
        "to become larger and larger. However, training these models with billions of parameters, such as\n",
        "those with 30B to 175B parameters, raises the bar for NLP research. Tuning LLMs often requires\n",
        "expensive GPU resources, such as 8×80GB devices, making it difficult for small labs and companies\n",
        "to participate in this area of research.\n",
        "Recently, parameter-efficient fine-tuning methods (Ding et al., 2022), such as LoRA (Hu et al., 2022)\n",
        "and Prefix-tuning (Li & Liang, 2021), provide solutions for tuning LLMs with limited resources.\n",
        "However, these methods do not offer a practical solution for full parameter fine-tuning, which has\n",
        "been acknowledged as a more powerful approach than parameter-efficient fine-tuning (Ding et al.,\n",
        "2022; Sun et al., 2023). In this work, we aim to explore techniques for accomplishing full parameter\n",
        "fine-tuning in resource-limited scenarios.\n",
        "We analyze the four aspects of memory usage in LLMs, namely activation, optimizer states, gradient\n",
        "tensor and parameters, and optimize the training process in three folds: 1) We rethink the functionality of an optimizer from an algorithmic perspective and find that SGD is a good replacement in terms\n",
        "of fine-tuning full parameter for LLMs. This allows us to remove the entire part of optimizer states\n",
        "since SGD does not store any intermediate state (Sec-3.1). 2) Our proposed optimizer, LOMO as illustrated in Figure 1, reduces the memory usage of gradient tensors to O(1), equivalent to the largest\n",
        "gradient tensor’s memory usage (Sec-3.2). 3) To stabilize mix-precision training with LOMO, we\n",
        "integrate gradient normalization, loss scaling, and transition certain computations to full precision\n",
        "during training (Sec-3.3).\"\"\"\n",
        "\n",
        "prompt_template=f'''USER: {prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=4096)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt_template):])"
      ],
      "metadata": {
        "id": "DC25qH4oake3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"最新のAIについてプレゼンを書いて\"\n",
        "\n",
        "prompt_template=f'''USER: {prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=4096)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt_template):])"
      ],
      "metadata": {
        "id": "_162E2_ufy7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}