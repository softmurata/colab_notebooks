{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoOGybbY6H4H3M+n5u64U7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/opencalm3b_finetuning_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lora Instruct"
      ],
      "metadata": {
        "id": "mOSQXXlaFwCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "DmhemdX9JmL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/leehanchung/lora-instruct\n",
        "%cd lora-instruct\n",
        "!pip install poetry\n",
        "!poetry export --without-hashes --with dev --output requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "DR3ToDGOE6UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning"
      ],
      "metadata": {
        "id": "BJYF-1ZyJoRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lora-instruct\n",
        "!python finetune.py \\\n",
        "    --base_model 'cyberagent/open-calm-3b' \\\n",
        "    --data_path 'kunishou/databricks-dolly-15k-ja' \\\n",
        "    --output_dir './lora-alpaca' \\\n",
        "    --batch_size 16 \\\n",
        "    --micro_batch_size 2 \\\n",
        "    --num_epochs 3 \\\n",
        "    --learning_rate 3e-4 \\\n",
        "    --cutoff_len 512 \\\n",
        "    --val_set_size 2000 \\\n",
        "    --lora_r 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --train_on_inputs \\\n",
        "    --add_eos_tokens \\\n",
        "    --group_by_length\n"
      ],
      "metadata": {
        "id": "Yxp0wVXuFBfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7JTZy6DV2Hqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/lora-instruct/lora-alpaca /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "DK5yPKm92Ihl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference\n",
        "\n",
        "https://note.com/npaka/n/n227771a2527c"
      ],
      "metadata": {
        "id": "R4EwLzexZq97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"cyberagent/open-calm-3b\"\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"cyberagent/open-calm-3b\", \n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "06davBr3ZsKW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# PEFTモデルの追加\n",
        "model = PeftModel.from_pretrained(model, \"/content/lora-instruct/lora-alpaca\", device_map=\"auto\")\n",
        "model = model.to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "GDwAREHdzuZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wC-iN_9X0Tzn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = generate_prompt(\"明日の天気を教えてほしい。\")\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model.generate(\n",
        "      **inputs, \n",
        "      max_new_tokens=128, \n",
        "      do_sample=True, \n",
        "      temperature=0.7, \n",
        "      top_p=0.7, \n",
        "      top_k=50, \n",
        "      return_dict_in_generate=True\n",
        "  )\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "\n",
        "# 確認\n",
        "print(output_str)"
      ],
      "metadata": {
        "id": "_VY5d8J_z4aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpaca lora finetuning"
      ],
      "metadata": {
        "id": "hjHNu3LYE4lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tloen/alpaca-lora.git\n",
        "%cd alpaca-lora\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "c200k23EJS68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "batch_size=128,\n",
        "micro_batch_size=2,  # gradient_accumulation_steps=64\n",
        "num_epochs=3,\n",
        "learning_rate=3e-4,\n",
        "lora_r=8,\n",
        "lora_alpha=16,\n",
        "lora_dropout=5e-2,\n",
        "lora_target_modules=['query_key_value'],\n",
        "train_on_inputs=True,\n",
        "add_eos_tokens=True,\n",
        "warmup_steps=100,\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uvfPJ3158Qz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune.py \\\n",
        "    --base_model 'cyberagent/open-calm-1b' \\\n",
        "    --data_path 'kunishou/databricks-dolly-15k-ja' \\\n",
        "    --output_dir './lora-alpaca' \\\n",
        "    --batch_size 4 \\\n",
        "    --micro_batch_size 1 \\\n",
        "    --num_epochs 3 \\\n",
        "    --learning_rate 3e-4 \\\n",
        "    --cutoff_len 512 \\\n",
        "    --val_set_size 2000 \\\n",
        "    --lora_r 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --lora_target_modules 'query_key_value' \\\n",
        "    --train_on_inputs \\\n",
        "    --add_eos_tokens \\\n",
        "    --group_by_length"
      ],
      "metadata": {
        "id": "hqhU2WDa7N3t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}