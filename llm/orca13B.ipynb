{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUU5AqQeZsHQuTH2Gc7zTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/orca13B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "mVkSXODF8mUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKevZZEC8bi-"
      },
      "outputs": [],
      "source": [
        "!pip install -q auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "CGrddnGn8p2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import argparse\n",
        "\n",
        "model_name_or_path = \"TheBloke/orca_mini_13B-GPTQ\"\n",
        "model_basename = \"orca-mini-13b-GPTQ-4bit-128g.no-act.order\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=False,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "-SCWbRKy8h4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference with decode"
      ],
      "metadata": {
        "id": "PwqC6TLY8ufB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: check the prompt template is correct for this model.\n",
        "prompt = \"Please translate the following English context into Japanese. \\ Context: In the realm of large language models (LLMs), there has been a constant pursuit to enhance the capabilities of smaller models without compromising their efficiency. The traditional approach has been to use imitation learning, where smaller models learn from the outputs generated by large foundation models (LFMs). However, this approach has been marred by several challenges, including limited imitation signals from shallow LFM outputs, small-scale homogeneous training data, and a lack of rigorous evaluation. This often leads to smaller models imitating the style but not the reasoning process of LFMs. The paper Orca: Progressive Learning from Complex Explanation Traces of GPT-4 introduces Orca, a 13-billion parameter model designed to imitate the reasoning process of large foundation models (LFMs) such as GPT-4. Unlike traditional large language models (LLMs), Orca employs a unique training approach that combines progressive learning and teacher assistance to overcome the capacity gap between smaller student models and their larger counterparts.\"\n",
        "prompt_template=f'''USER: {prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=1024)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt_template):])"
      ],
      "metadata": {
        "id": "77Fa65vX8vdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = '''You act as a prompt generator. Please do the classification task like the following examples.\n",
        "\n",
        "USER: replace chair into sofa at the left side\n",
        "ASSISTANT: {\"part\": \"left\", \"source object\": [\"chair\"], \"target object\": [\"sofa\"]}\n",
        "\n",
        "USER: change wooden table into white table in the right part\n",
        "ASSISTANT: {\"part\": \"right\", \"source object\": [\"wooden table\"], \"target object\": [\"white table\"]}\n",
        "\n",
        "USER: replace blue chair and red sofa into yellow table and green chair at the bottom\n",
        "ASSISTANT: {\"part\": \"bottom\", \"source object\": [\"blue chair\", \"red sofa\"], \"target object\": [\"yellow table\", \"green chair\"]}\n",
        "\n",
        "USER: replace sofa and shelf into chair and picture at the right side in the room\n",
        "ASSISTANT:'''\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=1024)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt_template):])"
      ],
      "metadata": {
        "id": "voskmfUJAVOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference with pipeline"
      ],
      "metadata": {
        "id": "dHjABkH380N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "AbOKFknU81Za"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}