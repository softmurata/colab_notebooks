{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhvBeTwQBB8jdrw0sLLRYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/llama_deus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "Q0lZnuARvPsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install peft gradio accelerate bitsandbytes\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "aGzsSk5BFSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "SePPRHe_vUgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubtgdUP3EoiZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import gradio as gr\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "\n",
        "# colab pro以上でのプランでA100を使用しないと動かないかも\n",
        "\n",
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-13b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-30b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-65b-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL,device_map={'': 0})\n",
        "\n",
        "LORA_WEIGHTS = \"teknium/llama-deus-7b-v3-lora\"\n",
        "# LORA_WEIGHTS =\"kunishou/Japanese-Alpaca-LoRA-13b-v0\"\n",
        "# LORA_WEIGHTS = \"kunishou/Japanese-Alpaca-LoRA-30b-v0\"\n",
        "# LORA_WEIGHTS = \"kunishou/Japanese-Alpaca-LoRA-65b-v0\"\n",
        "\n",
        "if BASE_MODEL == \"decapoda-research/llama-7b-hf\":\n",
        "  model_param = \"7B\"\n",
        "elif BASE_MODEL == \"decapoda-research/llama-13b-hf\":\n",
        "  model_param = \"13B\"\n",
        "elif BASE_MODEL == \"decapoda-research/llama-30b-hf\":\n",
        "  model_param = \"30B\"\n",
        "else:\n",
        "  model_param = \"65B\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        load_in_8bit=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        # device_map=\"auto\",\n",
        "        device_map={'': 0},\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0},)\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Input:\n",
        "{input}\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Response:\"\"\"\n",
        "\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=256,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=3,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return output.split(\"### Response:\")[1].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "I want you act as a professional scenario writer. \n",
        "\"\"\"\n",
        "input = \"\"\"\n",
        "Please write 20 minutes presentations about the latest generative AI.\n",
        "\"\"\"\n",
        "\n",
        "response = evaluate(instruction, input, max_new_tokens=1024)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cPomaNsr3gC",
        "outputId": "faf60d97-ff36-4fa0-868c-370e821b6fbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Generative Artificial Intelligence: The Future of Innovation\n",
            "\n",
            "Synopsis:\n",
            "In the 21st century, advancements in technology have revolutionized the way we live, work, and interact with one another. Among these innovations, generative artificial intelligence (GAI) has emerged as a game-changing technology, poised to reshape industries and redefine the very essence of what it means to be intelligent. In this presentation, we will explore the latest developments in GAI, examining its potential applications, benefits, and challenges. We will delve into the fascinating world of machine learning, deep learning, and neural networks, uncovering how these cutting-edge technologies are pushing the boundaries of human creativity and ingenuity. Whether you are a seasoned professional, a budding entrepreneur, or simply someone curious about the future of innovation, this presentation is sure to leave you inspired and invigorated by the limitless possibilities that GAI holds for us all.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "### Instruction\n",
        "I want you act as a professional scenario writer. Please write 20 minutes presentations about the latest generative AI.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = evaluate(instruction, max_new_tokens=2048)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTHCZtJ3zafR",
        "outputId": "5c98ab60-88b8-4918-b685-75335ebe4a3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slide 1: Introduction to Generative Artificial Intelligence (GAI)\n",
            "- Definition of GAI\n",
            "- Explanation of how GAI works\n",
            "- Brief overview of its applications\n",
            "\n",
            "Slides 2-5: Key Features and Capabilities of Generative GAIs\n",
            "- Discussion of unsupervised and supervised learning\n",
            "- Description of the use of neural networks and deep learning techniques\n",
            "- Overview of the ability of GAEs to generate new and creative content\n",
            "- Example use cases and applications of generative GAI in fields such as art, music, writing, and design.\n",
            "- Implications and potential future developments in the realm of GGAI. \n",
            "- Conclusion and summary of the key features and capabilities of a generative artificial intelligence.\n"
          ]
        }
      ]
    }
  ]
}