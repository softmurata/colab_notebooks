{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjWxHgSewRtKRVmk98RAb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/OpenAssistant_Llama2_13B_Orca_v2_8K_3166_GPTQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "aVh_0HWFGNML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyvcJ4BXEBw7"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "3c7doZMFGOiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "model_name_or_path = \"TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=False,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "lTTc8B0AEKKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "HNNfIs9bGPsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"AIについて教えて\"\n",
        "prompt_template=f'''<|prompter|>{prompt}<|endoftext|><|assistant|>\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]).split(\"<|assistant|>\")[-1].split(\"<|endoftext|>\")[0])"
      ],
      "metadata": {
        "id": "tZyXup4AFZoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template='''<|prompter|>replace chair into sofa at the left side<|endoftext|>\n",
        "<|assistant|>{\"part\": \"left\", \"source object\": [\"chair\"], \"target object\": [\"sofa\"]}<|endoftext|>\n",
        "<|prompter|>change wooden table into white table in the right part<|endoftext|>\n",
        "<|assistant|>{\"part\": \"right\", \"source object\": [\"wooden table\"], \"target object\": [\"white table\"]}<|endoftext|>\n",
        "<|prompter|>replace blue chair and red sofa into yellow table and green chair at the bottom<|endoftext|>\n",
        "<|assistant|>{\"part\": \"bottom\", \"source object\": [\"blue chair\", \"red sofa\"], \"target object\": [\"yellow table\", \"green chair\"]}<|endoftext|>\n",
        "<|prompter|>replace sofa and shelf into chair and picture at the right side in the room<|endoftext|>\n",
        "<|assistant|>'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]).split(\"<|assistant|>\")[-1].split(\"<|endoftext|>\")[0])"
      ],
      "metadata": {
        "id": "KxbHRHqcGpWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Please summarize the following context at the bullet points\n",
        "\n",
        "Context:\n",
        "    Elon Musk is a well-known entrepreneur, inventor, and engineer. He was born on June 28, 1971, in Pretoria, South Africa, and later moved to the United States where he pursued his education and built his career. Musk is the founder and CEO of several successful companies, including SpaceX, Tesla, Neuralink, and The Boring Company.\n",
        "\n",
        "    SpaceX is a private space exploration company that aims to make space travel affordable and accessible. Tesla is an electric car company that focuses on developing sustainable transportation solutions. Neuralink is a company that works on developing brain-machine interfaces to improve human cognitive abilities. The Boring Company aims to revolutionize tunneling technology to reduce the cost and time required for tunnel construction.\n",
        "\n",
        "    Elon Musk is known for his ambitious goals, innovative ideas, and ability to disrupt established industries. He has been recognized for his contributions to science and technology and has received numerous awards and honors throughout his career.\n",
        "\"\"\"\n",
        "prompt_template=f'''<|prompter|>{prompt}<|endoftext|><|assistant|>\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]).split(\"<|assistant|>\")[-1].split(\"<|endoftext|>\")[0])"
      ],
      "metadata": {
        "id": "IxrFcUEUIXvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}