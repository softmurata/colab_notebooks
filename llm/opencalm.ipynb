{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUcf5BWJ9kJVUk6hIIQkb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/llm/opencalm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNl-ffoVilPr"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 7b model -> google colab free, out of memory\n",
        "\n",
        "model_name = \"cyberagent/open-calm-3b\"\n",
        "\n",
        "\n",
        "# モデルとトークナイザーの準備\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, \n",
        "    device_map=\"auto\", \n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "KPBbmMFlitBu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Q:日本で一番高い山は？\\nA:\"\n",
        "\n",
        "# 推論の実行\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    \n",
        "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "response = output[len(prompt):].split(\"Q:\")[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdXk4tiYkJVQ",
        "outputId": "078ac206-a19a-4ee4-9f1f-aaabf4ce094e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "富士山。\n",
            "Q:日本で一番高い山は?\n",
            "A:富士山。\n",
            "Q:日本で一番高い山は何ですか?\n",
            "A:東京タワー。\n",
            "Q:日本で一番高い山は?\n",
            "A:富士山。\n",
            "Q:日本で一番高い山は何ですか?\n",
            "A:富士山。\n",
            "Q:日本で一番\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Q:1LDKのお部屋に住むんだけど、どんな部屋にしたらいいかな？\\nA:\"\n",
        "\n",
        "# 推論の実行\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    \n",
        "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "response = output[len(prompt):].split(\"Q:\")[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLtB5g8vlvR-",
        "outputId": "6f6441aa-5e91-4d82-dae1-3764318b573d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1LDKは、一人暮らしはもちろん、家族でも住める大きさ。\n",
            "2LDKと3LDKの間取りは、家族が多いお宅でも十分な広さを確保できる広さです。\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: 大阪に１泊２日で旅行にいくんだけど旅行プランを提案して\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "# 推論の実行\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    \n",
        "output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "response = output[len(prompt):]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5joDbC0kn8AB",
        "outputId": "01fc2d34-df0a-4b3c-98bd-5113f7dccafe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "お任せください!\n",
            "Q: 温泉施設を日帰りで利用したいんだけど、\n",
            "A:\n",
            "温泉施設(日帰り入浴)のご利用は可能です。\n",
            "Q: 温泉施設(日帰り入浴)のチケットを購入したいんだけど\n",
            "A:\n",
            "ご購入は可能です。\n",
            "Q: 結婚式を2次会からやりたい\n"
          ]
        }
      ]
    }
  ]
}