{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJmGEUmoYosQu/Sgn6d2ct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/dococr/translationmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RefDocs: https://note.com/npaka/n/n5146d9a444b4"
      ],
      "metadata": {
        "id": "wNrYnYOAERpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fugumt"
      ],
      "metadata": {
        "id": "hayXaeQBGDlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Y8Vy0cEIc2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# パイプラインの準備\n",
        "ej_translator = pipeline(\"translation\", model=\"staka/fugumt-en-ja\")"
      ],
      "metadata": {
        "id": "HNUkdgKiEV6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 翻訳するテキスト\n",
        "text = \"\"\"Hitori Gotoh, also known as Bocchi-chan, \n",
        "is one of the main characters in the manga and anime series, \n",
        "Bocchi the Rock!. She is in the first year of Shuka High School and \n",
        "is in charge of the guitar and lyrics of the band, Kessoku Band.\"\"\"\n",
        "\n",
        "# 翻訳\n",
        "print(ej_translator(text)[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BI5lE8lEYvd",
        "outputId": "6649cb8d-90b8-400a-e130-a6f0c9990da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "漫画・アニメシリーズ「Bocchi the Rock!」の主人公のひとりで、修歌高校1年生の時、バンド「けっそくバンド」のギターと歌詞を担当。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パイプラインの準備\n",
        "je_translator = pipeline(\"translation\", model=\"staka/fugumt-ja-en\")"
      ],
      "metadata": {
        "id": "g0zfguG8FH4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\" あなたはプロの旅行プランナーのように振る舞ってください。１日で大阪を旅行するプランを教えてください。\"\"\"\n",
        "print(je_translator(query)[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emb8JD6XEzxe",
        "outputId": "b8ae8a34-ea2a-4d18-a74f-1ba072cdd600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please behave like a professional travel planner, please tell me your plan to travel to Osaka in one day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = \"\"\"As I am just a language model, here is my advice for planning a trip from Tokyo to Osaka in one day:\n",
        "\n",
        "1) Take the shinkansen (bullet train) from Tokyo Station at 7am or earlier if possible. The journey takes around 2 hours and 30 minutes. You can purchase tickets online beforehand through JR East Pass website.\n",
        "\n",
        "2) Arrive at Shin-Osaka station by 9:30am and take the subway Midosuji Line towards Umeda Sky Building. This will allow you enough time to visit some of the popular attractions such as Dotonbori area, Universal Studios Japan, and Osaka Castle Park.\n",
        "\n",
        "3) After exploring these places, head back to Shin-Osaka station and catch the bullet train back to Tokyo at 5pm or later. Alternatively, you could stay overnight in Osaka and explore more of its nightlife scene.\n",
        " \"\"\"\n",
        "print(ej_translator(answer)[0][\"translation_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsyUb9WoFYjk",
        "outputId": "4d306cd9-f31d-4518-b290-aa409300c2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私はただの言語モデルなので、1日で東京から大阪への旅行を計画するための私のアドバイスは次のとおりです。1)東京駅から7時かそれ以前に新幹線(新幹線)に乗る。旅には2時間30分ほどかかる。JR東日本パスのウェブサイトからオンラインでチケットを購入できる。2)新大阪駅に9時半に到着して地下鉄御堂筋線を梅田スカイビルに向かわせる。これは、道堀、ユニバーサル・スタジオ・ジャパン、大阪キャッスル・パークなどの人気アトラクションのいくつかを訪れるのに十分な時間になる。3)これらの場所を探索した後、新大阪駅に戻り、午後5時以降に東京に戻る弾丸を捕まえる。また、大阪で一晩滞在して、そのナイトライフシーンをもっと探索することもできる。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title nllb"
      ],
      "metadata": {
        "id": "miX-D50aHSwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "id": "OMGQUwkgHRI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"  # 3.3B, 1.3Bあるけど多分無理\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "wQPSC7kzGHeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"UN Chief says there is no military solution in Syria\"\n",
        "inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"jpn_Jpan\"], max_length=30\n",
        ")\n",
        "ret = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltw7XGrGHh75",
        "outputId": "4bf9f036-81d5-4ac8-9afa-c2f1ad1752b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "国連長官はシリアに軍事的解決はないと\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\" We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks. For example, it passes a simulated bar exam with a score around the top 10% of test takers; in contrast, GPT-3.5’s score was around the bottom 10%. We’ve spent 6 months iteratively aligning GPT-4 using lessons from our adversarial testing program as well as ChatGPT, resulting in our best-ever results (though far from perfect) on factuality, steerability, and refusing to go outside of guardrails. \"\"\"\n",
        "inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"jpn_Jpan\"], max_length=200\n",
        ")\n",
        "ret = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "print(ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JofCU2ARHv_t",
        "outputId": "fba25255-dd21-4ebd-cad4-f0f502edc08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-4は,大きな多型モデル (画像とテキスト入力を受け入れ,テキスト出力を発信する) で,現実世界の多くのシナリオにおいて人間よりも能力が低いが,様々な専門的および学術基準で人間のレベルでのパフォーマンスを示しています.例えば,テスト参加者のトップ10%のスコアでシミュレーションバー試験を通過し,GPT-3.5のスコアは,下の10%のスコアでした.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama adapter\n"
      ],
      "metadata": {
        "id": "Em5yulptLaSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ZrrSkywalker/LLaMA-Adapter.git\n",
        "%cd LLaMA-Adapter\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "7iRt_SvSLcHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwBsIuGMMT4I",
        "outputId": "6855494e-d67b-4deb-bef4-961db9dad8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/nyanko7/LLaMA-7B/raw/main/checklist.chk -P /content/checkpoints\n",
        "!wget https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/consolidated.00.pth -P /content/checkpoints\n",
        "!wget https://huggingface.co/nyanko7/LLaMA-7B/raw/main/params.json -P /content/checkpoints\n",
        "!wget https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/tokenizer.model -P /conent/checkponts"
      ],
      "metadata": {
        "id": "1naGgDIoMbBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir adapterckpts"
      ],
      "metadata": {
        "id": "ZPuKKuPDMuH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ZrrSkywalker/LLaMA-Adapter/releases/download/v.1.0.0/llama_adapter_len10_layer30_release.pth -P /content/adapterckpts"
      ],
      "metadata": {
        "id": "ZU9rc3h6M0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Adapter/"
      ],
      "metadata": {
        "id": "ls5FPZFAN5zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cpu memoryが足りなくて動かせなさそう。\n",
        "!torchrun --nproc_per_node 1 example.py \\\n",
        "         --ckpt_dir ../checkpoints \\\n",
        "         --tokenizer_path ../checkpoints/tokenizer.model \\\n",
        "         --adapter_path ../adapterckpts/"
      ],
      "metadata": {
        "id": "n4AKhJTIMGpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download pdf"
      ],
      "metadata": {
        "id": "H-YRfx1lRBam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "awOhGsTtRCnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "ar_title = \"Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach\"\n",
        "search = arxiv.Search(\n",
        "        query=f\"ti:{ar_title}\",\n",
        "        max_results = 1,\n",
        ")"
      ],
      "metadata": {
        "id": "37POoU0fRGNW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = []\n",
        "for result in search.results():\n",
        "    print(result, result.published.year, result.title, result.summary)\n",
        "    result_list.append(result)"
      ],
      "metadata": {
        "id": "JOHqG340RwbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "title = result_list[0].title\n",
        "dirpath = f\"/content/papers/{title}\"\n",
        "os.makedirs(dirpath, exist_ok=True)\n",
        "result_list[0].download_pdf(dirpath=dirpath,filename=f\"{title}.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "jbg6Nke3SOz9",
        "outputId": "5ce60166-4db2-4d35-aa43-3954aabd011e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/papers/Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach/Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easyocr install"
      ],
      "metadata": {
        "id": "KZn8Lp1xXGE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/JaidedAI/EasyOCR.git"
      ],
      "metadata": {
        "id": "RpSZvgQxXFJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF Image tools"
      ],
      "metadata": {
        "id": "mVYkcJuwTeT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf image tool install\n",
        "!sudo apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install img2pdf\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "oLEtkI69TcR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path, convert_from_bytes\n",
        "from pdf2image.exceptions import (\n",
        "    PDFInfoNotInstalledError,\n",
        "    PDFPageCountError,\n",
        "    PDFSyntaxError\n",
        ")"
      ],
      "metadata": {
        "id": "z-G7JQcaTj9q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = convert_from_path(f\"/content/papers/{ar_title}/{ar_title}.pdf\")"
      ],
      "metadata": {
        "id": "ZDYzzOPxTkcA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "display(images[0])"
      ],
      "metadata": {
        "id": "vpH2PMiGU1jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DiT Install\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install gradio\n",
        "%cd /content\n",
        "!git clone https://github.com/microsoft/unilm.git\n",
        "\n",
        "%cd /content\n",
        "!wget https://huggingface.co/spaces/nielsr/dit-document-layout-analysis/raw/main/cascade_dit_base.yml\n",
        "!wget https://huggingface.co/spaces/nielsr/dit-document-layout-analysis/raw/main/Base-RCNN-FPN.yml"
      ],
      "metadata": {
        "id": "LfdQou3cWNgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "DHg5vmQfUt38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "# Please change data_structure.py\n",
        "# from collections import Iterable -> from collections.abc import Iterable\n",
        "\n",
        "import os\n",
        "# code: https://huggingface.co/spaces/nielsr/dit-document-layout-analysis/blob/main/app.py\n",
        "import sys\n",
        "sys.path.append(\"unilm\")\n",
        "\n",
        "import cv2\n",
        "\n",
        "from unilm.dit.object_detection.ditod import add_vit_config\n",
        "\n",
        "import torch\n",
        "\n",
        "from detectron2.config import CfgNode as CN\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "# Step 1: instantiate config\n",
        "cfg = get_cfg()\n",
        "add_vit_config(cfg)\n",
        "cfg.merge_from_file(\"cascade_dit_base.yml\")\n",
        "\n",
        "# Step 2: add model weights URL to config\n",
        "cfg.MODEL.WEIGHTS = \"https://layoutlm.blob.core.windows.net/dit/dit-fts/publaynet_dit-b_cascade.pth\"\n",
        "\n",
        "# Step 3: set device\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Step 4: define model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def analyze_image(img):\n",
        "    md = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
        "    if cfg.DATASETS.TEST[0]=='icdar2019_test':\n",
        "        md.set(thing_classes=[\"table\"])\n",
        "    else:\n",
        "        md.set(thing_classes=[\"text\",\"title\",\"list\",\"table\",\"figure\"])\n",
        "    \n",
        "    output = predictor(img)[\"instances\"]\n",
        "    v = Visualizer(img[:, :, ::-1],\n",
        "                    md,\n",
        "                    scale=1.0,\n",
        "                    instance_mode=ColorMode.SEGMENTATION)\n",
        "    result = v.draw_instance_predictions(output.to(\"cpu\"))\n",
        "    result_image = result.get_image()[:, :, ::-1]\n",
        "    \n",
        "    return result_image"
      ],
      "metadata": {
        "id": "ztST66lIWXbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0e5662-d951-4ef8-c209-8cbc432e735c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for gradio demo\n",
        "\"\"\"\n",
        "# gradio demo\n",
        "title = \"Interactive demo: Document Layout Analysis with DiT\"\n",
        "description = \"Demo for Microsoft's DiT, the Document Image Transformer for state-of-the-art document understanding tasks. This particular model is fine-tuned on PubLayNet, a large dataset for document layout analysis (read more at the links below). To use it, simply upload an image or use the example image below and click 'Submit'. Results will show up in a few seconds. If you want to make the output bigger, right-click on it and select 'Open image in new tab'.\"\n",
        "article = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2203.02378' target='_blank'>Paper</a> | <a href='https://github.com/microsoft/unilm/tree/master/dit' target='_blank'>Github Repo</a></p> | <a href='https://huggingface.co/docs/transformers/master/en/model_doc/dit' target='_blank'>HuggingFace doc</a></p>\"\n",
        "examples =[['publaynet_example.jpeg']]\n",
        "css = \".output-image, .input-image, .image-preview {height: 600px !important}\"\n",
        "\n",
        "iface = gr.Interface(fn=analyze_image, \n",
        "                     inputs=gr.inputs.Image(type=\"numpy\", label=\"document image\"), \n",
        "                     outputs=gr.outputs.Image(type=\"numpy\", label=\"annotated document\"),\n",
        "                     title=title,\n",
        "                     description=description,\n",
        "                     examples=examples,\n",
        "                     article=article,\n",
        "                     css=css,\n",
        "                     enable_queue=True)\n",
        "iface.launch(debug=True, cache_examples=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nSWEQIQKZDD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "img_id = 5\n",
        "res_img = analyze_image(np.asarray(images[img_id]))"
      ],
      "metadata": {
        "id": "iWb4r4zpWnaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image.fromarray(res_img))"
      ],
      "metadata": {
        "id": "T2_Fc447Yses"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only inference"
      ],
      "metadata": {
        "id": "grhZ2ziFZY_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "import cv2\n",
        "reader = easyocr.Reader(['en']) # this needs to run only once to load the model into memory"
      ],
      "metadata": {
        "id": "OUVNnIGsgXCu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# utils function\n",
        "def create_ocr_sentence(img, img_type):\n",
        "  save_img_path = f\"{img_type}.jpg\"\n",
        "  cv2.imwrite(save_img_path, img)\n",
        "  ocr_result = reader.readtext(save_img_path, detail=0)\n",
        "  ocr_str = \"\"\n",
        "  for orr in ocr_result:\n",
        "    ocr_str += f\"{orr} \"\n",
        "  ocr_str = ocr_str[:-1]\n",
        "  \n",
        "  return ocr_str\n",
        "\n",
        "def create_ocr_and_textimg(text_imgs):\n",
        "  text_str = \"\"\n",
        "  base_text = None\n",
        "  bh = 0\n",
        "  bw = 0\n",
        "  # text_imgs = class_imgs[\"text\"]\n",
        "  for text in text_imgs:\n",
        "    cv2.imwrite(\"text.jpg\", text)\n",
        "    if base_text is None:\n",
        "      base_text = text\n",
        "      bh, bw = base_text.shape[:2]\n",
        "    else:\n",
        "      res_text = cv2.resize(text, (bw, bh))\n",
        "      base_text = np.concatenate([base_text, res_text], 0)\n",
        "    text_result = reader.readtext(\"text.jpg\",  detail = 0)\n",
        "        \n",
        "    for tr in text_result:\n",
        "      text_str += f\"{tr} \"\n",
        "  text_str = text_str[:-1]\n",
        "\n",
        "  return text_str, base_text"
      ],
      "metadata": {
        "id": "h1VU7TKPk3LT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference function\n",
        "def inference_at_onepage(img_id, images, predictor, ocr_reader, result_json, score_threshold=0.5, draw=False):\n",
        "  img = np.asarray(images[img_id])\n",
        "  # inference with unilm\n",
        "  output = predictor(img)[\"instances\"]\n",
        "  id2label = {0:\"text\",1:\"title\",2:\"list\",3:\"table\",4:\"figure\"}\n",
        "  class_imgs = {\"text\": [], \"title\": [], \"list\": [], \"table\": [], \"figure\": []}\n",
        "  image_height, image_width = output.image_size\n",
        "  for pc, pb, ps in zip(output.pred_classes, output.pred_boxes, output.scores):\n",
        "    if ps.detach().cpu().numpy() < score_threshold:\n",
        "      continue\n",
        "    class_id = int(pc.detach().cpu().numpy())\n",
        "    bbox_numpy = pb.detach().cpu().numpy()\n",
        "    # print(id2label[class_id])\n",
        "    # print(bbox_numpy)\n",
        "    h = int(bbox_numpy[3] - bbox_numpy[1])\n",
        "    w = int(bbox_numpy[2] - bbox_numpy[0])\n",
        "    if pc == 0 and h < 100:\n",
        "      continue\n",
        "    crop_img = img[int(bbox_numpy[1]):int(bbox_numpy[3]), int(bbox_numpy[0]):int(bbox_numpy[2]), :]\n",
        "    class_imgs[id2label[class_id]].append(crop_img)\n",
        "    if draw:\n",
        "      display(id2label[class_id], Image.fromarray(crop_img))\n",
        "\n",
        "  # store other classes\n",
        "  class_types = [\"list\", \"table\", \"figure\"]\n",
        "  for clt in class_types:\n",
        "    if len(class_imgs[clt]) > 0:\n",
        "      if clt not in result_json.keys():\n",
        "        result_json[clt] = class_imgs[clt]\n",
        "      else:\n",
        "        result_json[clt].extend(class_imgs[clt])\n",
        "  \n",
        "\n",
        "  # get result json\n",
        "  if img_id == 0:\n",
        "    for idx, (title, text) in enumerate(zip(class_imgs[\"title\"], class_imgs[\"text\"])):\n",
        "      if idx == 0:\n",
        "        title_str = create_ocr_sentence(title, \"title\")\n",
        "        text_str = create_ocr_sentence(text, \"text\")\n",
        "\n",
        "        result_json[\"other_text\"] = [text_str]\n",
        "        result_json[\"title\"] = title_str\n",
        "\n",
        "        # display result\n",
        "        if draw:\n",
        "          display(Image.fromarray(title), title_str)\n",
        "          display(Image.fromarray(text), text_str)\n",
        "        \n",
        "        # add img\n",
        "        if len(class_imgs[\"figure\"]) > 0:\n",
        "          result_json[\"title\" + \"_figure\"] = class_imgs[\"figure\"]\n",
        "\n",
        "      else:\n",
        "        title_str = create_ocr_sentence(title, \"title\")\n",
        "        text_str = create_ocr_sentence(text, \"text\")\n",
        "\n",
        "        result_json[title_str] = text_str \n",
        "\n",
        "        # display result\n",
        "        if draw:\n",
        "          display(Image.fromarray(title), title_str)\n",
        "          display(Image.fromarray(text), text_str)\n",
        "\n",
        "        if len(class_imgs[\"figure\"]) > 0:\n",
        "          result_json[title_str + \"_figure\"] = class_imgs[\"figure\"]\n",
        "  else:\n",
        "    if \"title\" in class_imgs.keys():\n",
        "      for title in class_imgs[\"title\"]:\n",
        "        title_str = create_ocr_sentence(title, \"title\")\n",
        "\n",
        "        text_str, base_text = create_ocr_and_textimg(class_imgs[\"text\"])\n",
        "        result_json[title_str] = text_str\n",
        "\n",
        "        # display result\n",
        "        if draw:\n",
        "          display(Image.fromarray(title), title_str)\n",
        "          display(Image.fromarray(base_text), text_str)\n",
        "        \n",
        "        if len(class_imgs[\"figure\"]) > 0:\n",
        "          result_json[title_str + \"_figure\"] = class_imgs[\"figure\"]\n",
        "    else:\n",
        "      text_str, base_text = create_ocr_and_textimg(class_imgs[\"text\"])\n",
        "\n",
        "      if \"other_text\" not in result_json.keys():\n",
        "        result_json[\"other_text\"] = []\n",
        "      else:\n",
        "        result_json[\"other_text\"].append(text_str)\n",
        "\n",
        "      if draw:\n",
        "        display(\"other text\")\n",
        "        display(Image.fromarray(base_text), text_str)\n",
        "\n",
        "      if len(class_imgs[\"figure\"]) > 0:\n",
        "          result_json[\"other_figure\"] = class_imgs[\"figure\"]\n",
        "\n",
        "  return result_json"
      ],
      "metadata": {
        "id": "w9YPKYMkf25j"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference onepage"
      ],
      "metadata": {
        "id": "leXaGccXmJ8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_json = {}\n",
        "img_id = 1\n",
        "result_json = inference_at_onepage(img_id, images, predictor, reader, result_json, draw=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMJyoy-VlDh5",
        "outputId": "6aa0c4a9-a391-45c6-b177-f4cf160a0add"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_json.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bCg2qg0lVN3",
        "outputId": "f9aa232a-3929-436f-ec08-f2ed9b0f3a63"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['list', 'figure', '2 Methodology', '2 Methodology_figure'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference full pdf"
      ],
      "metadata": {
        "id": "G1prRYk8mLeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_json = {}\n",
        "for img_id in range(10):\n",
        "  result_json = inference_at_onepage(img_id, images, predictor, reader, result_json, draw=False)\n",
        "print(result_json.keys())"
      ],
      "metadata": {
        "id": "wnXPNCIRnP_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RedPajama INCITE 3B chat"
      ],
      "metadata": {
        "id": "X9JrXMHW1_WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate sentencepiece bitsandbytes"
      ],
      "metadata": {
        "id": "ayqUxMGV2Bo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "gBEPI-aZ4KBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "PO_wIM3o2Uky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "model = model.to('cuda:0')"
      ],
      "metadata": {
        "id": "84jHnrvu2XHB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference samples for llm\n",
        "prompt = \"\"\"\n",
        "<human>: Please summarize the following context. \\n Context: \n",
        "Recent text-to-image generation models have demonstrated impressive capability of generating text-aligned images with high fidelity: However; generating images of novel concept provided by the user input image is still challenging task: To address this problem, researchers have been exploring various methods for cus tomizing pre-trained text-to-image generation models. Currently, most existing methods for customizing pre-trained text-to-image generation models involve the use of regularization techniques to prevent over-- While regularization will ease the challenge of customization and leads to successful content creation with respect to text guidance, it may restrict the model capability, resulting in the loss of detailed information and inferior performance. In this work, we propose a novel framework for customized text-to-image generation without the use of regulariza tion. Specifically, Our proposed framework consists of an encoder network and novel sampling method which can tackle the over-fitting problem without the use of regularization:  With the proposed framework; we are able to customize large scale text-to-image generation model within half a minute on single GPU, with only one image provided by the user: We demonstrate in experiments that OUr proposed framework outperforms existing methods, and preserves more ~grained details. -fitting: fine-\n",
        "\\n<bot>:\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "id": "yatI59H32eZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom llm\n",
        "from langchain.llms.base import LLM\n",
        "from langchain import PromptTemplate\n",
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun"
      ],
      "metadata": {
        "id": "ZoYKf8j02zMj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom llm model\n",
        "class RedPajamaINCITELLM(LLM):\n",
        "    \n",
        "    n: int\n",
        "        \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "    \n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = model.generate(\n",
        "              **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
        "              )\n",
        "        token = outputs.sequences[0, input_length:]\n",
        "        output_str = tokenizer.decode(token).split(\"<human>\")[0]\n",
        "        # print(\"Model Response:\", response)\n",
        "        return output_str\n",
        "    \n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "\n",
        "        return {\"n\": self.n}"
      ],
      "metadata": {
        "id": "dQgJdF6t4aYf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<human>: Please summarize the following context. \\n Context: {context} \\n<bot>:\"\"\"\n",
        "\n",
        "prompttemp = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "llm = RedPajamaINCITELLM(n=400)"
      ],
      "metadata": {
        "id": "tBg-jyyT42AF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check llm inference\n",
        "context = \"Recent text-to-image generation models have demonstrated impressive capability of generating text-aligned images with high fidelity: However; generating images of novel concept provided by the user input image is still challenging task: To address this problem, researchers have been exploring various methods for cus tomizing pre-trained text-to-image generation models. Currently, most existing methods for customizing pre-trained text-to-image generation models involve the use of regularization techniques to prevent over-- While regularization will ease the challenge of customization and leads to successful content creation with respect to text guidance, it may restrict the model capability, resulting in the loss of detailed information and inferior performance. In this work, we propose a novel framework for customized text-to-image generation without the use of regulariza tion. Specifically, Our proposed framework consists of an encoder network and novel sampling method which can tackle the over-fitting problem without the use of regularization:  With the proposed framework; we are able to customize large scale text-to-image generation model within half a minute on single GPU, with only one image provided by the user: We demonstrate in experiments that OUr proposed framework outperforms existing methods, and preserves more ~grained details. -fitting: fine-\"\n",
        "\n",
        "print(llm(\n",
        "    prompttemp.format(\n",
        "        context=context\n",
        "    )\n",
        "))"
      ],
      "metadata": {
        "id": "aV4X8AjB5EMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF handling"
      ],
      "metadata": {
        "id": "Kfk2dTcOlce2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import img2pdf\n",
        "import cv2\n",
        " \n",
        "text_img = class_imgs[\"text\"][0]\n",
        "cv2.imwrite(\"1.jpg\", text_img)\n",
        "\n",
        "# 1つの画像をPDFに変換する \n",
        "with open(\"1.pdf\", \"wb\") as f: \n",
        "    f.write(img2pdf.convert('1.jpg'))"
      ],
      "metadata": {
        "id": "5fXja1FCgsfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pypdf\n",
        "src_pdf = pypdf.PdfReader(\"1.pdf\")\n",
        "print(src_pdf)"
      ],
      "metadata": {
        "id": "jXTOn-cxiL0l",
        "outputId": "3d8b5b65-24a5-4833-b948-e534e6493296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pypdf._reader.PdfReader object at 0x7fda188c0e20>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PowerPoint"
      ],
      "metadata": {
        "id": "BCb5xTI5oWW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx"
      ],
      "metadata": {
        "id": "txKWZ53WoXb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_json.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlDVlBhpCQG",
        "outputId": "410ab2dd-e18a-46f9-d1ec-02428df6575c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['other_text', 'title', 'Introduction', 'Abstract', 'list', 'figure', '2 Methodology', '2.1 Fusion Sampling', '3 Experiments', '3.2 Quantitative Results', '3.1 Qualitative Results', '3.4 Ablation Study', '3.3 Human Evaluation', '4 Discussion', '5 Conclusion', 'References'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import MSO_ANCHOR, MSO_AUTO_SIZE\n",
        "from pptx import  Presentation\n",
        "prs = Presentation()\n",
        "\n",
        "# title slide\n",
        "title_slide_layout = prs.slide_layouts[0]\t# \"タイトルとスライド\"のレイアウトオブジェクトを取得\n",
        "sld_title = prs.slides.add_slide(title_slide_layout)\n",
        "sld_title.placeholders[0].text = result_json[\"title\"]\n",
        "\n",
        "# create abstract slide\n",
        "title_slide_layout = prs.slide_layouts[1]\n",
        "sld_abst = prs.slides.add_slide(title_slide_layout)\n",
        "\n",
        "for p in sld_abst.placeholders:\n",
        "  print(p.name)\n",
        "\n",
        "insert_type = \"Abstract\"\n",
        "sld_abst.placeholders[0].text = insert_type\n",
        "\n",
        "summ_str = llm(\n",
        "    prompttemp.format(\n",
        "        context=result_json[insert_type]\n",
        "    )\n",
        ")\n",
        "\n",
        "sld_abst.placeholders[1].text = summ_str\n",
        "\n",
        "shapes = sld_abst.shapes\n",
        "\n",
        "for idx, (shape, sent) in enumerate(zip(shapes, [insert_type, summ_str])):\n",
        "  text_frame = shape.text_frame\n",
        "  text_frame.clear()\n",
        "\n",
        "  if idx == 0:\n",
        "    p = text_frame.paragraphs[0]\n",
        "    run = p.add_run()\n",
        "    run.text = sent\n",
        "\n",
        "    font = run.font\n",
        "    font.name = 'Calibri'\n",
        "    font.size = Pt(18)\n",
        "  else:\n",
        "    p = text_frame.paragraphs[0]\n",
        "    run = p.add_run()\n",
        "    run.text = sent\n",
        "\n",
        "    font = run.font\n",
        "    font.name = 'Calibri'\n",
        "    font.size = Pt(14)\n",
        "\n",
        "\n",
        "# create overview slide\n",
        "title_slide_layout = prs.slide_layouts[8] # \"タイトル付き図\"のレイアウトオブジェクトを取得\n",
        "sld_ovw = prs.slides.add_slide(title_slide_layout) #  新しいスライドを追加\n",
        "\n",
        "for p in sld_ovw.placeholders:\n",
        "  print(p.name)               \t\t\t  # >>Title 1　Picture Placeholder 2　Text Placeholder 3\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------\n",
        "insert_type = \"2 Methodology\"\n",
        "overview_img = result_json[insert_type + \"_figure\"][0]\n",
        "cv2.imwrite(\"overview.png\", cv2.cvtColor(overview_img, cv2.COLOR_RGB2BGR))\n",
        "sld_ovw.placeholders[0].text = insert_type             \n",
        "sld_ovw.placeholders[1].insert_picture('overview.png') # 画像ファイルを画像プレースフォルダーに貼り付ける\n",
        "\n",
        "summ_str = summ_str = llm(\n",
        "    prompttemp.format(\n",
        "        context=result_json[insert_type]\n",
        "    )\n",
        ")\n",
        "\n",
        "sld_ovw.placeholders[2].text = summ_str\n",
        "\n",
        "\n",
        "# create experiment slide\n",
        "title_slide_layout = prs.slide_layouts[8] # \"タイトル付き図\"のレイアウトオブジェクトを取得\n",
        "sld_exp = prs.slides.add_slide(title_slide_layout) #  新しいスライドを追加\n",
        "\n",
        "for p in sld_exp.placeholders:\n",
        "  print(p.name)               \t\t\t  # >>Title 1　Picture Placeholder 2　Text Placeholder 3\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------\n",
        "insert_type = \"3 Experiments\"\n",
        "target_img = result_json[insert_type + \"_figure\"][0]\n",
        "cv2.imwrite(\"experiment.png\", cv2.cvtColor(target_img, cv2.COLOR_RGB2BGR))\n",
        "sld_exp.placeholders[0].text = insert_type             \n",
        "sld_exp.placeholders[1].insert_picture('experiment.png') # 画像ファイルを画像プレースフォルダーに貼り付ける\n",
        "\n",
        "summ_str = llm(\n",
        "    prompttemp.format(\n",
        "        context=result_json[insert_type]\n",
        "    )\n",
        ")\n",
        "\n",
        "sld_exp.placeholders[2].text = summ_str\n",
        "\n",
        "\n",
        "# create discussion slide\n",
        "title_slide_layout = prs.slide_layouts[8] # \"タイトル付き図\"のレイアウトオブジェクトを取得\n",
        "sld_exp = prs.slides.add_slide(title_slide_layout) #  新しいスライドを追加\n",
        "\n",
        "for p in sld_exp.placeholders:\n",
        "  print(p.name)               \t\t\t  # >>Title 1　Picture Placeholder 2　Text Placeholder 3\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------\n",
        "insert_type = \"4 Discussion\"\n",
        "target_img = result_json[insert_type + \"_figure\"][0]\n",
        "cv2.imwrite(f\"{insert_type}.png\", cv2.cvtColor(target_img, cv2.COLOR_RGB2BGR))\n",
        "sld_exp.placeholders[0].text = insert_type             \n",
        "sld_exp.placeholders[1].insert_picture(f'{insert_type}.png') # 画像ファイルを画像プレースフォルダーに貼り付ける\n",
        "\n",
        "summ_str = llm(\n",
        "    prompttemp.format(\n",
        "        context=result_json[insert_type]\n",
        "    )\n",
        ")\n",
        "\n",
        "sld_exp.placeholders[2].text = summ_str\n",
        "\n",
        "prs.save('doc.pptx')\n"
      ],
      "metadata": {
        "id": "_sURn8PtqL0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}