{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ0j2gYJsNugQaBRQJiP4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/pix2pix/controlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h61C2A9pc36H",
        "outputId": "ee22cf03-74e3-4986-abab-d390063ebc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ControlNet'...\n",
            "remote: Enumerating objects: 820, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 820 (delta 105), reused 87 (delta 83), pack-reused 665\u001b[K\n",
            "Receiving objects: 100% (820/820), 84.33 MiB | 33.10 MiB/s, done.\n",
            "Resolving deltas: 100% (250/250), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lllyasviel/ControlNet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements.txt\n",
        "\"\"\"\n",
        "gradio\n",
        "albumentations\n",
        "opencv-contrib-python\n",
        "imageio\n",
        "imageio-ffmpeg\n",
        "pytorch-lightning\n",
        "omegaconf\n",
        "test-tube\n",
        "streamlit\n",
        "einops\n",
        "transformers\n",
        "webdataset\n",
        "kornia\n",
        "open_clip_torch\n",
        "invisible-watermark\n",
        "streamlit-drawable-canvas\n",
        "torchmetrics\n",
        "timm\n",
        "addict\n",
        "yapf\n",
        "prettytable\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AIbMhu7sdUiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ControlNet\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-FYfiOA8dNtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers triton"
      ],
      "metadata": {
        "id": "ABHcRR0VfxEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title canny\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth -P models\n",
        "# !wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth -P models\n",
        "# !wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth -P models\n",
        "# !wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth -P models"
      ],
      "metadata": {
        "id": "oymy6cxAeXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_canny2image.py"
      ],
      "metadata": {
        "id": "ynvPq_Kne1tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title pose\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth -P models"
      ],
      "metadata": {
        "id": "40ZE_pwJjs6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth -P annotator/ckpts"
      ],
      "metadata": {
        "id": "CxNwtdBck9Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth -P annotator/ckpts"
      ],
      "metadata": {
        "id": "srFDIPZplCP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Please change gradio_pose2image.py -> block.launch(share=True)\n",
        "!python gradio_pose2image.py"
      ],
      "metadata": {
        "id": "O6b4a6v2jySi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title mlsd\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth -P models"
      ],
      "metadata": {
        "id": "yzsTTE4kqDfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth -P annotator/ckpts"
      ],
      "metadata": {
        "id": "3oFSWpB3qme-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_hough2image.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC0ixauiqTWq",
        "outputId": "71febbd5-2a37-4317-ee63-016ee2621fed"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 01:22:48.337526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 01:22:49.379907: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 01:22:49.380027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 01:22:49.380056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled.\n",
            "Error caught was: module 'triton.language' has no attribute 'constexpr'\n",
            "logging improved.\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
            "  rank_zero_deprecation(\n",
            "ControlLDM: Running in eps-prediction mode\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
            "Loaded model config from [./models/cldm_v15.yaml]\n",
            "Loaded state_dict from [./models/control_sd15_mlsd.pth]\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://c509365f-c81f-45ac.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 441323603\n",
            "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n",
            "DDIM Sampler: 100% 20/20 [00:19<00:00,  1.00it/s]\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 2087371526\n",
            "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n",
            "DDIM Sampler: 100% 20/20 [00:20<00:00,  1.00s/it]\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 2087371526\n",
            "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n",
            "DDIM Sampler: 100% 20/20 [00:20<00:00,  1.01s/it]\n",
            "INFO:lightning_fabric.utilities.seed:Global seed set to 2087371526\n",
            "Data shape for DDIM sampling is (1, 4, 64, 96), eta 0.0\n",
            "Running DDIM Sampling with 20 timesteps\n",
            "DDIM Sampler: 100% 20/20 [00:20<00:00,  1.01s/it]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1664, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"gradio_hough2image.py\", line 96, in <module>\n",
            "    block.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1581, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1667, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/networking.py\", line 43, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c509365f-c81f-45ac.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Advanced \n",
        "!wget https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-full.safetensors -P models"
      ],
      "metadata": {
        "id": "dBDkq-wcly_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt -P models"
      ],
      "metadata": {
        "id": "hbuHUjWjnKhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nKo4Ll5noS7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cannot transfer because of out of memory\n",
        "!python tool_transfer_control.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-_6ph3Yn1J2",
        "outputId": "529439f8-3466-4b1c-a099-efd3a1e93dbe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 01:13:06.248756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 01:13:10.105106: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 01:13:10.105784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 01:13:10.105809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled.\n",
            "Error caught was: module 'triton.language' has no attribute 'constexpr'\n",
            "logging improved.\n",
            "Loaded state_dict from [./models/v1-5-pruned.ckpt]\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}