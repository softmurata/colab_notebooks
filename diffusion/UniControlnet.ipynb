{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHkcfOd25uaoDhKJmk3NY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/UniControlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "6mOvM3ZQ4b1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShihaoZhaoZSH/Uni-ControlNet.git"
      ],
      "metadata": {
        "id": "iR8X8VpO0x8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNT0ex0y0kZN"
      },
      "outputs": [],
      "source": [
        "!pip install gradio==3.16.2 albumentations==1.3.0 \\\n",
        "      imageio==2.9.0 \\\n",
        "      imageio-ffmpeg==0.4.2 \\\n",
        "      pytorch-lightning==1.6.0 \\\n",
        "      omegaconf==2.1.1 \\\n",
        "      test-tube==0.7.5 \\\n",
        "      streamlit==1.12.1 \\\n",
        "      einops==0.3.0 \\\n",
        "      transformers==4.19.2 \\\n",
        "      webdataset==0.2.5 \\\n",
        "      kornia==0.6 \\\n",
        "      open_clip_torch==2.0.2 \\\n",
        "      invisible-watermark==0.1.5 \\\n",
        "      streamlit-drawable-canvas==0.8.0 \\\n",
        "      torchmetrics==0.7.0 \\\n",
        "      timm==0.6.12 \\\n",
        "      addict==2.4.0 \\\n",
        "      yapf==0.32.0 \\\n",
        "      prettytable==3.6.0 \\\n",
        "      safetensors==0.2.7 \\\n",
        "      basicsr==1.4.2 \\\n",
        "      datasets==2.10.1 \\\n",
        "      pathlib==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/arogozhnikov/einops.git"
      ],
      "metadata": {
        "id": "oT7nTp-N6Aqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "EH5DtZAq7Dtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Uni-ControlNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkANkjlg1r79",
        "outputId": "843d5f39-06ba-4c15-9fb3-f1512023be6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Uni-ControlNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download models\n",
        "!wget https://huggingface.co/shihaozhao/uni-controlnet/resolve/main/uni.ckpt -P ckpt"
      ],
      "metadata": {
        "id": "jbUtc1EB10i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start WebUI"
      ],
      "metadata": {
        "id": "eFf25DWU1o1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Uni-ControlNet\n",
        "!python src/test/test.py"
      ],
      "metadata": {
        "id": "cf0S8uuZ2DOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected annotator"
      ],
      "metadata": {
        "id": "t_jnHXBTpl8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_selected = [\"hed\", \"sketch\", \"midas\", \"content\"]  # \"canny\", \"mlsd\", \"openpose\", \"seg\""
      ],
      "metadata": {
        "id": "zOgQGqnQqKt9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Uni-ControlNet\n",
        "from utils.share import *\n",
        "import utils.config as config\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from annotator.util import resize_image, HWC3\n",
        "from annotator.canny import CannyDetector\n",
        "from annotator.mlsd import MLSDdetector\n",
        "from annotator.hed import HEDdetector\n",
        "from annotator.sketch import SketchDetector\n",
        "from annotator.openpose import OpenposeDetector\n",
        "from annotator.midas import MidasDetector\n",
        "from annotator.uniformer import UniformerDetector\n",
        "from annotator.content import ContentDetector\n",
        "\n",
        "from models.util import create_model, load_state_dict\n",
        "from models.ddim_hacked import DDIMSampler\n",
        "\n",
        "\n",
        "if \"canny\" in model_selected:\n",
        "  apply_canny = CannyDetector()\n",
        "else:\n",
        "  apply_canny = None\n",
        "\n",
        "if \"mlsd\" in model_selected:\n",
        "  apply_mlsd = MLSDdetector()\n",
        "else:\n",
        "  apply_mlsd = None\n",
        "\n",
        "if \"hed\" in model_selected:\n",
        "  apply_hed = HEDdetector()\n",
        "else:\n",
        "  apply_hed = None\n",
        "\n",
        "if \"sketch\" in model_selected:\n",
        "  apply_sketch = SketchDetector()\n",
        "else:\n",
        "  apply_sketch = None\n",
        "\n",
        "if \"openpose\" in model_selected:\n",
        "  apply_openpose = OpenposeDetector()\n",
        "else:\n",
        "  apply_openpose = None\n",
        "\n",
        "if \"midas\" in model_selected:\n",
        "  apply_midas = MidasDetector()\n",
        "else:\n",
        "  apply_midas = None\n",
        "\n",
        "if \"seg\" in model_selected:\n",
        "  apply_seg = UniformerDetector()\n",
        "else:\n",
        "  apply_seg = None\n",
        "\n",
        "if \"content\" in model_selected:\n",
        "  apply_content = ContentDetector()\n",
        "else:\n",
        "  apply_content = None\n",
        "\n",
        "\n",
        "model = create_model('./configs/uni_v15.yaml').cpu()\n",
        "model.load_state_dict(load_state_dict('./ckpt/uni.ckpt', location='cuda'))\n",
        "model = model.cuda()\n",
        "ddim_sampler = DDIMSampler(model)\n",
        "\n",
        "\n",
        "def process(canny_image, mlsd_image, hed_image, sketch_image, openpose_image, midas_image, seg_image, content_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta, low_threshold, high_threshold, value_threshold, distance_threshold, alpha, global_strength):\n",
        "    \n",
        "    seed_everything(seed)\n",
        "\n",
        "    if canny_image is not None:\n",
        "        anchor_image = canny_image\n",
        "    elif mlsd_image is not None:\n",
        "        anchor_image = mlsd_image\n",
        "    elif hed_image is not None:\n",
        "        anchor_image = hed_image\n",
        "    elif sketch_image is not None:\n",
        "        anchor_image = sketch_image\n",
        "    elif openpose_image is not None:\n",
        "        anchor_image = openpose_image\n",
        "    elif midas_image is not None:\n",
        "        anchor_image = midas_image\n",
        "    elif seg_image is not None:\n",
        "        anchor_image = seg_image\n",
        "    elif content_image is not None:\n",
        "        anchor_image = content_image\n",
        "    else:\n",
        "        anchor_image = np.zeros((image_resolution, image_resolution, 3)).astype(np.uint8)\n",
        "    H, W, C = resize_image(HWC3(anchor_image), image_resolution).shape\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if canny_image is not None:\n",
        "            canny_image = cv2.resize(canny_image, (W, H))\n",
        "            canny_detected_map = HWC3(apply_canny(HWC3(canny_image), low_threshold, high_threshold))\n",
        "        else:\n",
        "            canny_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if mlsd_image is not None:\n",
        "            mlsd_image = cv2.resize(mlsd_image, (W, H))\n",
        "            mlsd_detected_map = HWC3(apply_mlsd(HWC3(mlsd_image), value_threshold, distance_threshold))\n",
        "        else:\n",
        "            mlsd_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if hed_image is not None:\n",
        "            hed_image = cv2.resize(hed_image, (W, H))\n",
        "            hed_detected_map = HWC3(apply_hed(HWC3(hed_image)))\n",
        "        else:\n",
        "            hed_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if sketch_image is not None:\n",
        "            sketch_image = cv2.resize(sketch_image, (W, H))\n",
        "            sketch_detected_map = HWC3(apply_sketch(HWC3(sketch_image)))            \n",
        "        else:\n",
        "            sketch_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if openpose_image is not None:\n",
        "            openpose_image = cv2.resize(openpose_image, (W, H))\n",
        "            openpose_detected_map, _ = apply_openpose(HWC3(openpose_image), False)\n",
        "            openpose_detected_map = HWC3(openpose_detected_map)\n",
        "        else:\n",
        "            openpose_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if midas_image is not None:\n",
        "            midas_image = cv2.resize(midas_image, (W, H))\n",
        "            midas_detected_map = HWC3(apply_midas(HWC3(midas_image), alpha))\n",
        "        else:\n",
        "            midas_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if seg_image is not None:\n",
        "            seg_image = cv2.resize(seg_image, (W, H))\n",
        "            seg_detected_map, _ = apply_seg(HWC3(seg_image))\n",
        "            seg_detected_map = HWC3(seg_detected_map)\n",
        "        else:\n",
        "            seg_detected_map = np.zeros((H, W, C)).astype(np.uint8)\n",
        "        if content_image is not None:\n",
        "            content_emb = apply_content(content_image)\n",
        "        else:\n",
        "            content_emb = np.zeros((768))\n",
        "\n",
        "        detected_maps_list = [canny_detected_map, \n",
        "                              mlsd_detected_map, \n",
        "                              hed_detected_map,\n",
        "                              sketch_detected_map,\n",
        "                              openpose_detected_map,\n",
        "                              midas_detected_map,\n",
        "                              seg_detected_map                          \n",
        "                              ]\n",
        "        detected_maps = np.concatenate(detected_maps_list, axis=2)\n",
        "\n",
        "        local_control = torch.from_numpy(detected_maps.copy()).float().cuda() / 255.0\n",
        "        local_control = torch.stack([local_control for _ in range(num_samples)], dim=0)\n",
        "        local_control = einops.rearrange(local_control, 'b h w c -> b c h w').clone()\n",
        "        global_control = torch.from_numpy(content_emb.copy()).float().cuda().clone()\n",
        "        global_control = torch.stack([global_control for _ in range(num_samples)], dim=0)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        uc_local_control = local_control\n",
        "        uc_global_control = torch.zeros_like(global_control)\n",
        "        cond = {\"local_control\": [local_control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)], 'global_control': [global_control]}\n",
        "        un_cond = {\"local_control\": [uc_local_control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)], 'global_control': [uc_global_control]}\n",
        "        shape = (4, H // 8, W // 8)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=True)\n",
        "\n",
        "        model.control_scales = [strength] * 13\n",
        "        samples, _ = ddim_sampler.sample(ddim_steps, num_samples,\n",
        "                                                     shape, cond, verbose=False, eta=eta,\n",
        "                                                     unconditional_guidance_scale=scale,\n",
        "                                                     unconditional_conditioning=un_cond, global_strength=global_strength)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        x_samples = model.decode_first_stage(samples)\n",
        "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
        "        results = [x_samples[i] for i in range(num_samples)]\n",
        "\n",
        "    return [results, detected_maps_list]\n",
        "\n",
        "\n",
        "block = gr.Blocks().queue()\n",
        "with block:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"## Uni-ControlNet Demo\")\n",
        "    with gr.Row():\n",
        "        canny_image = gr.Image(source='upload', type=\"numpy\", label='canny')\n",
        "        mlsd_image = gr.Image(source='upload', type=\"numpy\", label='mlsd')\n",
        "        hed_image = gr.Image(source='upload', type=\"numpy\", label='hed')\n",
        "        sketch_image = gr.Image(source='upload', type=\"numpy\", label='sketch')\n",
        "    with gr.Row():\n",
        "        openpose_image = gr.Image(source='upload', type=\"numpy\", label='openpose')\n",
        "        midas_image = gr.Image(source='upload', type=\"numpy\", label='midas')\n",
        "        seg_image = gr.Image(source='upload', type=\"numpy\", label='seg')\n",
        "        content_image = gr.Image(source='upload', type=\"numpy\", label='content')\n",
        "    with gr.Row():\n",
        "        prompt = gr.Textbox(label=\"Prompt\")\n",
        "    with gr.Row():\n",
        "        run_button = gr.Button(label=\"Run\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=4, step=1)\n",
        "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
        "                strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=0.75, step=0.01)\n",
        "\n",
        "                global_strength = gr.Slider(label=\"Global Strength\", minimum=0, maximum=2, value=1, step=0.01)\n",
        "                \n",
        "                low_threshold = gr.Slider(label=\"Canny Low Threshold\", minimum=1, maximum=255, value=100, step=1)\n",
        "                high_threshold = gr.Slider(label=\"Canny High Threshold\", minimum=1, maximum=255, value=200, step=1)\n",
        "\n",
        "                value_threshold = gr.Slider(label=\"Hough Value Threshold (MLSD)\", minimum=0.01, maximum=2.0, value=0.1, step=0.01)\n",
        "                distance_threshold = gr.Slider(label=\"Hough Distance Threshold (MLSD)\", minimum=0.01, maximum=20.0, value=0.1, step=0.01)\n",
        "                alpha = gr.Slider(label=\"Alpha\", minimum=0.1, maximum=20.0, value=6.2, step=0.01)\n",
        "                \n",
        "                ddim_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=7.5, step=0.1)\n",
        "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647,  value=42, step=1)\n",
        "                eta = gr.Number(label=\"Eta (DDIM)\", value=0.0)\n",
        "                \n",
        "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality, extremely detailed')\n",
        "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
        "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
        "        \n",
        "    with gr.Row():\n",
        "        image_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=4, height='auto')\n",
        "    with gr.Row():\n",
        "        cond_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=4, height='auto')\n",
        "    \n",
        "    ips = [canny_image, mlsd_image, hed_image, sketch_image, openpose_image, midas_image, seg_image, content_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta, low_threshold, high_threshold, value_threshold, distance_threshold, alpha, global_strength]\n",
        "    run_button.click(fn=process, inputs=ips, outputs=[image_gallery, cond_gallery])\n",
        "\n",
        "\n",
        "# block.launch(server_name='0.0.0.0')\n",
        "block.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "98MImSbeplF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}