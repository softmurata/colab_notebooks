{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTdMkcJ70QG4m6sHCzaxnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/fastcomposer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "TraH0ZvGuBhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gR57XDHdMrm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate datasets evaluate diffusers xformers triton scipy clip\n",
        "!git clone https://github.com/mit-han-lab/fastcomposer.git\n",
        "%cd fastcomposer\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "L2UcDVFauDIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fastcomposer\n",
        "!mkdir -p model/fastcomposer\n",
        "%cd model/fastcomposer\n",
        "!wget https://huggingface.co/mit-han-lab/fastcomposer/resolve/main/pytorch_model.bin"
      ],
      "metadata": {
        "id": "Mph0F1DUddPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "DoB_Y3lDtnZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fastcomposer/inference.py\n",
        "\"\"\"\n",
        "from fastcomposer.transforms import get_object_transforms\n",
        "from fastcomposer.data import DemoDataset\n",
        "from fastcomposer.model import FastComposerModel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import CLIPTokenizer\n",
        "from accelerate.utils import set_seed\n",
        "from fastcomposer.utils import parse_args\n",
        "from accelerate import Accelerator\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from fastcomposer.pipeline import (\n",
        "    stable_diffusion_call_with_references_delayed_conditioning,\n",
        ")\n",
        "import types\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    # Handle the repository creation\n",
        "    if accelerator.is_main_process:\n",
        "        if args.output_dir is not None:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        args.pretrained_model_name_or_path, torch_dtype=weight_dtype\n",
        "    )\n",
        "\n",
        "    # add own\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    \n",
        "    model = FastComposerModel.from_pretrained(args)\n",
        "\n",
        "    ckpt_name = \"pytorch_model.bin\"\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(Path(args.finetuned_model_path) / ckpt_name, map_location=\"cuda\")\n",
        "    ) # change cpu -> cuda\n",
        "\n",
        "    model = model.to(device=accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    pipe.unet = model.unet\n",
        "\n",
        "    if args.enable_xformers_memory_efficient_attention:\n",
        "        pipe.unet.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    pipe.text_encoder = model.text_encoder\n",
        "    pipe.image_encoder = model.image_encoder\n",
        "\n",
        "    pipe.postfuse_module = model.postfuse_module\n",
        "\n",
        "    pipe.inference = types.MethodType(\n",
        "        stable_diffusion_call_with_references_delayed_conditioning, pipe\n",
        "    )\n",
        "\n",
        "    del model\n",
        "\n",
        "    pipe = pipe.to(accelerator.device)\n",
        "\n",
        "    # Set up the dataset\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        args.pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\",\n",
        "        revision=args.revision,\n",
        "    )\n",
        "\n",
        "    object_transforms = get_object_transforms(args)\n",
        "\n",
        "    demo_dataset = DemoDataset(\n",
        "        test_caption=args.test_caption,\n",
        "        test_reference_folder=args.test_reference_folder,\n",
        "        tokenizer=tokenizer,\n",
        "        object_transforms=object_transforms,\n",
        "        device=accelerator.device,\n",
        "        max_num_objects=args.max_num_objects,\n",
        "    )\n",
        "\n",
        "    image_ids = os.listdir(args.test_reference_folder)\n",
        "    print(f\"Image IDs: {image_ids}\")\n",
        "    demo_dataset.set_image_ids(image_ids)\n",
        "\n",
        "    unique_token = \"<|image|>\"\n",
        "\n",
        "    prompt = args.test_caption\n",
        "    prompt_text_only = prompt.replace(unique_token, \"\")\n",
        "\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    batch = demo_dataset.get_data()\n",
        "\n",
        "    input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
        "    text = tokenizer.batch_decode(input_ids)[0]\n",
        "    print(prompt)\n",
        "    # print(input_ids)\n",
        "    image_token_mask = batch[\"image_token_mask\"].to(accelerator.device)\n",
        "\n",
        "    # print(image_token_mask)\n",
        "    all_object_pixel_values = (\n",
        "        batch[\"object_pixel_values\"].unsqueeze(0).to(accelerator.device)\n",
        "    )\n",
        "    num_objects = batch[\"num_objects\"].unsqueeze(0).to(accelerator.device)\n",
        "\n",
        "    all_object_pixel_values = all_object_pixel_values.to(\n",
        "        dtype=weight_dtype, device=accelerator.device\n",
        "    )\n",
        "\n",
        "    object_pixel_values = all_object_pixel_values  # [:, 0, :, :, :]\n",
        "    if pipe.image_encoder is not None:\n",
        "        global_object_embeds = pipe.image_encoder(object_pixel_values)\n",
        "    else:\n",
        "        global_object_embeds = None\n",
        "\n",
        "    encoder_hidden_states = pipe.text_encoder(\n",
        "        input_ids, image_token_mask, global_object_embeds, num_objects\n",
        "    )[0]\n",
        "\n",
        "    encoder_hidden_states_text_only = pipe._encode_prompt(\n",
        "        prompt_text_only,\n",
        "        accelerator.device,\n",
        "        args.num_images_per_prompt,\n",
        "        do_classifier_free_guidance=False,\n",
        "    )\n",
        "\n",
        "    encoder_hidden_states = pipe.postfuse_module(\n",
        "        encoder_hidden_states,\n",
        "        global_object_embeds,\n",
        "        image_token_mask,\n",
        "        num_objects,\n",
        "    )\n",
        "\n",
        "    cross_attention_kwargs = {}\n",
        "\n",
        "    images = pipe.inference(\n",
        "        prompt_embeds=encoder_hidden_states,\n",
        "        num_inference_steps=args.inference_steps,\n",
        "        height=args.generate_height,\n",
        "        width=args.generate_width,\n",
        "        guidance_scale=args.guidance_scale,\n",
        "        num_images_per_prompt=args.num_images_per_prompt,\n",
        "        cross_attention_kwargs=cross_attention_kwargs,\n",
        "        prompt_embeds_text_only=encoder_hidden_states_text_only,\n",
        "        start_merge_step=args.start_merge_step,\n",
        "    ).images\n",
        "\n",
        "    for instance_id in range(args.num_images_per_prompt):\n",
        "        images[instance_id].save(\n",
        "            os.path.join(\n",
        "                args.output_dir,\n",
        "                f\"output_{instance_id}.png\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2oYviJuEtorN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fastcomposer\n",
        "# !bash script/run_inference.sh\n",
        "\n",
        "from accelerate.utils import write_basic_config\n",
        "\n",
        "write_basic_config()\n",
        "\n",
        "CAPTION=\"a man <|image|> and a man <|image|> are reading book together\"\n",
        "DEMO_NAME=\"newton_einstein\"\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 accelerate launch \\\n",
        "    --mixed_precision=fp16 \\\n",
        "    fastcomposer/inference.py \\\n",
        "    --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \\\n",
        "    --finetuned_model_path model/fastcomposer \\\n",
        "    --test_reference_folder data/newton_einstein \\\n",
        "    --test_caption \"a man <|image|> and a man <|image|> are reading book together\" \\\n",
        "    --output_dir outputs/newton_einstein \\\n",
        "    --mixed_precision fp16 \\\n",
        "    --image_encoder_type clip \\\n",
        "    --image_encoder_name_or_path openai/clip-vit-large-patch14 \\\n",
        "    --num_image_tokens 1 \\\n",
        "    --max_num_objects 2 \\\n",
        "    --object_resolution 224 \\\n",
        "    --generate_height 512 \\\n",
        "    --generate_width 512 \\\n",
        "    --num_images_per_prompt 1 \\\n",
        "    --num_rows 1 \\\n",
        "    --seed 42 \\\n",
        "    --guidance_scale 5 \\\n",
        "    --inference_steps 50 \\\n",
        "    --start_merge_step 10 \\\n",
        "    --no_object_augmentation"
      ],
      "metadata": {
        "id": "g8UKq9GHdkwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "display image"
      ],
      "metadata": {
        "id": "S-30wIFBtfjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "display(Image.open(\"/content/fastcomposer/outputs/newton_einstein/output_0.png\"))"
      ],
      "metadata": {
        "id": "Q45IEtWttiFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced"
      ],
      "metadata": {
        "id": "LNNwII92eDIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.softbank.jp/corp/set/data/aboutus/profile/officer/img/officer-01.jpg -P /content/fastcomposer/data/son_bill/son\n",
        "!wget https://images.forbesjapan.com/media/article/60581/images/main_image_ef0d8efdc943d72876e65a13a5e957af3b954661.jpg -P /content/fastcomposer/data/son_bill/bill"
      ],
      "metadata": {
        "id": "0LY_EltzeEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fastcomposer\n",
        "# !bash script/run_inference.sh\n",
        "\n",
        "from accelerate.utils import write_basic_config\n",
        "\n",
        "write_basic_config()\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 accelerate launch \\\n",
        "    --mixed_precision=fp16 \\\n",
        "    fastcomposer/inference.py \\\n",
        "    --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \\\n",
        "    --finetuned_model_path model/fastcomposer \\\n",
        "    --test_reference_folder data/son_bill \\\n",
        "    --test_caption \"a man <|image|> and a man <|image|> are reading book together\" \\\n",
        "    --output_dir outputs/son_bill \\\n",
        "    --mixed_precision fp16 \\\n",
        "    --image_encoder_type clip \\\n",
        "    --image_encoder_name_or_path openai/clip-vit-large-patch14 \\\n",
        "    --num_image_tokens 1 \\\n",
        "    --max_num_objects 2 \\\n",
        "    --object_resolution 224 \\\n",
        "    --generate_height 512 \\\n",
        "    --generate_width 512 \\\n",
        "    --num_images_per_prompt 1 \\\n",
        "    --num_rows 1 \\\n",
        "    --seed 42 \\\n",
        "    --guidance_scale 5 \\\n",
        "    --inference_steps 50 \\\n",
        "    --start_merge_step 10 \\\n",
        "    --no_object_augmentation"
      ],
      "metadata": {
        "id": "bFUXWvcEe6YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "display(Image.open(\"/content/fastcomposer/outputs/son_bill/output_0.png\"))"
      ],
      "metadata": {
        "id": "XQPZBgeVwDh_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}