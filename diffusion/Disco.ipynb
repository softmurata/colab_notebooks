{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6Nc9AjU6iVKE/OOZQoI8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/Disco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "bDuikuYzRhM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUz6vYPG0E-N"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Wangt-CN/DisCo.git\n",
        "\n",
        "!pip install progressbar psutil pymongo simplejson yacs boto3 pyyaml ete3 easydict deprecated future django orderedset python-magic datasets h5py omegaconf einops ipdb\n",
        "!pip install git+https://github.com/microsoft/azfuse.git\n",
        "\n",
        "## for acceleration\n",
        "!pip install deepspeed\n",
        "!pip install xformers\n",
        "\n",
        "!pip install colorlog deepdish configobj json_lines pytorch-lightning\n",
        "!pip install transformers==4.27.4 kornia==0.6\n",
        "!pip install tensorboardX\n",
        "!pip install diffusers==0.14.0\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "ATJJtoowRinv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/disco-checkpoint-share/checkpoint_ft/moretiktok_cfg/mp_rank_00_model_states.pt -P /content"
      ],
      "metadata": {
        "id": "cbQ2ujCCReLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference preparation"
      ],
      "metadata": {
        "id": "rnbm7FXGRk5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DisCo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmAEP_FGJCad",
        "outputId": "1a950b30-8dd4-4745-e48b-2722b8be77be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DisCo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app_demo_image_edit.py\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from config import *\n",
        "from config.ref_attn_clip_combine_controlnet.net import Net, inner_collect_fn\n",
        "\n",
        "class Args(BasicArgs):\n",
        "    task_name, method_name = BasicArgs.parse_config_name(__file__)\n",
        "    log_dir = os.path.join(BasicArgs.root_dir, task_name, method_name)\n",
        "\n",
        "    # data\n",
        "    # dataset_cf = 'dataset/app_demo_human_image_edit_gradio.py'\n",
        "    dataset_cf = 'dataset/app_demo_human_image_edit_singleinput.py'\n",
        "    # dataset_cf = 'dataset/for_vis/tiktok_controlnet_t2i_imagevar_combine_evalwebattr_pose_specific.py'\n",
        "    max_train_samples = None\n",
        "    max_eval_samples = None\n",
        "    # max_eval_samples = 2\n",
        "    max_video_len = 1  # L=16\n",
        "    debug_max_video_len = 1\n",
        "    img_full_size = (256, 256)\n",
        "    img_size = (256, 256)\n",
        "    fps = 5\n",
        "    data_dir = \"./blob_dir/data/mtp_vlp_ray/debug/debug_pretrain\"\n",
        "    debug_train_yaml = './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/train_webvid2.5m_2.yaml'\n",
        "    debug_val_yaml = './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/val_webvid2.5m.yaml'\n",
        "    train_yaml = './blob_dir/debug_output/video_sythesis/dataset/composite/train_webvid10m_a_54.yaml'\n",
        "    val_yaml = './blob_dir/debug_output/video_sythesis/dataset/composite/val_webvid10m_a.yaml'\n",
        "    web_data_root = '/datadrive_d/wangtan/azure_storage/vigstandard_data/linjli/debug_output/video_sythesis/dataset/Lindsey_0504_youtube/frames/single_person'\n",
        "    # WT: for tiktok image data dir\n",
        "    tiktok_data_root = 'keli/dataset/TikTok_dataset/'\n",
        "    # tiktok_ann_root = 'keli/dataset/TikTok_dataset/pair_ann'\n",
        "    refer_sdvae = False\n",
        "\n",
        "    eval_before_train = True\n",
        "    eval_enc_dec_only = False\n",
        "\n",
        "    # training\n",
        "    local_train_batch_size = 8\n",
        "    local_eval_batch_size = 8\n",
        "    learning_rate = 3e-5\n",
        "    null_caption = False\n",
        "    refer_sdvae = False\n",
        "\n",
        "\n",
        "    # max_norm = 1.0\n",
        "    epochs = 50\n",
        "    num_workers = 4\n",
        "    eval_step = 5\n",
        "    save_step = 5\n",
        "    drop_text = 1.0 # drop text only activate in args.null_caption, default=1.0\n",
        "    scale_factor = 0.18215\n",
        "    # pretrained_model_path = os.path.join(BasicArgs.root_dir, 'diffusers/stable-diffusion-v2-1')\n",
        "    pretrained_model_path = \"lambdalabs/sd-image-variations-diffusers\" # os.path.join(BasicArgs.root_dir, 'diffusers/sd-image-variations-diffusers')\n",
        "    sd15_path = \"runwayml/stable-diffusion-v1-5\" # os.path.join(BasicArgs.root_dir, 'diffusers/stable-diffusion-v1-5-2')\n",
        "    gradient_checkpointing = True\n",
        "    enable_xformers_memory_efficient_attention = True\n",
        "    freeze_unet=True\n",
        "\n",
        "    # sample\n",
        "    num_inf_images_per_prompt = 1\n",
        "    num_inference_steps = 50\n",
        "    guidance_scale = 7.5\n",
        "\n",
        "    # others\n",
        "    seed = 42\n",
        "    # set_seed(seed)\n",
        "\n",
        "args = Args()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "D0KZiXg-cCYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets/app_demo_human_image_edit_singleinput.py\n",
        "\"\"\"\n",
        "from config import *\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "import os, math, re, json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "training_templates_smallest = [\n",
        "    'photo of a sks {}',\n",
        "]\n",
        "\n",
        "reg_templates_smallest = [\n",
        "    'photo of a {}',\n",
        "]\n",
        "coco_joints_name = ['Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear', 'Left Shoulder', 'Right Shoulder', 'Left Elbow', 'Right Elbow', 'Left Wrist',\n",
        "            'Right Wrist', 'Left Hip', 'Right Hip', 'Left Knee', 'Right Knee', 'Left Ankle', 'Right Ankle', 'Pelvis', 'Neck']\n",
        "\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, args, yaml_file, split='train', preprocesser=None):\n",
        "        self.dataset = \"tiktok\"\n",
        "        self.args = args\n",
        "        self.split = split\n",
        "        self.is_train = split == \"train\"\n",
        "        self.is_composite = False\n",
        "        self.on_memory = getattr(args, 'on_memory', False)\n",
        "        self.img_size = getattr(args, 'img_full_size', args.img_size)\n",
        "        self.max_video_len = 1 ## Todo, now it is image-based dataloader\n",
        "        self.size_frame = 1 ## Todo\n",
        "        self.yaml_file = yaml_file\n",
        "        self.stickwidth = 4\n",
        "        self.preprocesser = preprocesser\n",
        "        self.limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \\\n",
        "                [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \\\n",
        "                [1, 16], [16, 18], [3, 17], [6, 18]]\n",
        "\n",
        "        self.colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
        "                [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
        "                [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                self.img_size,\n",
        "                scale=(1.0, 1.0), ratio=(1., 1.),\n",
        "                interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ])\n",
        "        self.ref_transform = transforms.Compose([ # follow CLIP transform\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomResizedCrop(\n",
        "                (224, 224),\n",
        "                scale=(1.0, 1.0), ratio=(1., 1.),\n",
        "                interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.Normalize([0.48145466, 0.4578275, 0.40821073],\n",
        "                                 [0.26862954, 0.26130258, 0.27577711]),\n",
        "        ])\n",
        "\n",
        "        self.ref_transform_mask = transforms.Compose([  # follow CLIP transform\n",
        "            transforms.RandomResizedCrop(\n",
        "                (224, 224),\n",
        "                scale=(1.0, 1.0), ratio=(1., 1.),\n",
        "                interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.cond_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                self.img_size,\n",
        "                scale=(1.0, 1.0), ratio=(1., 1.),\n",
        "                interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.total_num_videos = 340\n",
        "        self.image_path = '{}/{}'\n",
        "        self.ref_image_path = '{:05d}/images/{:04d}.png'\n",
        "        # self.anno_pose_path = '{}/openpose_json/{:04d}.jpg.json'\n",
        "        self.anno_pose_path = '{:05d}/openpose_json/{:04d}.png.json'\n",
        "        self.anno_path = 'GIT/{:05d}/labels/{:04d}.txt'\n",
        "        self.ref_mask_path = '{}/groundsam/{}.mask.jpg'\n",
        "\n",
        "        self.ref_image_path_web = '{}/{}'\n",
        "        self.anno_pose_path_web = '{}/openpose_json/{}.json'\n",
        "\n",
        "        self.image_paths_list = []\n",
        "        self.ref_image_paths_list = []\n",
        "        self.pose_image_paths_list = []\n",
        "        self.anno_list = []\n",
        "        self.anno_pose_list = []\n",
        "        self.mask_list = []\n",
        "        self.file_name_id = []\n",
        "\n",
        "        assert split == 'val'\n",
        "\n",
        "        # for specific choose the video\n",
        "        ref_fg_folder = '/content/DisCo/demo_data/fg'\n",
        "        ref_bg_folder = '/content/DisCo/demo_data/bg'\n",
        "        ref_pose_folder = '/content/DisCo/demo_data/pose'\n",
        "        # ref_fg_folder = '/home1/wangtan/code/ms_internship2/github_repo/DisCo/demo_data/fg'\n",
        "        # ref_bg_folder = '/home1/wangtan/code/ms_internship2/github_repo/DisCo/demo_data/bg'\n",
        "        # ref_pose_folder = '/home1/wangtan/code/ms_internship2/github_repo/DisCo/demo_data/pose'\n",
        "        bg_list = os.listdir(os.path.join(ref_bg_folder, 'images'))\n",
        "\n",
        "\n",
        "        ref_fg_files_list = os.listdir(os.path.join(ref_fg_folder, 'images'))\n",
        "        # Kevin Ver: always chooose the 1st frame as the referece image\n",
        "        # TODO: WT Revision: if t = n, then reference frm could be between frame(0)~frame(n-1)\n",
        "        for ref_img_name in ref_fg_files_list:\n",
        "            ref_image_path = os.path.join(ref_fg_folder, 'images', ref_img_name)\n",
        "            ref_mask_path = os.path.join(ref_fg_folder, 'masks', ref_img_name)\n",
        "\n",
        "            pose_files_list = os.listdir(ref_pose_folder)\n",
        "            for pose_img_name in pose_files_list:\n",
        "                pose_image_path = os.path.join(ref_pose_folder, pose_img_name)\n",
        "\n",
        "                self.pose_image_paths_list.append(pose_image_path) # actually have no gt file, just use the target pose file\n",
        "                self.ref_image_paths_list.append(ref_image_path)\n",
        "                self.mask_list.append(ref_mask_path)\n",
        "                self.file_name_id.append(f'ref{ref_img_name}--pose{pose_img_name}')\n",
        "\n",
        "        self.bgref_ref_image_paths_list = []\n",
        "        self.bgref_mask_list = []\n",
        "        for ref_bg_name in bg_list:\n",
        "            ref_image_fname = os.path.join(ref_bg_folder, 'images', ref_bg_name)\n",
        "            ref_mask_fname = os.path.join(ref_bg_folder, 'masks', ref_bg_name)\n",
        "\n",
        "            self.bgref_ref_image_paths_list.append(ref_image_fname)\n",
        "            self.bgref_mask_list.append(ref_mask_fname)\n",
        "        self.bgref_num_images = len(self.bgref_ref_image_paths_list)\n",
        "\n",
        "\n",
        "        self.num_images = len(self.ref_image_paths_list)\n",
        "        self._length = self.num_images\n",
        "        print('number of samples:',self._length)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            if getattr(self.args, 'max_train_samples', None):\n",
        "                return min(self.args.max_train_samples, self._length)\n",
        "            else:\n",
        "                return self._length\n",
        "        else:\n",
        "            if getattr(self.args, 'max_eval_samples', None):\n",
        "                return min(self.args.max_eval_samples, self._length)\n",
        "            else:\n",
        "                return self._length\n",
        "\n",
        "    def normalize_mask(self, mask):\n",
        "        mask[mask>=0.001] = 1\n",
        "        mask[mask<0.001] = 0\n",
        "        return mask\n",
        "\n",
        "    # draw the body keypoint and lims\n",
        "    def draw_bodypose(self, canvas, pose):\n",
        "        canvas = cv2.cvtColor(np.array(canvas), cv2.COLOR_RGB2BGR)\n",
        "        canvas = np.zeros_like(canvas)\n",
        "\n",
        "        for i in range(18):\n",
        "            x, y = pose[i][0:2]\n",
        "            if x>=0 and y>=0:\n",
        "                cv2.circle(canvas, (int(x), int(y)), 4, self.colors[i], thickness=-1)\n",
        "                # cv2.putText(canvas, '%d'%(i), (int(x), int(y)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "        for limb_idx in range(17):\n",
        "            cur_canvas = canvas.copy()\n",
        "            index_a = self.limbSeq[limb_idx][0]-1\n",
        "            index_b = self.limbSeq[limb_idx][1]-1\n",
        "\n",
        "            if pose[index_a][0]<0 or pose[index_b][0]<0 or pose[index_a][1]<0 or pose[index_b][1]<0:\n",
        "                continue\n",
        "\n",
        "            Y = [pose[index_a][0], pose[index_b][0]]\n",
        "            X = [pose[index_a][1], pose[index_b][1]]\n",
        "            mX = np.mean(X)\n",
        "            mY = np.mean(Y)\n",
        "            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
        "            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
        "            polygon = cv2.ellipse2Poly((int(mY), int(mX)), (int(length / 2), self.stickwidth), int(angle), 0, 360, 1)\n",
        "            cv2.fillConvexPoly(cur_canvas, polygon, self.colors[limb_idx])\n",
        "            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
        "        # Convert color space from BGR to RGB\n",
        "        # canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)\n",
        "        # Create PIL image object from numpy array\n",
        "        canvas = Image.fromarray(canvas)\n",
        "        return canvas\n",
        "\n",
        "\n",
        "    def coco2openpose(self, img, coco_keypoints):\n",
        "\n",
        "        # coco keypoints: [x1,y1,v1,...,xk,yk,vk]       (k=17)\n",
        "        #     ['Nose', Leye', 'Reye', 'Lear', 'Rear', 'Lsho', 'Rsho', 'Lelb',\n",
        "        #      'Relb', 'Lwri', 'Rwri', 'Lhip', 'Rhip', 'Lkne', 'Rkne', 'Lank', 'Rank']\n",
        "        # openpose keypoints: [y1,...,yk], [x1,...xk]   (k=18, with Neck)\n",
        "        #     ['Nose' (0), *'Neck'* (1), 'Rsho' (2), 'Relb' (3), 'Rwri' (4), 'Lsho' (5), 'Lelb' (6), 'Lwri' (7),'Rhip' (8),\n",
        "        #      'Rkne' (9), 'Rank' (10), 'Lhip' (11), 'Lkne' (12), 'Lank' (13), 'Reye' (14), 'Leye' (15), 'Rear' (16), 'Lear' (17)]\n",
        "\n",
        "        openpose_keypoints = [\n",
        "            coco_keypoints[0], # Nose (0)\n",
        "            list((np.asarray(coco_keypoints[5]) + np.asarray(coco_keypoints[6]))/2), # Neck (1)\n",
        "            coco_keypoints[6], # RShoulder (2)\n",
        "            coco_keypoints[8], # RElbow (3)\n",
        "            coco_keypoints[10], # RWrist (4)\n",
        "            coco_keypoints[5], # LShoulder (5)\n",
        "            coco_keypoints[7], # LElbow (6)\n",
        "            coco_keypoints[9], # LWrist (7)\n",
        "            coco_keypoints[12], # RHip (8)\n",
        "            coco_keypoints[14], # RKnee (9)\n",
        "            coco_keypoints[16], # RAnkle (10)\n",
        "            coco_keypoints[11], # LHip (11)\n",
        "            coco_keypoints[13], # LKnee (12)\n",
        "            coco_keypoints[15], # LAnkle (13)\n",
        "            coco_keypoints[2], # REye (14)\n",
        "            coco_keypoints[1], # LEye (15)\n",
        "            coco_keypoints[4], # REar (16)\n",
        "            coco_keypoints[3], # LEar (17)\n",
        "        ]\n",
        "        return self.draw_bodypose(img, openpose_keypoints)\n",
        "\n",
        "    def load_image(self, path):\n",
        "        if os.path.exists(path):\n",
        "            image = Image.open(path)\n",
        "            if not image.mode == \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "        else:\n",
        "            image = None\n",
        "        return image\n",
        "\n",
        "    def load_mask(self, path):\n",
        "        try:\n",
        "            if os.path.exists(path):\n",
        "                img = self.load_image(path)\n",
        "                img = np.asarray(img)\n",
        "                bk = img[:, :, :] == [68, 0, 83]\n",
        "                fg = (bk == False)\n",
        "                fg = fg * 255.0\n",
        "                mask = fg.astype(np.uint8)\n",
        "                ipl_mask = Image.fromarray(mask)\n",
        "            else:\n",
        "                ipl_mask = None\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            ipl_mask = None\n",
        "        return ipl_mask\n",
        "\n",
        "    def load_mask_tiktok(self, path):\n",
        "        if os.path.exists(path):\n",
        "            image = Image.open(path)\n",
        "            if not image.mode == \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "        else:\n",
        "            image = None\n",
        "        return image\n",
        "\n",
        "    def load_openpose(self, anno_pose_path, ref_image):\n",
        "        # Load detected openpose keypoint json file\n",
        "        pose_without_visibletag = []\n",
        "        f = open(anno_pose_path, 'r')\n",
        "        d = json.load(f)\n",
        "        # if there is a valid openpose skeleton, load it\n",
        "        if len(d) > 0:\n",
        "            for j in range(17):\n",
        "                x = d[0]['keypoints'][j][0]\n",
        "                y = d[0]['keypoints'][j][1]\n",
        "                pose_without_visibletag.append([x, y])\n",
        "        else:  # if there is not valid openpose skeleton, add a dummy one\n",
        "            for j in range(17):\n",
        "                x = -1\n",
        "                y = -1\n",
        "                pose_without_visibletag.append([x, y])\n",
        "\n",
        "                # convert coordinates to skeleton image\n",
        "        skeleton_img = self.coco2openpose(ref_image, pose_without_visibletag)\n",
        "        return skeleton_img\n",
        "\n",
        "    def get_img_txt_pair(self, idx):\n",
        "        img_path = self.ref_image_paths_list[idx % self.num_images]\n",
        "        ref_img_path = self.ref_image_paths_list[idx % self.num_images]\n",
        "        ref_mask_path = self.mask_list[idx % self.num_images]\n",
        "        anno_pose_path = self.pose_image_paths_list[idx % self.num_images]\n",
        "        img_key =  self.file_name_id[idx % self.num_images]\n",
        "\n",
        "        ref_mask = self.load_mask_tiktok(ref_mask_path)\n",
        "        ref_image = Image.open(ref_img_path)\n",
        "        if not ref_image.mode == \"RGB\":\n",
        "            ref_image = ref_image.convert(\"RGB\")\n",
        "        ref_mask = ref_mask.resize(ref_image.size) # resize the mask to img\n",
        "        img = Image.open(img_path)\n",
        "        if not img.mode == \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "\n",
        "        # Load detected openpose keypoint json file\n",
        "        pose_without_visibletag = []\n",
        "        f = open(anno_pose_path,'r')\n",
        "        d = json.load(f)\n",
        "        # if there is a valid openpose skeleton, load it\n",
        "        if len(d)>0:\n",
        "            for j in range(17):\n",
        "                x = d[0]['keypoints'][j][0]\n",
        "                y = d[0]['keypoints'][j][1]\n",
        "                pose_without_visibletag.append([x,y])\n",
        "        else: # if there is not valid openpose skeleton, add a dummy one\n",
        "            for j in range(17):\n",
        "                x = -1\n",
        "                y = -1\n",
        "                pose_without_visibletag.append([x,y])\n",
        "\n",
        "        # convert coordinates to skeleton image\n",
        "        skeleton_img = self.coco2openpose(ref_image, pose_without_visibletag)\n",
        "\n",
        "        # preparing outputs\n",
        "        meta_data = {}\n",
        "        meta_data['img'] = img\n",
        "        meta_data['img_key'] = img_key\n",
        "        meta_data['is_video'] = False\n",
        "        meta_data['skeleton_img'] = skeleton_img\n",
        "        meta_data['reference_img'] = ref_image\n",
        "        meta_data['ref_mask'] = ref_mask\n",
        "        return meta_data\n",
        "\n",
        "    def augmentation(self, frame, transform, state=None):\n",
        "        if state is not None:\n",
        "            torch.set_rng_state(state)\n",
        "        return transform(frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw_data = self.get_img_txt_pair(idx)\n",
        "        img = raw_data['img']\n",
        "        skeleton_img = raw_data['skeleton_img']\n",
        "        reference_img = raw_data['reference_img']\n",
        "        img_key = raw_data['img_key']\n",
        "\n",
        "\n",
        "        ### random sample background\n",
        "        ref_bg_idx = random.choice(list(range(0, self.bgref_num_images)))\n",
        "        ref_bg_img_path = self.bgref_ref_image_paths_list[ref_bg_idx]\n",
        "        ref_bg_ref_mask_path = os.path.join(self.args.web_data_root, self.bgref_mask_list[ref_bg_idx])\n",
        "        ref_bg_image = Image.open(os.path.join(self.args.web_data_root, ref_bg_img_path))\n",
        "        ref_bg_ref_mask = self.load_mask_tiktok(ref_bg_ref_mask_path)\n",
        "        if not ref_bg_image.mode == \"RGB\":\n",
        "            ref_bg_image = ref_bg_image.convert(\"RGB\")\n",
        "        ref_bg_ref_mask = ref_bg_ref_mask.resize(ref_bg_image.size)  # resize the mask to img\n",
        "\n",
        "        img_key = img_key + '_{}'.format(ref_bg_img_path.split('/')[-1])\n",
        "\n",
        "\n",
        "        reference_img_controlnet = reference_img\n",
        "        state = torch.get_rng_state()\n",
        "        img = self.augmentation(img, self.transform, state)\n",
        "        skeleton_img = self.augmentation(skeleton_img, self.cond_transform, state)\n",
        "        reference_img_controlnet = self.augmentation(reference_img_controlnet, self.transform, state)\n",
        "        ref_bg_reference_img_controlnet = self.augmentation(ref_bg_image, self.transform, state)\n",
        "\n",
        "        reference_img_vae = reference_img_controlnet\n",
        "        if getattr(self.args, 'refer_clip_preprocess', None):\n",
        "            reference_img = self.preprocesser(reference_img).pixel_values[0] # use clip preprocess\n",
        "        else:\n",
        "            reference_img = self.augmentation(reference_img, self.ref_transform)\n",
        "\n",
        "        if self.args.combine_use_mask:\n",
        "            mask_img_ref = raw_data['ref_mask']\n",
        "            assert not getattr(self.args, 'refer_clip_preprocess', None) # mask not support the CLIP process\n",
        "\n",
        "            # ### first resize mask to the img size\n",
        "            mask_img_ref = mask_img_ref.resize(raw_data['reference_img'].size)\n",
        "\n",
        "            reference_img_mask = self.augmentation(mask_img_ref, self.ref_transform_mask, state)\n",
        "            reference_img_controlnet_mask = self.augmentation(mask_img_ref, self.cond_transform, state)  # controlnet path input\n",
        "            ref_bg_reference_img_controlnet_mask = self.augmentation(ref_bg_ref_mask, self.cond_transform, state)  # controlnet path input\n",
        "\n",
        "            # linshi wangtan\n",
        "            reference_img_mask = self.normalize_mask(reference_img_mask)\n",
        "            reference_img_controlnet_mask = self.normalize_mask(reference_img_controlnet_mask)\n",
        "            ref_bg_reference_img_controlnet_mask = self.normalize_mask(ref_bg_reference_img_controlnet_mask)\n",
        "\n",
        "            # apply the mask\n",
        "            reference_img = reference_img * reference_img_mask# foreground\n",
        "            reference_img_vae = reference_img_vae * reference_img_controlnet_mask # foreground, but for vae\n",
        "            # reference_img_controlnet = reference_img_controlnet * (1 - reference_img_controlnet_mask)# background\n",
        "            reference_img_controlnet = ref_bg_reference_img_controlnet * (1 - ref_bg_reference_img_controlnet_mask)  # background\n",
        "\n",
        "        outputs = {'img_key':img_key, 'label_imgs': img, 'cond_imgs': skeleton_img, 'reference_img': reference_img, 'reference_img_controlnet':reference_img_controlnet, 'reference_img_vae':reference_img_vae}\n",
        "        if self.args.combine_use_mask:\n",
        "            outputs['background_mask'] = (1 - reference_img_mask)\n",
        "            outputs['background_mask_controlnet'] = (1 - reference_img_controlnet_mask)\n",
        "        outputs['save_filename'] = self.file_name_id[idx % self.num_images]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def preprocess_input(self, reference_img, fg_mask, ref_bg_image, bg_mask, skeleton_img):\n",
        "        fg_mask = fg_mask.resize(reference_img.size)\n",
        "\n",
        "\n",
        "        reference_img_controlnet = reference_img\n",
        "        state = torch.get_rng_state()\n",
        "        img = self.augmentation(reference_img, self.transform, state)\n",
        "        skeleton_img = self.augmentation(skeleton_img, self.cond_transform, state)\n",
        "        reference_img_controlnet = self.augmentation(reference_img_controlnet, self.transform, state)\n",
        "        ref_bg_reference_img_controlnet = self.augmentation(ref_bg_image, self.transform, state)\n",
        "        reference_img_vae = reference_img_controlnet\n",
        "        reference_img = self.augmentation(reference_img, self.ref_transform)\n",
        "\n",
        "\n",
        "        reference_fg_mask = self.augmentation(fg_mask, self.ref_transform_mask, state)\n",
        "        reference_fg_controlnet_mask = self.augmentation(fg_mask, self.cond_transform, state)  # controlnet path input\n",
        "        ref_bg_reference_img_controlnet_mask = self.augmentation(bg_mask, self.cond_transform, state)  # controlnet path input\n",
        "\n",
        "        # linshi wangtan\n",
        "        reference_img_mask = self.normalize_mask(reference_fg_mask)\n",
        "        reference_img_controlnet_mask = self.normalize_mask(reference_fg_controlnet_mask)\n",
        "        ref_bg_reference_img_controlnet_mask = self.normalize_mask(ref_bg_reference_img_controlnet_mask)\n",
        "\n",
        "        # apply the mask\n",
        "        reference_img = reference_img * reference_img_mask  # foreground\n",
        "        reference_img_vae = reference_img_vae * reference_img_controlnet_mask  # foreground, but for vae\n",
        "        reference_img_controlnet = ref_bg_reference_img_controlnet * (1 - ref_bg_reference_img_controlnet_mask)  # background\n",
        "        outputs = {'label_imgs': img.unsqueeze(0), 'cond_imgs': skeleton_img.unsqueeze(0), 'reference_img': reference_img.unsqueeze(0), 'reference_img_controlnet':reference_img_controlnet.unsqueeze(0), 'reference_img_vae':reference_img_vae.unsqueeze(0)}\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def preprocess_masked_input(self, reference_img_masked, ref_bg_image_masked, skeleton_img):\n",
        "        reference_img = reference_img_masked\n",
        "        ref_bg_image = ref_bg_image_masked\n",
        "\n",
        "        def pil2binary_fg(img):\n",
        "            xx = np.array(img.convert('L'))\n",
        "            xx[xx > 0] = 255\n",
        "            xx[xx < 255] = 0\n",
        "            return xx\n",
        "        def pil2binary_bg(img):\n",
        "            xx = np.array(img.convert('L'))\n",
        "            xx[xx == 0] = 255\n",
        "            xx[xx < 255] = 0\n",
        "            return xx\n",
        "\n",
        "        fg_mask = Image.fromarray(pil2binary_fg(reference_img)).convert('RGB')\n",
        "        bg_mask = Image.fromarray(pil2binary_bg(ref_bg_image)).convert('RGB')\n",
        "\n",
        "        fg_mask = fg_mask.resize(reference_img.size)\n",
        "\n",
        "        reference_img_controlnet = reference_img\n",
        "        state = torch.get_rng_state()\n",
        "        img = self.augmentation(reference_img, self.transform, state)\n",
        "        skeleton_img = self.augmentation(skeleton_img, self.cond_transform, state)\n",
        "        reference_img_controlnet = self.augmentation(reference_img_controlnet, self.transform, state)\n",
        "        ref_bg_reference_img_controlnet = self.augmentation(ref_bg_image, self.transform, state)\n",
        "        reference_img_vae = reference_img_controlnet\n",
        "        reference_img = self.augmentation(reference_img, self.ref_transform)\n",
        "\n",
        "\n",
        "        reference_fg_mask = self.augmentation(fg_mask, self.ref_transform_mask, state)\n",
        "        reference_fg_controlnet_mask = self.augmentation(fg_mask, self.cond_transform, state)  # controlnet path input\n",
        "        ref_bg_reference_img_controlnet_mask = self.augmentation(bg_mask, self.cond_transform, state)  # controlnet path input\n",
        "\n",
        "        # linshi wangtan\n",
        "        reference_img_mask = self.normalize_mask(reference_fg_mask)\n",
        "        reference_img_controlnet_mask = self.normalize_mask(reference_fg_controlnet_mask)\n",
        "        ref_bg_reference_img_controlnet_mask = self.normalize_mask(ref_bg_reference_img_controlnet_mask)\n",
        "\n",
        "        # apply the mask\n",
        "        reference_img = reference_img * reference_img_mask  # foreground\n",
        "        reference_img_vae = reference_img_vae * reference_img_controlnet_mask  # foreground, but for vae\n",
        "        reference_img_controlnet = ref_bg_reference_img_controlnet * (1 - ref_bg_reference_img_controlnet_mask)  # background\n",
        "        outputs = {'label_imgs': img.unsqueeze(0), 'cond_imgs': skeleton_img.unsqueeze(0), 'reference_img': reference_img.unsqueeze(0), 'reference_img_controlnet':reference_img_controlnet.unsqueeze(0), 'reference_img_vae':reference_img_vae.unsqueeze(0)}\n",
        "        return outputs\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eRBUA33ncMa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"WANDB_ENABLE\"] = \"0\"\n",
        "\n",
        "from utils.wutils_ldm import *\n",
        "from agent import Agent_LDM, WarmupLinearLR, WarmupLinearConstantLR\n",
        "import torch\n",
        "from config import BasicArgs\n",
        "from utils.lib import *\n",
        "# from utils.args import parse_with_cf\n",
        "from utils.dist import dist_init\n",
        "from dataset.tsv_dataset import make_data_sampler, make_batch_data_sampler\n",
        "from finetune_sdm_yaml import get_loader_info, make_data_loader\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')"
      ],
      "metadata": {
        "id": "416NTaas0qIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.args import sharedArgs\n",
        "manual_args = ['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True', '--root_dir', '/content/DisCo', '--local_train_batch_size', '32', '--local_eval_batch_size', '32', '--log_dir', 'exp/tiktok_ft', '--epochs', '20', '--deepspeed', '--eval_step', '500',\n",
        "               '--save_step', '500', '--gradient_accumulate_steps', '1', '--learning_rate', '2e-4', '--fix_dist_seed', 'True', '--loss_target',\n",
        "               'noise', '--unet_unfreeze_type', 'all', '--guidance_scale', '3', '--refer_sdvae', 'True', '--ref_null_caption', 'False', '--combine_clip_local', 'True', '--combine_use_mask', 'True', '--conds', 'poses','masks', '--pretrained_model', '/content/mp_rank_00_model_states.pt', '--eval_save_filename', 'try']\n",
        "parsed_args = sharedArgs.parser.parse_args(args=manual_args)\n",
        "# args = sharedArgs.parser.parse_args(args=['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True'])\n",
        "# import pdb; pdb.set_trace()\n",
        "\n",
        "###### process the args #######\n",
        "if parsed_args.root_dir:\n",
        "    BasicArgs.root_dir = parsed_args.root_dir\n",
        "else:\n",
        "    parsed_args.root_dir = BasicArgs.root_dir\n",
        "parsed_args.pretrained_model_path = os.path.join(parsed_args.root_dir, parsed_args.pretrained_model_path)\n",
        "\n",
        "def parse_with_cf(parsed_args):\n",
        "    \"\"\"This function will set args based on the input config file.\n",
        "    (1) it only overwrites unset parameters,\n",
        "        i.e., these parameters not set from user command line input\n",
        "    (2) it also sets configs in the config file but declared in the parser\n",
        "    \"\"\"\n",
        "    # convert to EasyDict object,\n",
        "    # enabling access from attributes even for nested config\n",
        "    # e.g., args.train_datasets[0].name\n",
        "    args = edict(vars(parsed_args))\n",
        "    if os.path.exists(parsed_args.cf):\n",
        "        cf = import_filename(parsed_args.cf)\n",
        "        config_args = edict(vars(cf.Args))\n",
        "        override_keys = {arg[2:].split(\"=\")[0] for arg in manual_args\n",
        "                         if arg.startswith(\"--\")}\n",
        "        # import pdb;pdb.set_trace()\n",
        "        for k, v in config_args.items():\n",
        "            if k not in override_keys:\n",
        "                setattr(args, k, v)\n",
        "    else:\n",
        "        raise NotImplementedError('Config filename %s does not exist.' % args.cf)\n",
        "    return args\n",
        "\n",
        "args = parse_with_cf(parsed_args)\n",
        "\n",
        "args.n_gpu = T.cuda.device_count() # local size\n",
        "args.local_size = args.n_gpu\n",
        "if args.root_dir not in args.log_dir:\n",
        "    args.log_dir = os.path.join(args.root_dir, args.log_dir)\n",
        "if args.stepwise_sample_depth == -1:\n",
        "    args.interpolation = None\n",
        "    args.interpolate_mode = None\n",
        "if args.interpolation != \"interpolate\":\n",
        "    args.interpolate_mode = None\n",
        "\n",
        "assert args.eval_step > 0, \"eval_step must be positive\"\n",
        "assert args.save_step > 0, \"save_step must be positive\"\n",
        "\n",
        "dist_init(args)\n",
        "args.dist = args.distributed\n",
        "args.nodes = args.num_nodes\n",
        "args.world_size = args.num_gpus\n",
        "args.train_batch_size = args.local_train_batch_size * args.world_size\n",
        "args.eval_batch_size = args.local_eval_batch_size * args.world_size\n",
        "#############################################\n",
        "\n",
        "cf = import_filename(args.cf)\n",
        "Net, inner_collect_fn = cf.Net, cf.inner_collect_fn\n",
        "\n",
        "dataset_cf = import_filename(args.dataset_cf)\n",
        "BaseDataset = dataset_cf.BaseDataset\n",
        "\n",
        "# args = update_args(parsed_args, args)\n",
        "\n",
        "# init models\n",
        "logger.info('Building models...')\n",
        "model = Net(args)\n",
        "print(f\"Args: {edict(vars(args))}\")"
      ],
      "metadata": {
        "id": "sQqgJfb12Tiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.warning(\"Do eval_visu...\")\n",
        "if getattr(args, 'refer_clip_preprocess', None):\n",
        "    eval_dataset = BaseDataset(args, args.val_yaml, split='val', preprocesser=model.feature_extractor)\n",
        "else:\n",
        "    eval_dataset = BaseDataset(args, args.val_yaml, split='val')\n",
        "eval_dataloader, eval_info = make_data_loader(\n",
        "    args, args.local_eval_batch_size,\n",
        "    eval_dataset)\n",
        "\n",
        "\n",
        "trainer = Agent_LDM(args=args, model=model)\n",
        "trainer.eval_demo_pre()"
      ],
      "metadata": {
        "id": "M5iCbhhMT161"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start WebUI"
      ],
      "metadata": {
        "id": "hHmD6dCfRqIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "azp9IFzWQaDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image):\n",
        "    if not image.mode == \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img, *args, **kwargs):\n",
        "    reference_fg = load_image(reference_fg)\n",
        "    fg_mask = load_image(fg_mask)\n",
        "    ref_bg_image = load_image(ref_bg_image)\n",
        "    bg_mask = load_image(bg_mask)\n",
        "    skeleton_img = load_image(skeleton_img)\n",
        "\n",
        "    input_data = [reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img]\n",
        "    output_image = trainer.eval_demo_run(input_data, eval_dataset=eval_dataset)\n",
        "    return output_image\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference_masked(reference_fg, ref_bg_image, skeleton_img, *args, **kwargs):\n",
        "    reference_fg = load_image(reference_fg)\n",
        "    ref_bg_image = load_image(ref_bg_image)\n",
        "    skeleton_img = load_image(skeleton_img)\n",
        "\n",
        "    input_data = [reference_fg, ref_bg_image, skeleton_img]\n",
        "    output_image = trainer.eval_demo_run_masked(input_data, eval_dataset=eval_dataset)\n",
        "    return output_image"
      ],
      "metadata": {
        "id": "zuODpyGjQnH7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "'''\n",
        "launch app\n",
        "'''\n",
        "title = \"DisCo Demo (Video Demo Comming Soon!)\"\n",
        "description = \"\"\"<p style='text-align: center'> <a href='https://disco-dance.github.io/' target='_blank'>Project Page</a> | <a href='https://arxiv.org/pdf/2212.11270.pdf' target='_blank'>Paper</a> | <a href='https://github.com/microsoft/X-Decoder' target='_blank'>Github Repo</a> | <a href='https://youtu.be/wYp6vmyolqE' target='_blank'>Video</a> </p>\n",
        "<p>Skip the queue by duplicating this space and upgrading to GPU in settings</p>\n",
        "<a href=\"https://huggingface.co/spaces/xdecoder/Demo?duplicate=true\"><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    # DisCo Demo (Video Demo Comming Soon!)\n",
        "    Start edit the human with provided human foreground, background, pose.\n",
        "\n",
        "    Note that for self-uploaded images, TikTok-Style human images are preferred.\n",
        "\n",
        "    [Project Page](https://disco-dance.github.io/) | [Github](https://github.com/Wangt-CN/DisCo)\n",
        "    \"\"\")\n",
        "    #     gr.Markdown(\n",
        "#     \"\"\"\n",
        "#     ## DisCo Demo (Video Demo Comming Soon!)\n",
        "#     <p style='text-align: center'> <a href='https://disco-dance.github.io/' target='_blank'>Project Page</a> | <a href='https://arxiv.org/pdf/2212.11270.pdf' target='_blank'>Paper</a> | <a href='https://github.com/microsoft/X-Decoder' target='_blank'>Github Repo</a> | <a href='https://youtu.be/wYp6vmyolqE' target='_blank'>Video</a> </p>\n",
        "# <p>Skip the queue by duplicating this space and upgrading to GPU in settings</p>\n",
        "# <a href=\"https://huggingface.co/spaces/xdecoder/Demo?duplicate=true\"><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
        "#     \"\"\")\n",
        "\n",
        "    with gr.Row().style(equal_height=False):\n",
        "        with gr.Column(min_width=400, scale=2):\n",
        "            input_fg = gr.Image(type='pil',label=\"Foreground Image\")\n",
        "            gr.Examples(examples=[\"./demo_data/fg/masked_images/00035.png\", \"./demo_data/fg/masked_images/00335.png\", \"./demo_data/fg/masked_images/00147.png\", \"./demo_data/fg/masked_images/00072.png\", \"./demo_data/fg/masked_images/00115.png\"], inputs=input_fg)\n",
        "\n",
        "            input_bg = gr.Image(type='pil',label=\"Background Image\")\n",
        "            gr.Examples(examples=[\"./demo_data/bg/masked_images/00035.png\", \"./demo_data/bg/masked_images/00335.png\", \"./demo_data/bg/masked_images/00147.png\", \"./demo_data/bg/masked_images/00072.png\", \"./demo_data/bg/masked_images/00115.png\"], inputs=input_bg)\n",
        "\n",
        "            input_pose = gr.Image(type='pil',label=\"Target Pose\",scale=1)\n",
        "            gr.Examples(examples=[\"./demo_data/pose_img/0049.png\",\"./demo_data/pose_img/0198.png\",\"./demo_data/pose_img/0213.png\",\"./demo_data/pose_img/0264.png\",\"./demo_data/pose_img/0144.png\",\"./demo_data/pose_img/0054.png\"], inputs=input_pose)\n",
        "\n",
        "            btn = gr.Button(\"Generate\")\n",
        "\n",
        "\n",
        "        with gr.Column(min_width=150):\n",
        "            output_img = gr.Image(type='pil',label=\"Edited Human Image\")\n",
        "\n",
        "    btn.click(inference_masked, inputs=[input_fg, input_bg, input_pose], outputs=[output_img])\n",
        "\n",
        "demo.queue(concurrency_count=2)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "Ak5aq-d2QWdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}