{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTVpE06chjANpZ8LgLDbyC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/UniDiffuser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "u9TZNj58eLOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Ogn1ikeDiB"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text to Image"
      ],
      "metadata": {
        "id": "8nxIWgRDeqTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import UniDiffuserPipeline"
      ],
      "metadata": {
        "id": "N4tlZ-7CeNxH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id_or_path = \"thu-ml/unidiffuser-v1\"\n",
        "pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "# This mode can be inferred from the input provided to the `pipe`. \n",
        "pipe.set_text_to_image_mode()"
      ],
      "metadata": {
        "id": "s6gPIkkheYEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"an elephant under the sea\"\n",
        "sample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0).images[0]\n",
        "display(sample)"
      ],
      "metadata": {
        "id": "qFaUnBR6eZmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.save(f\"{prompt}.jpg\")"
      ],
      "metadata": {
        "id": "kPYZs6UBe-VY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image to Text"
      ],
      "metadata": {
        "id": "idm5jagYeulf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import UniDiffuserPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"thu-ml/unidiffuser-v1\"\n",
        "pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe.to(device)"
      ],
      "metadata": {
        "id": "bQhPMdUaevoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Image-to-text generation\n",
        "init_image = Image.open(\"/content/an elephant under the sea.jpg\").resize((512, 512))\n",
        "\n",
        "sample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\n",
        "i2t_text = sample.text[0]\n",
        "print(i2t_text)"
      ],
      "metadata": {
        "id": "AkfqpXfjfK2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image variation"
      ],
      "metadata": {
        "id": "ZQFeq6Mxfohw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import UniDiffuserPipeline\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"thu-ml/unidiffuser-v1\"\n",
        "pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe.to(device)\n",
        "\n",
        "# Image variation can be performed with a image-to-text generation followed by a text-to-image generation:\n",
        "init_image = Image.open(\"/content/an elephant under the sea.jpg\").resize((512, 512))\n",
        "\n",
        "sample = pipe(image=init_image, num_inference_steps=20, guidance_scale=8.0)\n",
        "i2t_text = sample.text[0]\n",
        "print(i2t_text)\n",
        "\n",
        "# 2. Text-to-image generation\n",
        "sample = pipe(prompt=i2t_text, num_inference_steps=20, guidance_scale=8.0)\n",
        "final_image = sample.images[0]\n",
        "display(final_image)"
      ],
      "metadata": {
        "id": "8Xy0dANxfp_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text variation"
      ],
      "metadata": {
        "id": "7JEZEvTzgeIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import UniDiffuserPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"thu-ml/unidiffuser-v1\"\n",
        "pipe = UniDiffuserPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
        "pipe.to(device)\n",
        "\n",
        "# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:\n",
        "# 1. Text-to-image generation\n",
        "prompt = \"an elephant under the sea\"\n",
        "\n",
        "sample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\n",
        "t2i_image = sample.images[0]\n",
        "t2i_image.save(\"unidiffuser_text2img_sample_image.png\")\n",
        "\n",
        "# 2. Image-to-text generation\n",
        "sample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)\n",
        "final_prompt = sample.text[0]\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "id": "I_4PvfUxgfVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text variation can be performed with a text-to-image generation followed by a image-to-text generation:\n",
        "# 1. Text-to-image generation\n",
        "prompt = \"an elephant under the sea\"\n",
        "\n",
        "sample = pipe(prompt=prompt, num_inference_steps=20, guidance_scale=8.0)\n",
        "t2i_image = sample.images[0]\n",
        "t2i_image.save(\"unidiffuser_text2img_sample_image.png\")\n",
        "\n",
        "# 2. Image-to-text generation\n",
        "sample = pipe(image=t2i_image, num_inference_steps=20, guidance_scale=8.0)\n",
        "final_prompt = sample.text[0]\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "id": "RU-VeS16iFTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}