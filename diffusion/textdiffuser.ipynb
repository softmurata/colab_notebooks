{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP73f4KCfSBgUS5wpsVU5d2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/textdiffuser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "1Bzz_CXTOkO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FDOzFggapyM"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/microsoft/unilm.git\n",
        "%cd unilm/textdiffuser\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install datasets==2.11.0 tokenizers==0.13.3 transformers==4.27.4 xformers==0.0.16 accelerate==0.18.0 triton==2.0.0.post1 termcolor==2.3.0 tinydb flask\n",
        "!pip install TorchSnooper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/unilm/textdiffuser\n",
        "!git clone https://github.com/huggingface/diffusers\n",
        "!cp ./assets/files/scheduling_ddpm.py ./diffusers/src/diffusers/schedulers/scheduling_ddpm.py\n",
        "!cp ./assets/files/unet_2d_condition.py ./diffusers/src/diffusers/models/unet_2d_condition.py\n",
        "!cp ./assets/files/modeling_utils.py ./diffusers/src/diffusers/models/modeling_utils.py\n",
        "%cd diffusers\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "xWjPG1HUbWZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "4vHcIpJjOh6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/unilm/textdiffuser\n",
        "!wget https://layoutlm.blob.core.windows.net/textdiffuser/textdiffuser-ckpt.zip"
      ],
      "metadata": {
        "id": "q5dkwrfnbkBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip textdiffuser-ckpt.zip\n",
        "!rm -rf textdiffuser-ckpt.zip"
      ],
      "metadata": {
        "id": "4t-kzP3UdBgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "mgkT0Agmib4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change util.py and layout_generator.py"
      ],
      "metadata": {
        "id": "1Pwzy6WPCBQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from PIL import ImageFont\n",
        "font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "font = ImageFont.truetype(font_path, size=128)"
      ],
      "metadata": {
        "id": "qymMocXeGBPA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# util.py\n",
        "\"\"\"\n",
        "# ------------------------------------------\n",
        "# TextDiffuser: Diffusion Models as Text Painters\n",
        "# Paper Link: https://arxiv.org/abs/2305.10855\n",
        "# Code Link: https://github.com/microsoft/unilm/tree/master/textdiffuser\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# This file defines a set of commonly used utility functions.\n",
        "# ------------------------------------------\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import math\n",
        "import shutil\n",
        "import string\n",
        "import textwrap\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFont, ImageDraw, ImageOps\n",
        "import cv2\n",
        "\n",
        "from typing import *\n",
        "\n",
        "# define alphabet and alphabet_dic\n",
        "alphabet = string.digits + string.ascii_lowercase + string.ascii_uppercase + string.punctuation + ' ' # len(aphabet) = 95\n",
        "alphabet_dic = {}\n",
        "for index, c in enumerate(alphabet):\n",
        "    alphabet_dic[c] = index + 1 # the index 0 stands for non-character\n",
        "    \n",
        "\n",
        "def transform_mask(mask_root: str):\n",
        "    \n",
        "    img = cv2.imread(mask_root)\n",
        "    img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    ret, binary = cv2.threshold(gray, 250, 255, cv2.THRESH_BINARY) # pixel value is set to 0 or 255 according to the threshold\n",
        "    return 1 - (binary.astype(np.float32) / 255) \n",
        "\n",
        "\n",
        "def segmentation_mask_visualization(font_path: str, segmentation_mask: np.array):\n",
        "    \n",
        "    segmentation_mask = cv2.resize(segmentation_mask, (64, 64), interpolation=cv2.INTER_NEAREST)\n",
        "    # font = ImageFont.truetype(font_path, 8)\n",
        "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "    font = ImageFont.truetype(font_path, size=8)\n",
        "    blank = Image.new('RGB', (512,512), (0,0,0))\n",
        "    d = ImageDraw.Draw(blank)\n",
        "    for i in range(64):\n",
        "        for j in range(64):\n",
        "            if int(segmentation_mask[i][j]) == 0 or int(segmentation_mask[i][j])-1 >= len(alphabet): \n",
        "                continue\n",
        "            else:\n",
        "                d.text((j*8, i*8), alphabet[int(segmentation_mask[i][j])-1], font=font, fill=(0, 255, 0))\n",
        "    return blank\n",
        "\n",
        "\n",
        "def make_caption_pil(font_path: str, captions: List[str]):\n",
        "    \n",
        "    caption_pil_list = []\n",
        "    # font = ImageFont.truetype(font_path, 18)\n",
        "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "    font = ImageFont.truetype(font_path, size=18)\n",
        "\n",
        "    for caption in captions:\n",
        "        border_size = 2\n",
        "        img = Image.new('RGB', (512-4,48-4), (255,255,255)) \n",
        "        img = ImageOps.expand(img, border=(border_size, border_size, border_size, border_size), fill=(127, 127, 127))\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        border_size = 2\n",
        "        text = caption\n",
        "        lines = textwrap.wrap(text, width=40)\n",
        "        x, y = 4, 4\n",
        "        line_height = font.getsize('A')[1] + 4 \n",
        "\n",
        "        start = 0\n",
        "        for line in lines:\n",
        "            draw.text((x, y+start), line, font=font, fill=(200, 127, 0))\n",
        "            y += line_height\n",
        "\n",
        "        caption_pil_list.append(img)\n",
        "    return caption_pil_list\n",
        "\n",
        "\n",
        "def filter_segmentation_mask(segmentation_mask: np.array):\n",
        "    \n",
        "    segmentation_mask[segmentation_mask==alphabet_dic['-']] = 0\n",
        "    segmentation_mask[segmentation_mask==alphabet_dic[' ']] = 0\n",
        "    return segmentation_mask\n",
        "    \n",
        "    \n",
        "\n",
        "def combine_image(args, sub_output_dir: str, pred_image_list: List, image_pil: Image, character_mask_pil: Image, character_mask_highlight_pil: Image, caption_pil_list: List):\n",
        "    \n",
        "    \n",
        "    # # create a \"latest\" folder to store the results \n",
        "    # if os.path.exists(f'{args.output_dir}/latest'):\n",
        "    #     shutil.rmtree(f'{args.output_dir}/latest')\n",
        "    # os.mkdir(f'{args.output_dir}/latest')\n",
        "    \n",
        "    # save each predicted image\n",
        "    # os.makedirs(f'{args.output_dir}/{sub_output_dir}', exist_ok=True)\n",
        "    for index, img in enumerate(pred_image_list):\n",
        "        img.save(f'{args.output_dir}/{sub_output_dir}/{index}.jpg')\n",
        "        # img.save(f'{args.output_dir}/latest/{index}.jpg')\n",
        "        \n",
        "    length = len(pred_image_list)\n",
        "    lines = math.ceil(length / 3)\n",
        "    \n",
        "    blank = Image.new('RGB', (512*3, 512*(lines+1)+48*lines), (0,0,0)) \n",
        "    blank.paste(image_pil,(0,0))\n",
        "    blank.paste(character_mask_pil,(512,0))\n",
        "    blank.paste(character_mask_highlight_pil,(512*2,0))\n",
        "    \n",
        "    for i in range(length):\n",
        "        row, col = i // 3, i % 3\n",
        "        blank.paste(pred_image_list[i],(512*col,512*(row+1)+48*row))\n",
        "        blank.paste(caption_pil_list[i],(512*col,512*(row+1)+48*row+512))\n",
        "    \n",
        "    blank.save(f'{args.output_dir}/{sub_output_dir}/combine.jpg')\n",
        "    # blank.save(f'{args.output_dir}/latest/combine.jpg')\n",
        "    \n",
        "    return blank.convert('RGB')\n",
        "    \n",
        "    \n",
        "def get_width(font_path, text):\n",
        "    \n",
        "    # font = ImageFont.truetype(font_path, 24)\n",
        "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "    font = ImageFont.truetype(font_path, size=24)\n",
        "    width, _ = font.getsize(text)\n",
        "    return width\n",
        "\n",
        "\n",
        "\n",
        "def get_key_words(text: str):\n",
        "   \n",
        "\n",
        "    words = []\n",
        "    text = text\n",
        "    matches = re.findall(r\"'(.*?)'\", text) # find the keywords enclosed by ''\n",
        "    if matches:\n",
        "        for match in matches:\n",
        "            words.extend(match.split())\n",
        "            \n",
        "    if len(words) >= 8:\n",
        "        return []\n",
        "   \n",
        "    return words\n",
        "\n",
        "\n",
        "def adjust_overlap_box(box_output, current_index):\n",
        "    \n",
        "    \n",
        "    if current_index == 0:\n",
        "        return box_output\n",
        "    else:\n",
        "        # judge whether it contains overlap with the last output\n",
        "        last_box = box_output[0, current_index-1, :]\n",
        "        xmin_last, ymin_last, xmax_last, ymax_last = last_box\n",
        "        \n",
        "        current_box = box_output[0, current_index, :]\n",
        "        xmin, ymin, xmax, ymax = current_box\n",
        "        \n",
        "        if xmin_last <= xmin <= xmax_last and ymin_last <= ymin <= ymax_last:\n",
        "            print('adjust overlapping')\n",
        "            distance_x = xmax_last - xmin\n",
        "            distance_y = ymax_last - ymin\n",
        "            if distance_x <= distance_y:\n",
        "                # avoid overlap\n",
        "                new_x_min = xmax_last + 0.025\n",
        "                new_x_max = xmax - xmin + xmax_last + 0.025\n",
        "                box_output[0,current_index,0] = new_x_min\n",
        "                box_output[0,current_index,2] = new_x_max\n",
        "            else:\n",
        "                new_y_min = ymax_last + 0.025\n",
        "                new_y_max = ymax - ymin + ymax_last + 0.025\n",
        "                box_output[0,current_index,1] = new_y_min\n",
        "                box_output[0,current_index,3] = new_y_max  \n",
        "                \n",
        "        elif xmin_last <= xmin <= xmax_last and ymin_last <= ymax <= ymax_last:\n",
        "            print('adjust overlapping')\n",
        "            new_x_min = xmax_last + 0.05\n",
        "            new_x_max = xmax - xmin + xmax_last + 0.05\n",
        "            box_output[0,current_index,0] = new_x_min\n",
        "            box_output[0,current_index,2] = new_x_max\n",
        "                    \n",
        "        return box_output\n",
        "    \n",
        "    \n",
        "def shrink_box(box, scale_factor = 0.9):\n",
        "    \n",
        "    \n",
        "    x1, y1, x2, y2 = box\n",
        "    x1_new = x1 + (x2 - x1) * (1 - scale_factor) / 2\n",
        "    y1_new = y1 + (y2 - y1) * (1 - scale_factor) / 2\n",
        "    x2_new = x2 - (x2 - x1) * (1 - scale_factor) / 2\n",
        "    y2_new = y2 - (y2 - y1) * (1 - scale_factor) / 2\n",
        "    return (x1_new, y1_new, x2_new, y2_new)\n",
        "\n",
        "\n",
        "def adjust_font_size(args, width, height, draw, text):\n",
        "    \n",
        "    \n",
        "    size_start = height\n",
        "    while True:\n",
        "        # font = ImageFont.truetype(args.font_path, size_start)\n",
        "        font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "        font = ImageFont.truetype(font_path, size=size_start)\n",
        "        text_width, _ = draw.textsize(text, font=font)\n",
        "        if text_width >= width:\n",
        "            size_start = size_start - 1\n",
        "        else:\n",
        "            return size_start\n",
        "    \n",
        "    \n",
        "def inpainting_merge_image(original_image, mask_image, inpainting_image):\n",
        "    \n",
        "    \n",
        "    original_image = original_image.resize((512, 512))\n",
        "    mask_image = mask_image.resize((512, 512))\n",
        "    inpainting_image = inpainting_image.resize((512, 512))\n",
        "    mask_image.convert('L')\n",
        "    threshold = 250 \n",
        "    table = []\n",
        "    for i in range(256):\n",
        "        if i < threshold:\n",
        "            table.append(1)\n",
        "        else:\n",
        "            table.append(0)\n",
        "    mask_image = mask_image.point(table, \"1\")\n",
        "    merged_image = Image.composite(inpainting_image, original_image, mask_image)\n",
        "    return merged_image\n",
        "\"\"\"\n",
        "\n",
        "# model/layout_generator.py\n",
        "\"\"\"\n",
        "# ------------------------------------------\n",
        "# TextDiffuser: Diffusion Models as Text Painters\n",
        "# Paper Link: https://arxiv.org/abs/2305.10855\n",
        "# Code Link: https://github.com/microsoft/unilm/tree/master/textdiffuser\n",
        "# Copyright (c) Microsoft Corporation.\n",
        "# This file aims to predict the layout of keywords in user prompts.\n",
        "# ------------------------------------------\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPTokenizer\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from util import get_width, get_key_words, adjust_overlap_box, shrink_box, adjust_font_size, alphabet_dic\n",
        "from model.layout_transformer import LayoutTransformer, TextConditioner\n",
        "from termcolor import colored\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# import layout transformer\n",
        "model = LayoutTransformer().cuda().eval()\n",
        "model.load_state_dict(torch.load('textdiffuser-ckpt/layout_transformer.pth'))\n",
        "\n",
        "# import text encoder and tokenizer\n",
        "text_encoder = TextConditioner().cuda().eval()\n",
        "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "\n",
        "\n",
        "def process_caption(font_path, caption, keywords):\n",
        "    # remove punctuations. please remove this statement if you want to paint punctuations\n",
        "    caption = re.sub(u\"([^\\u0041-\\u005a\\u0061-\\u007a\\u0030-\\u0039])\", \" \", caption) \n",
        "    \n",
        "    # tokenize it into ids and get length\n",
        "    caption_words = tokenizer([caption], truncation=True, max_length=77, return_length=True, return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    caption_words_ids = caption_words['input_ids'] # (1, 77)\n",
        "    length = caption_words['length'] # (1, )\n",
        "    \n",
        "    # convert id to words\n",
        "    words = tokenizer.convert_ids_to_tokens(caption_words_ids.view(-1).tolist())\n",
        "    words = [i.replace('</w>', '') for i in words]\n",
        "    words_valid = words[:int(length)]\n",
        "\n",
        "    # store the box coordinates and state of each token\n",
        "    info_array = np.zeros((77,5)) # (77, 5)\n",
        "\n",
        "    # split the caption into words and convert them into lower case\n",
        "    caption_split = caption.split() \n",
        "    caption_split = [i.lower() for i in caption_split]\n",
        "\n",
        "    start_dic = {} # get the start index of each word\n",
        "    state_list = [] # 0: start, 1: middle, 2: special token\n",
        "    word_match_list = [] # the index of the word in the caption\n",
        "    current_caption_index = 0 \n",
        "    current_match = ''\n",
        "    for i in range(length): \n",
        "\n",
        "        # the first and last token are special tokens\n",
        "        if i == 0 or i == length-1:\n",
        "            state_list.append(2) \n",
        "            word_match_list.append(127)\n",
        "            continue\n",
        "\n",
        "        if current_match == '':\n",
        "            state_list.append(0)\n",
        "            start_dic[current_caption_index] = i\n",
        "        else:\n",
        "            state_list.append(1)\n",
        "\n",
        "        current_match += words_valid[i]\n",
        "        word_match_list.append(current_caption_index)\n",
        "        if current_match == caption_split[current_caption_index]:\n",
        "            current_match = ''\n",
        "            current_caption_index += 1\n",
        "\n",
        "    while len(state_list) < 77:\n",
        "        state_list.append(127)\n",
        "    while len(word_match_list) < 77:\n",
        "        word_match_list.append(127)\n",
        "\n",
        "    length_list = []\n",
        "    width_list =[]\n",
        "    for i in range(len(word_match_list)):\n",
        "        if word_match_list[i] == 127:\n",
        "            length_list.append(0)\n",
        "            width_list.append(0)\n",
        "        else:\n",
        "            length_list.append(len(caption.split()[word_match_list[i]]))\n",
        "            width_list.append(get_width(font_path, caption.split()[word_match_list[i]]))\n",
        "\n",
        "    while len(length_list) < 77:\n",
        "        length_list.append(127)\n",
        "        width_list.append(0)\n",
        "\n",
        "    length_list = torch.Tensor(length_list).long() # (77, )\n",
        "    width_list = torch.Tensor(width_list).long() # (77, )\n",
        "\n",
        "    boxes = []\n",
        "    duplicate_dict = {} # some words may appear more than once\n",
        "    for keyword in keywords: \n",
        "        keyword = keyword.lower()\n",
        "        if keyword in caption_split:\n",
        "            if keyword not in duplicate_dict:\n",
        "                duplicate_dict[keyword] = caption_split.index(keyword) \n",
        "                index = caption_split.index(keyword)\n",
        "            else:\n",
        "                if duplicate_dict[keyword]+1 < len(caption_split) and keyword in caption_split[duplicate_dict[keyword]+1:]:\n",
        "                    index = duplicate_dict[keyword] + caption_split[duplicate_dict[keyword]+1:].index(keyword)\n",
        "                    duplicate_dict[keyword] = index\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "            index = caption_split.index(keyword) \n",
        "            index = start_dic[index] \n",
        "            info_array[index][0] = 1 \n",
        "\n",
        "            box = [0,0,0,0] \n",
        "            boxes.append(list(box))\n",
        "            info_array[index][1:] = box\n",
        "    \n",
        "    boxes_length = len(boxes)\n",
        "    if boxes_length > 8:\n",
        "        boxes = boxes[:8]\n",
        "    while len(boxes) < 8:\n",
        "        boxes.append([0,0,0,0])\n",
        "\n",
        "    return caption, length_list, width_list, torch.from_numpy(info_array), words, torch.Tensor(state_list).long(), torch.Tensor(word_match_list).long(), torch.Tensor(boxes), boxes_length\n",
        "\n",
        "\n",
        "def get_layout_from_prompt(args):\n",
        "\n",
        "    # prompt = args.prompt\n",
        "    font_path = args.font_path\n",
        "    keywords = get_key_words(args.prompt)\n",
        "    \n",
        "    print(f'{colored(\"[!]\", \"red\")} Detected keywords: {keywords} from prompt {args.prompt}')\n",
        "    \n",
        "    text_embedding, mask = text_encoder(args.prompt) # (1, 77 768) / (1, 77)\n",
        "\n",
        "    # process all relevant info\n",
        "    caption, length_list, width_list, target, words, state_list, word_match_list, boxes, boxes_length = process_caption(font_path, args.prompt, keywords)\n",
        "    target = target.cuda().unsqueeze(0) # (77, 5)\n",
        "    width_list = width_list.cuda().unsqueeze(0) # (77, )\n",
        "    length_list = length_list.cuda().unsqueeze(0) # (77, )\n",
        "    state_list = state_list.cuda().unsqueeze(0) # (77, )\n",
        "    word_match_list = word_match_list.cuda().unsqueeze(0) # (77, )\n",
        "\n",
        "    padding = torch.zeros(1, 1, 4).cuda()\n",
        "    boxes = boxes.unsqueeze(0).cuda()\n",
        "    right_shifted_boxes = torch.cat([padding, boxes[:,0:-1,:]],1) # (1, 8, 4)\n",
        "   \n",
        "    # inference\n",
        "    return_boxes= []\n",
        "    with torch.no_grad():\n",
        "        for box_index in range(boxes_length):\n",
        "            \n",
        "            if box_index == 0:\n",
        "                encoder_embedding = None\n",
        "                \n",
        "            output, encoder_embedding = model(text_embedding, length_list, width_list, mask, state_list, word_match_list, target, right_shifted_boxes, train=False, encoder_embedding=encoder_embedding) \n",
        "            output = torch.clamp(output, min=0, max=1) # (1, 8, 4)\n",
        "            \n",
        "            # add overlap detection\n",
        "            output = adjust_overlap_box(output, box_index) # (1, 8, 4)\n",
        "            \n",
        "            right_shifted_boxes[:,box_index+1,:] = output[:,box_index,:]\n",
        "            xmin, ymin, xmax, ymax = output[0, box_index, :].tolist()\n",
        "            return_boxes.append([xmin, ymin, xmax, ymax])\n",
        "            \n",
        "            \n",
        "    # print the location of keywords\n",
        "    print(f'index\\tkeyword\\tx_min\\ty_min\\tx_max\\ty_max')\n",
        "    for index, keyword in enumerate(keywords):\n",
        "        x_min = int(return_boxes[index][0] * 512)\n",
        "        y_min = int(return_boxes[index][1] * 512)\n",
        "        x_max = int(return_boxes[index][2] * 512)\n",
        "        y_max = int(return_boxes[index][3] * 512)\n",
        "        print(f'{index}\\t{keyword}\\t{x_min}\\t{y_min}\\t{x_max}\\t{y_max}')\n",
        "    \n",
        "    \n",
        "    # paint the layout\n",
        "    render_image = Image.new('RGB', (512, 512), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(render_image)\n",
        "    segmentation_mask = Image.new(\"L\", (512,512), 0)\n",
        "    segmentation_mask_draw = ImageDraw.Draw(segmentation_mask)\n",
        "\n",
        "    for index, box in enumerate(return_boxes):\n",
        "        box = [int(i*512) for i in box]\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        \n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        text = keywords[index]\n",
        "\n",
        "        font_size = adjust_font_size(args, width, height, draw, text)\n",
        "        # font = ImageFont.truetype(args.font_path, font_size)\n",
        "        font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
        "        font = ImageFont.truetype(font_path, size=font_size)\n",
        "\n",
        "        # draw.rectangle([xmin, ymin, xmax,ymax], outline=(255,0,0))\n",
        "        draw.text((xmin, ymin), text, font=font, fill=(0, 0, 0))\n",
        "            \n",
        "        boxes = []\n",
        "        for i, char in enumerate(text):\n",
        "            \n",
        "            # paint character-level segmentation masks\n",
        "            # https://github.com/python-pillow/Pillow/issues/3921\n",
        "            bottom_1 = font.getsize(text[i])[1]\n",
        "            right, bottom_2 = font.getsize(text[:i+1])\n",
        "            bottom = bottom_1 if bottom_1 < bottom_2 else bottom_2\n",
        "            width, height = font.getmask(char).size\n",
        "            right += xmin\n",
        "            bottom += ymin\n",
        "            top = bottom - height\n",
        "            left = right - width\n",
        "            \n",
        "            char_box = (left, top, right, bottom)\n",
        "            boxes.append(char_box)\n",
        "            \n",
        "            char_index = alphabet_dic[char]\n",
        "            segmentation_mask_draw.rectangle(shrink_box(char_box, scale_factor = 0.9), fill=char_index)\n",
        "    \n",
        "    print(f'{colored(\"[âˆš]\", \"green\")} Layout is successfully generated')\n",
        "    return render_image, segmentation_mask\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gEVlG8PeJEPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/unilm/textdiffuser\n",
        "# Text-to-Image\n",
        "!CUDA_VISIBLE_DEVICES=0 python inference.py \\\n",
        "  --mode=\"text-to-image\" \\\n",
        "  --resume_from_checkpoint=\"textdiffuser-ckpt/diffusion_backbone\" \\\n",
        "  --prompt=\"A sign that says 'Hello'\" \\\n",
        "  --output_dir=\"./output\" \\\n",
        "  --vis_num=4"
      ],
      "metadata": {
        "id": "7Uj3ysalbrq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "display(Image.open(\"/content/unilm/textdiffuser/output/latest/0.jpg\"))"
      ],
      "metadata": {
        "id": "cbx-b-kUpZLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}