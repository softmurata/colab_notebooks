{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRVr1AfTcNp//O1l2kmmba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/DIFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "b1ehEdYLGlqX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6WwL2O4GahG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade diffusers[torch]\n",
        "!pip install transformers\n",
        "!pip install -U xformers\n",
        "!pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib widget\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import PILToTensor\n",
        "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
        "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
        "from diffusers import DDIMScheduler\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "NqStnmFhGrh4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define sd net"
      ],
      "metadata": {
        "id": "EKL1cCYwG81H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
        "    def forward(\n",
        "        self,\n",
        "        sample: torch.FloatTensor,\n",
        "        timestep: Union[torch.Tensor, float, int],\n",
        "        up_ft_indices,\n",
        "        encoder_hidden_states: torch.Tensor,\n",
        "        class_labels: Optional[torch.Tensor] = None,\n",
        "        timestep_cond: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n",
        "            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n",
        "            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n",
        "            cross_attention_kwargs (`dict`, *optional*):\n",
        "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
        "                `self.processor` in\n",
        "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
        "        \"\"\"\n",
        "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
        "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
        "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
        "        # on the fly if necessary.\n",
        "        default_overall_up_factor = 2**self.num_upsamplers\n",
        "\n",
        "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
        "        forward_upsample_size = False\n",
        "        upsample_size = None\n",
        "\n",
        "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
        "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
        "            forward_upsample_size = True\n",
        "\n",
        "        # prepare attention_mask\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
        "            attention_mask = attention_mask.unsqueeze(1)\n",
        "\n",
        "        # 0. center input if necessary\n",
        "        if self.config.center_input_sample:\n",
        "            sample = 2 * sample - 1.0\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            # This would be a good case for the `match` statement (Python 3.10+)\n",
        "            is_mps = sample.device.type == \"mps\"\n",
        "            if isinstance(timestep, float):\n",
        "                dtype = torch.float32 if is_mps else torch.float64\n",
        "            else:\n",
        "                dtype = torch.int32 if is_mps else torch.int64\n",
        "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
        "        elif len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        t_emb = self.time_proj(timesteps)\n",
        "\n",
        "        # timesteps does not contain any weights and will always return f32 tensors\n",
        "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
        "        # there might be better ways to encapsulate this.\n",
        "        t_emb = t_emb.to(dtype=self.dtype)\n",
        "\n",
        "        emb = self.time_embedding(t_emb, timestep_cond)\n",
        "\n",
        "        if self.class_embedding is not None:\n",
        "            if class_labels is None:\n",
        "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
        "\n",
        "            if self.config.class_embed_type == \"timestep\":\n",
        "                class_labels = self.time_proj(class_labels)\n",
        "\n",
        "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
        "            emb = emb + class_emb\n",
        "\n",
        "        # 2. pre-process\n",
        "        sample = self.conv_in(sample)\n",
        "\n",
        "        # 3. down\n",
        "        down_block_res_samples = (sample,)\n",
        "        for downsample_block in self.down_blocks:\n",
        "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
        "                sample, res_samples = downsample_block(\n",
        "                    hidden_states=sample,\n",
        "                    temb=emb,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    cross_attention_kwargs=cross_attention_kwargs,\n",
        "                )\n",
        "            else:\n",
        "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
        "\n",
        "            down_block_res_samples += res_samples\n",
        "\n",
        "        # 4. mid\n",
        "        if self.mid_block is not None:\n",
        "            sample = self.mid_block(\n",
        "                sample,\n",
        "                emb,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                cross_attention_kwargs=cross_attention_kwargs,\n",
        "            )\n",
        "\n",
        "        # 5. up\n",
        "        up_ft = {}\n",
        "        for i, upsample_block in enumerate(self.up_blocks):\n",
        "\n",
        "            if i > np.max(up_ft_indices):\n",
        "                break\n",
        "\n",
        "            is_final_block = i == len(self.up_blocks) - 1\n",
        "\n",
        "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
        "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
        "\n",
        "            # if we have not reached the final block and need to forward the\n",
        "            # upsample size, we do it here\n",
        "            if not is_final_block and forward_upsample_size:\n",
        "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
        "\n",
        "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
        "                sample = upsample_block(\n",
        "                    hidden_states=sample,\n",
        "                    temb=emb,\n",
        "                    res_hidden_states_tuple=res_samples,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    cross_attention_kwargs=cross_attention_kwargs,\n",
        "                    upsample_size=upsample_size,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                sample = upsample_block(\n",
        "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
        "                )\n",
        "\n",
        "            if i in up_ft_indices:\n",
        "                up_ft[i] = sample.detach()\n",
        "\n",
        "        output = {}\n",
        "        output['up_ft'] = up_ft\n",
        "        return output\n",
        "\n",
        "class OneStepSDPipeline(StableDiffusionPipeline):\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        img_tensor,\n",
        "        t,\n",
        "        up_ft_indices,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: int = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None\n",
        "    ):\n",
        "\n",
        "        device = self._execution_device\n",
        "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
        "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
        "        noise = torch.randn_like(latents).to(device)\n",
        "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
        "        unet_output = self.unet(latents_noisy,\n",
        "                               t,\n",
        "                               up_ft_indices,\n",
        "                               encoder_hidden_states=prompt_embeds,\n",
        "                               cross_attention_kwargs=cross_attention_kwargs)\n",
        "        return unet_output\n",
        "\n",
        "\n",
        "class SDFeaturizer:\n",
        "    def __init__(self, sd_id='stabilityai/stable-diffusion-2-1'):\n",
        "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\")\n",
        "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None)\n",
        "        onestep_pipe.vae.decoder = None\n",
        "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\")\n",
        "        gc.collect()\n",
        "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
        "        onestep_pipe.enable_attention_slicing()\n",
        "        onestep_pipe.enable_xformers_memory_efficient_attention()\n",
        "        self.pipe = onestep_pipe\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,\n",
        "                img_tensor, # single image, [1,c,h,w]\n",
        "                prompt,\n",
        "                t=261,\n",
        "                up_ft_index=1,\n",
        "                ensemble_size=8):\n",
        "        img_tensor = img_tensor.repeat(ensemble_size, 1, 1, 1).cuda() # ensem, c, h, w\n",
        "        prompt_embeds = self.pipe._encode_prompt(\n",
        "            prompt=prompt,\n",
        "            device='cuda',\n",
        "            num_images_per_prompt=1,\n",
        "            do_classifier_free_guidance=False) # [1, 77, dim]\n",
        "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1)\n",
        "        unet_ft_all = self.pipe(\n",
        "            img_tensor=img_tensor,\n",
        "            t=t,\n",
        "            up_ft_indices=[up_ft_index],\n",
        "            prompt_embeds=prompt_embeds)\n",
        "        unet_ft = unet_ft_all['up_ft'][up_ft_index] # ensem, c, h, w\n",
        "        unet_ft = unet_ft.mean(0, keepdim=True) # 1,c,h,w\n",
        "        return unet_ft"
      ],
      "metadata": {
        "id": "RgneK8qmGvZ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define visualization class"
      ],
      "metadata": {
        "id": "tjL0m8ReGybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Demo:\n",
        "    def __init__(self, imgs, ft, img_size):\n",
        "        self.ft = ft # NCHW\n",
        "        self.imgs = imgs\n",
        "        self.num_imgs = len(imgs)\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def plot_img_pairs(self, fig_size=3, alpha=0.45, scatter_size=70):\n",
        "\n",
        "        fig, axes = plt.subplots(1, self.num_imgs, figsize=(fig_size*self.num_imgs, fig_size))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        for i in range(self.num_imgs):\n",
        "            axes[i].imshow(self.imgs[i])\n",
        "            axes[i].axis('off')\n",
        "            if i == 0:\n",
        "                axes[i].set_title('source image')\n",
        "            else:\n",
        "                axes[i].set_title('target image')\n",
        "\n",
        "        num_channel = self.ft.size(1)\n",
        "        cos = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "        def onclick(event):\n",
        "            if event.inaxes == axes[0]:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
        "\n",
        "                    src_ft = self.ft[0].unsqueeze(0)\n",
        "                    src_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(src_ft)\n",
        "                    src_vec = src_ft[0, :, y, x].view(1, num_channel, 1, 1)  # 1, C, 1, 1\n",
        "                    del src_ft\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    trg_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(self.ft[1:])\n",
        "                    cos_map = cos(src_vec, trg_ft).cpu().numpy()  # N, H, W\n",
        "                    \n",
        "                    del trg_ft\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    axes[0].clear()\n",
        "                    axes[0].imshow(self.imgs[0])\n",
        "                    axes[0].axis('off')\n",
        "                    axes[0].scatter(x, y, c='r', s=scatter_size)\n",
        "                    axes[0].set_title('source image')\n",
        "\n",
        "                    for i in range(1, self.num_imgs):\n",
        "                        max_yx = np.unravel_index(cos_map[i-1].argmax(), cos_map[i-1].shape)\n",
        "                        axes[i].clear()\n",
        "\n",
        "                        heatmap = cos_map[i-1]\n",
        "                        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))  # Normalize to [0, 1]\n",
        "                        axes[i].imshow(self.imgs[i])\n",
        "                        axes[i].imshow(255 * heatmap, alpha=alpha, cmap='viridis')\n",
        "                        axes[i].axis('off')\n",
        "                        axes[i].scatter(max_yx[1].item(), max_yx[0].item(), c='r', s=scatter_size)\n",
        "                        axes[i].set_title('target image')\n",
        "\n",
        "                    del cos_map\n",
        "                    del heatmap\n",
        "                    gc.collect()\n",
        "\n",
        "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "TUEI6HceG0Vf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare demo data"
      ],
      "metadata": {
        "id": "bwh8ulweHAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can choose visualize cat or guitar\n",
        "category = random.choice(['cat', 'guitar'])\n",
        "print(f\"let's visualize semantic correspondence on {category}\")\n",
        "if category == 'cat':\n",
        "    !wget https://www.dropbox.com/s/hciam2gtspa9avl/example_shibao.png?dl=1 -O source.png\n",
        "    !wget https://www.dropbox.com/s/3aah93o4whxbef8/example_shimei.png?dl=1 -O target.png\n",
        "\n",
        "else:\n",
        "    !wget https://www.dropbox.com/s/qiz2osc5eqsveak/example_guitar.png?dl=1 -O source.png\n",
        "    !wget https://www.dropbox.com/s/fb9kkuaofmpt97i/example_guitar_sketch.png?dl=1 -O target.png\n",
        "\n",
        "prompt = f'a photo of a {category}'"
      ],
      "metadata": {
        "id": "dhJ4iiDOG_1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "evfnnOvhI1Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dift = SDFeaturizer(sd_id='stabilityai/stable-diffusion-2-1')"
      ],
      "metadata": {
        "id": "jbYvSiLzHJAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "colab only has limited RAM and GPU memory, so we delete model here after\n",
        "getting feature maps. if you wanna test on another image pair, please re-run \n",
        "this cell.\n",
        "also due to colab constraint, here we reduce image size from 768 to 512, and\n",
        "ensemble_size from 8 to 2.\n",
        "'''\n",
        "\n",
        "\n",
        "filelist = ['source.png', 'target.png']\n",
        "\n",
        "ft = []\n",
        "imglist = []\n",
        "\n",
        "# img_size = 512\n",
        "img_size = 448\n",
        "\n",
        "for filename in filelist:\n",
        "    img = Image.open(filename).convert('RGB')\n",
        "    img = img.resize((img_size, img_size))\n",
        "    imglist.append(img)\n",
        "    img_tensor = (PILToTensor()(img) / 255.0 - 0.5) * 2\n",
        "    ft.append(dift.forward(img_tensor,\n",
        "                           prompt=prompt,\n",
        "                           ensemble_size=2))\n",
        "ft = torch.cat(ft, dim=0)\n",
        "\n",
        "del dift\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFgUGs7cHOIm",
        "outputId": "22935525-32db-4453-c3e1-929ab785783f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = Demo(imglist, ft, img_size)"
      ],
      "metadata": {
        "id": "eD0wHvbXHRzZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "left is source image, right is target image.\n",
        "you can click on the source image, and DIFT will find the corresponding\n",
        "point on the right image, mark it with red point and also plot the per-pixel \n",
        "cosine distance as heatmap.\n",
        "'''\n",
        "demo.plot_img_pairs(fig_size=5)"
      ],
      "metadata": {
        "id": "NjB-dPtsHUY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}