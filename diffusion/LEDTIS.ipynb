{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0KKVvyFFC4knulSMcLn4X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusion/LEDTIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "a8OCwY-sniYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUr0pVl_k54X"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers diffusers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "9V1nYvLmnjnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "4K0R-ugflC7A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils function"
      ],
      "metadata": {
        "id": "-LT-0TiolHp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw ,ImageFont\n",
        "# from matplotlib import pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "# import os\n",
        "# import yaml\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "def load_512(image_path, left=0, right=0, top=0, bottom=0, device=None):\n",
        "    if type(image_path) is str:\n",
        "        image = np.array(Image.open(image_path).convert('RGB'))[:, :, :3]\n",
        "    else:\n",
        "        image = image_path\n",
        "    h, w, c = image.shape\n",
        "    left = min(left, w-1)\n",
        "    right = min(right, w - left - 1)\n",
        "    top = min(top, h - left - 1)\n",
        "    bottom = min(bottom, h - top - 1)\n",
        "    image = image[top:h-bottom, left:w-right]\n",
        "    h, w, c = image.shape\n",
        "    if h < w:\n",
        "        offset = (w - h) // 2\n",
        "        image = image[:, offset:offset + h]\n",
        "    elif w < h:\n",
        "        offset = (h - w) // 2\n",
        "        image = image[offset:offset + w]\n",
        "    image = np.array(Image.fromarray(image).resize((512, 512)))\n",
        "    image = torch.from_numpy(image).float() / 127.5 - 1\n",
        "    image = image.permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def load_real_image(folder = \"data/\", img_name = None, idx = 0, img_size=512, device='cuda'):\n",
        "    from PIL import Image\n",
        "    from glob import glob\n",
        "    if img_name is not None:\n",
        "        path = os.path.join(folder, img_name)\n",
        "    else:\n",
        "        path = glob(folder + \"*\")[idx]\n",
        "\n",
        "    img = Image.open(path).resize((img_size,\n",
        "                                    img_size))\n",
        "\n",
        "    img = pil_to_tensor(img).to(device)\n",
        "\n",
        "    if img.shape[1]== 4:\n",
        "        img = img[:,:3,:,:]\n",
        "    return img\n",
        "\n",
        "def mu_tilde(model, xt,x0, timestep):\n",
        "    \"mu_tilde(x_t, x_0) DDPM paper eq. 7\"\n",
        "    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n",
        "    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
        "    alpha_t = model.scheduler.alphas[timestep]\n",
        "    beta_t = 1 - alpha_t\n",
        "    alpha_bar = model.scheduler.alphas_cumprod[timestep]\n",
        "    return ((alpha_prod_t_prev ** 0.5 * beta_t) / (1-alpha_bar)) * x0 +  ((alpha_t**0.5 *(1-alpha_prod_t_prev)) / (1- alpha_bar))*xt\n",
        "\n",
        "def sample_xts_from_x0(model, x0, num_inference_steps=50):\n",
        "    \"\"\"\n",
        "    Samples from P(x_1:T|x_0)\n",
        "    \"\"\"\n",
        "    # torch.manual_seed(43256465436)\n",
        "    alpha_bar = model.scheduler.alphas_cumprod\n",
        "    sqrt_one_minus_alpha_bar = (1-alpha_bar) ** 0.5\n",
        "    alphas = model.scheduler.alphas\n",
        "    betas = 1 - alphas\n",
        "    variance_noise_shape = (\n",
        "            num_inference_steps,\n",
        "            model.unet.in_channels,\n",
        "            model.unet.sample_size,\n",
        "            model.unet.sample_size)\n",
        "\n",
        "    timesteps = model.scheduler.timesteps.to(model.device)\n",
        "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
        "    xts = torch.zeros(variance_noise_shape).to(x0.device)\n",
        "    for t in reversed(timesteps):\n",
        "        idx = t_to_idx[int(t)]\n",
        "        xts[idx] = x0 * (alpha_bar[t] ** 0.5) + torch.randn_like(x0) * sqrt_one_minus_alpha_bar[t]\n",
        "    xts = torch.cat([xts, x0 ],dim = 0)\n",
        "\n",
        "    return xts\n",
        "\n",
        "def encode_text(model, prompts):\n",
        "    text_input = model.tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=model.tokenizer.model_max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        text_encoding = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
        "    return text_encoding\n",
        "\n",
        "def forward_step(model, model_output, timestep, sample):\n",
        "    next_timestep = min(model.scheduler.config.num_train_timesteps - 2,\n",
        "                        timestep + model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps)\n",
        "\n",
        "    # 2. compute alphas, betas\n",
        "    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n",
        "    # alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep] if next_ltimestep >= 0 else self.scheduler.final_alpha_cumprod\n",
        "\n",
        "    beta_prod_t = 1 - alpha_prod_t\n",
        "\n",
        "    # 3. compute predicted original sample from predicted noise also called\n",
        "    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
        "\n",
        "    # 5. TODO: simple noising implementatiom\n",
        "    next_sample = model.scheduler.add_noise(pred_original_sample,\n",
        "                                    model_output,\n",
        "                                    torch.LongTensor([next_timestep]))\n",
        "    return next_sample\n",
        "\n",
        "\n",
        "def get_variance(model, timestep): #, prev_timestep):\n",
        "    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n",
        "    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n",
        "    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
        "    beta_prod_t = 1 - alpha_prod_t\n",
        "    beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
        "    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
        "    return variance\n",
        "\n",
        "def inversion_forward_process(model, x0,\n",
        "                            etas = None,\n",
        "                            prog_bar = False,\n",
        "                            prompt = \"\",\n",
        "                            cfg_scale = 3.5,\n",
        "                            num_inference_steps=50, eps = None):\n",
        "\n",
        "    if not prompt==\"\":\n",
        "        text_embeddings = encode_text(model, prompt)\n",
        "    uncond_embedding = encode_text(model, \"\")\n",
        "    timesteps = model.scheduler.timesteps.to(model.device)\n",
        "    variance_noise_shape = (\n",
        "        num_inference_steps,\n",
        "        model.unet.in_channels,\n",
        "        model.unet.sample_size,\n",
        "        model.unet.sample_size)\n",
        "    if etas is None or (type(etas) in [int, float] and etas == 0):\n",
        "        eta_is_zero = True\n",
        "        zs = None\n",
        "    else:\n",
        "        eta_is_zero = False\n",
        "        if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n",
        "        xts = sample_xts_from_x0(model, x0, num_inference_steps=num_inference_steps)\n",
        "        alpha_bar = model.scheduler.alphas_cumprod\n",
        "        zs = torch.zeros(size=variance_noise_shape, device=model.device)\n",
        "\n",
        "    t_to_idx = {int(v):k for k,v in enumerate(timesteps)}\n",
        "    xt = x0\n",
        "    op = tqdm(reversed(timesteps)) if prog_bar else reversed(timesteps)\n",
        "\n",
        "    for t in op:\n",
        "        idx = t_to_idx[int(t)]\n",
        "        # 1. predict noise residual\n",
        "        if not eta_is_zero:\n",
        "            xt = xts[idx][None]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model.unet.forward(xt, timestep =  t, encoder_hidden_states = uncond_embedding)\n",
        "            if not prompt==\"\":\n",
        "                cond_out = model.unet.forward(xt, timestep=t, encoder_hidden_states = text_embeddings)\n",
        "\n",
        "        if not prompt==\"\":\n",
        "            ## classifier free guidance\n",
        "            noise_pred = out.sample + cfg_scale * (cond_out.sample - out.sample)\n",
        "        else:\n",
        "            noise_pred = out.sample\n",
        "\n",
        "        if eta_is_zero:\n",
        "            # 2. compute more noisy image and set x_t -> x_t+1\n",
        "            xt = forward_step(model, noise_pred, t, xt)\n",
        "\n",
        "        else:\n",
        "            xtm1 =  xts[idx+1][None]\n",
        "            # pred of x0\n",
        "            pred_original_sample = (xt - (1-alpha_bar[t])  ** 0.5 * noise_pred ) / alpha_bar[t] ** 0.5\n",
        "\n",
        "            # direction to xt\n",
        "            prev_timestep = t - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n",
        "            alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
        "\n",
        "            variance = get_variance(model, t)\n",
        "            pred_sample_direction = (1 - alpha_prod_t_prev - etas[idx] * variance ) ** (0.5) * noise_pred\n",
        "\n",
        "            mu_xt = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
        "\n",
        "            z = (xtm1 - mu_xt ) / ( etas[idx] * variance ** 0.5 )\n",
        "            zs[idx] = z\n",
        "\n",
        "            # correction to avoid error accumulation\n",
        "            xtm1 = mu_xt + ( etas[idx] * variance ** 0.5 )*z\n",
        "            xts[idx+1] = xtm1\n",
        "\n",
        "    if not zs is None:\n",
        "        zs[-1] = torch.zeros_like(zs[-1])\n",
        "\n",
        "    return xt, zs, xts\n",
        "\n",
        "\n",
        "def reverse_step(model, model_output, timestep, sample, eta = 0, variance_noise=None):\n",
        "    # 1. get previous step value (=t-1)\n",
        "    prev_timestep = timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps\n",
        "    # 2. compute alphas, betas\n",
        "    alpha_prod_t = model.scheduler.alphas_cumprod[timestep]\n",
        "    alpha_prod_t_prev = model.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
        "    beta_prod_t = 1 - alpha_prod_t\n",
        "    # 3. compute predicted original sample from predicted noise also called\n",
        "    # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "    pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
        "    # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
        "    # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
        "    # variance = self.scheduler._get_variance(timestep, prev_timestep)\n",
        "    variance = get_variance(model, timestep) #, prev_timestep)\n",
        "    std_dev_t = eta * variance ** (0.5)\n",
        "    # Take care of asymetric reverse process (asyrp)\n",
        "    model_output_direction = model_output\n",
        "    # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "    # pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output_direction\n",
        "    pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * model_output_direction\n",
        "    # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "    prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
        "    # 8. Add noice if eta > 0\n",
        "    if eta > 0:\n",
        "        if variance_noise is None:\n",
        "            variance_noise = torch.randn(model_output.shape, device=model.device)\n",
        "        sigma_z =  eta * variance ** (0.5) * variance_noise\n",
        "        prev_sample = prev_sample + sigma_z\n",
        "\n",
        "    return prev_sample\n",
        "\n",
        "def inversion_reverse_process(model,\n",
        "                    xT,\n",
        "                    etas = 0,\n",
        "                    prompts = \"\",\n",
        "                    cfg_scales = None,\n",
        "                    prog_bar = False,\n",
        "                    zs = None,\n",
        "                    controller=None,\n",
        "                    asyrp = False):\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "\n",
        "    cfg_scales_tensor = torch.Tensor(cfg_scales).view(-1,1,1,1).to(model.device)\n",
        "\n",
        "    text_embeddings = encode_text(model, prompts)\n",
        "    uncond_embedding = encode_text(model, [\"\"] * batch_size)\n",
        "\n",
        "    if etas is None: etas = 0\n",
        "    if type(etas) in [int, float]: etas = [etas]*model.scheduler.num_inference_steps\n",
        "    assert len(etas) == model.scheduler.num_inference_steps\n",
        "    timesteps = model.scheduler.timesteps.to(model.device)\n",
        "\n",
        "    xt = xT.expand(batch_size, -1, -1, -1)\n",
        "    op = tqdm(timesteps[-zs.shape[0]:]) if prog_bar else timesteps[-zs.shape[0]:]\n",
        "\n",
        "    t_to_idx = {int(v):k for k,v in enumerate(timesteps[-zs.shape[0]:])}\n",
        "\n",
        "    for t in op:\n",
        "        idx = t_to_idx[int(t)]\n",
        "        ## Unconditional embedding\n",
        "        with torch.no_grad():\n",
        "            uncond_out = model.unet.forward(xt, timestep =  t,\n",
        "                                            encoder_hidden_states = uncond_embedding)\n",
        "\n",
        "            ## Conditional embedding\n",
        "        if prompts:\n",
        "            with torch.no_grad():\n",
        "                cond_out = model.unet.forward(xt, timestep =  t,\n",
        "                                                encoder_hidden_states = text_embeddings)\n",
        "\n",
        "\n",
        "        z = zs[idx] if not zs is None else None\n",
        "        z = z.expand(batch_size, -1, -1, -1)\n",
        "        if prompts:\n",
        "            ## classifier free guidance\n",
        "            noise_pred = uncond_out.sample + cfg_scales_tensor * (cond_out.sample - uncond_out.sample)\n",
        "        else:\n",
        "            noise_pred = uncond_out.sample\n",
        "        # 2. compute less noisy image and set x_t -> x_t-1\n",
        "        xt = reverse_step(model, noise_pred, t, xt, eta = etas[idx], variance_noise = z)\n",
        "        if controller is not None:\n",
        "            xt = controller.step_callback(xt)\n",
        "    return xt, zs\n"
      ],
      "metadata": {
        "id": "RFCjHyIAlIc6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageDraw ,ImageFont\n",
        "from matplotlib import pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "def show_torch_img(img):\n",
        "    img = to_np_image(img)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def to_np_image(all_images):\n",
        "    all_images = (all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()[0]\n",
        "    return all_images\n",
        "\n",
        "def tensor_to_pil(tensor_imgs):\n",
        "    if type(tensor_imgs) == list:\n",
        "        tensor_imgs = torch.cat(tensor_imgs)\n",
        "    tensor_imgs = (tensor_imgs / 2 + 0.5).clamp(0, 1)\n",
        "    to_pil = T.ToPILImage()\n",
        "    pil_imgs = [to_pil(img) for img in tensor_imgs]\n",
        "    return pil_imgs\n",
        "\n",
        "def pil_to_tensor(pil_imgs):\n",
        "    to_torch = T.ToTensor()\n",
        "    if type(pil_imgs) == PIL.Image.Image:\n",
        "        tensor_imgs = to_torch(pil_imgs).unsqueeze(0)*2-1\n",
        "    elif type(pil_imgs) == list:\n",
        "        tensor_imgs = torch.cat([to_torch(pil_imgs).unsqueeze(0)*2-1 for img in pil_imgs]).to(device)\n",
        "    else:\n",
        "        raise Exception(\"Input need to be PIL.Image or list of PIL.Image\")\n",
        "    return tensor_imgs\n",
        "\n",
        "\n",
        "## TODO implement this\n",
        "# n = 10\n",
        "# num_rows = 4\n",
        "# num_col = n // num_rows\n",
        "# num_col  = num_col + 1 if n % num_rows else num_col\n",
        "# num_col\n",
        "def add_margin(pil_img, top = 0, right = 0, bottom = 0,\n",
        "                    left = 0, color = (255,255,255)):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def image_grid(imgs, rows = 1, cols = None,\n",
        "                    size = None,\n",
        "                   titles = None, text_pos = (0, 0)):\n",
        "    if type(imgs) == list and type(imgs[0]) == torch.Tensor:\n",
        "        imgs = torch.cat(imgs)\n",
        "    if type(imgs) == torch.Tensor:\n",
        "        imgs = tensor_to_pil(imgs)\n",
        "\n",
        "    if not size is None:\n",
        "        imgs = [img.resize((size,size)) for img in imgs]\n",
        "    if cols is None:\n",
        "        cols = len(imgs)\n",
        "    assert len(imgs) >= rows*cols\n",
        "\n",
        "    top=20\n",
        "    w, h = imgs[0].size\n",
        "    delta = 0\n",
        "    if len(imgs)> 1 and not imgs[1].size[1] == h:\n",
        "        delta = top\n",
        "        h = imgs[1].size[1]\n",
        "    if not titles is  None:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\",\n",
        "                                    size = 20, encoding=\"unic\")\n",
        "        h = top + h\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h+delta))\n",
        "    for i, img in enumerate(imgs):\n",
        "\n",
        "        if not titles is  None:\n",
        "            img = add_margin(img, top = top, bottom = 0,left=0)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.text(text_pos, titles[i],(0,0,0),\n",
        "            font = font)\n",
        "        if not delta == 0 and i > 0:\n",
        "           grid.paste(img, box=(i%cols*w, i//cols*h+delta))\n",
        "        else:\n",
        "            grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "\n",
        "    return grid\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "input_folder - dataset folder\n",
        "\"\"\"\n",
        "def load_dataset(input_folder):\n",
        "    # full_file_names = glob.glob(input_folder)\n",
        "    # class_names = [x[0] for x in os.walk(input_folder)]\n",
        "    class_names = next(os.walk(input_folder))[1]\n",
        "    class_names[:] = [d for d in class_names if not d[0] == '.']\n",
        "    file_names=[]\n",
        "    for class_name in class_names:\n",
        "        cur_path = os.path.join(input_folder, class_name)\n",
        "        filenames = next(os.walk(cur_path), (None, None, []))[2]\n",
        "        filenames = [f for f in filenames if not f[0] == '.']\n",
        "        file_names.append(filenames)\n",
        "    return class_names, file_names\n",
        "\n",
        "\n",
        "def dataset_from_yaml(yaml_location):\n",
        "    with open(yaml_location, 'r') as stream:\n",
        "        data_loaded = yaml.safe_load(stream)\n",
        "\n",
        "    return data_loaded"
      ],
      "metadata": {
        "id": "4rl6HK_8lL8k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def invert(x0:torch.FloatTensor, prompt_src:str =\"\", num_inference_steps=100, cfg_scale_src = 3.5, eta = 1):\n",
        "\n",
        "  #  inverts a real image according to Algorihm 1 in https://arxiv.org/pdf/2304.06140.pdf,\n",
        "  #  based on the code in https://github.com/inbarhub/DDPM_inversion\n",
        "\n",
        "  #  returns wt, zs, wts:\n",
        "  #  wt - inverted latent\n",
        "  #  wts - intermediate inverted latents\n",
        "  #  zs - noise maps\n",
        "\n",
        "  sd_pipe.scheduler.set_timesteps(num_diffusion_steps)\n",
        "\n",
        "  # vae encode image\n",
        "  with autocast(\"cuda\"), inference_mode():\n",
        "      w0 = (sd_pipe.vae.encode(x0).latent_dist.mode() * 0.18215).float()\n",
        "\n",
        "  # find Zs and wts - forward process\n",
        "  wt, zs, wts = inversion_forward_process(sd_pipe, w0, etas=eta, prompt=prompt_src, cfg_scale=cfg_scale_src, prog_bar=True, num_inference_steps=num_diffusion_steps)\n",
        "  return zs, wts\n",
        "\n",
        "\n",
        "\n",
        "def sample(zs, wts, prompt_tar=\"\", cfg_scale_tar=15, skip=36, eta = 1):\n",
        "\n",
        "    # reverse process (via Zs and wT)\n",
        "    w0, _ = inversion_reverse_process(sd_pipe, xT=wts[skip], etas=eta, prompts=[prompt_tar], cfg_scales=[cfg_scale_tar], prog_bar=True, zs=zs[skip:])\n",
        "\n",
        "    # vae decode image\n",
        "    with autocast(\"cuda\"), inference_mode():\n",
        "        x0_dec = sd_pipe.vae.decode(1 / 0.18215 * w0).sample\n",
        "    if x0_dec.dim()<4:\n",
        "        x0_dec = x0_dec[None,:,:,:]\n",
        "    img = image_grid(x0_dec)\n",
        "    return img\n",
        "\n",
        "def foo(a:str = \"l\") -> int:\n",
        "  print(\"a\")\n",
        "foo(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWy1SJ9MlO2P",
        "outputId": "0dc1cc00-0cd2-49ab-e8ab-57cf5992726a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEGA pipeline"
      ],
      "metadata": {
        "id": "Q0RG4kKxlUFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import warnings\n",
        "from itertools import repeat\n",
        "from typing import Callable, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
        "from diffusers.schedulers import KarrasDiffusionSchedulers\n",
        "from diffusers.utils import logging, randn_tensor\n",
        "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
        "# from . import SemanticStableDiffusionPipelineOutput\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "class SemanticStableDiffusionPipeline(DiffusionPipeline):\n",
        "    r\"\"\"\n",
        "    Pipeline for text-to-image generation with latent editing.\n",
        "\n",
        "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
        "\n",
        "    This model builds on the implementation of ['StableDiffusionPipeline']\n",
        "\n",
        "    Args:\n",
        "        vae ([`AutoencoderKL`]):\n",
        "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
        "        text_encoder ([`CLIPTextModel`]):\n",
        "            Frozen text-encoder. Stable Diffusion uses the text portion of\n",
        "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
        "            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
        "        tokenizer (`CLIPTokenizer`):\n",
        "            Tokenizer of class\n",
        "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
        "        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
        "        scheduler ([`SchedulerMixin`]):\n",
        "            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n",
        "            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
        "        safety_checker ([`Q16SafetyChecker`]):\n",
        "            Classification module that estimates whether generated images could be considered offensive or harmful.\n",
        "            Please, refer to the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) for details.\n",
        "        feature_extractor ([`CLIPImageProcessor`]):\n",
        "            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
        "    \"\"\"\n",
        "\n",
        "    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: KarrasDiffusionSchedulers,\n",
        "        safety_checker: StableDiffusionSafetyChecker,\n",
        "        feature_extractor: CLIPImageProcessor,\n",
        "        requires_safety_checker: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if safety_checker is None and requires_safety_checker:\n",
        "            logger.warning(\n",
        "                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n",
        "                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n",
        "                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n",
        "                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n",
        "                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n",
        "                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n",
        "            )\n",
        "\n",
        "        if safety_checker is not None and feature_extractor is None:\n",
        "            raise ValueError(\n",
        "                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n",
        "                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n",
        "            )\n",
        "\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=safety_checker,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
        "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
        "        self.register_to_config(requires_safety_checker=requires_safety_checker)\n",
        "\n",
        "    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker\n",
        "    def run_safety_checker(self, image, device, dtype):\n",
        "        if self.safety_checker is None:\n",
        "            has_nsfw_concept = None\n",
        "        else:\n",
        "            if torch.is_tensor(image):\n",
        "                feature_extractor_input = self.image_processor.postprocess(image, output_type=\"pil\")\n",
        "            else:\n",
        "                feature_extractor_input = self.image_processor.numpy_to_pil(image)\n",
        "            safety_checker_input = self.feature_extractor(feature_extractor_input, return_tensors=\"pt\").to(device)\n",
        "            image, has_nsfw_concept = self.safety_checker(\n",
        "                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n",
        "            )\n",
        "        return image, has_nsfw_concept\n",
        "\n",
        "    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents\n",
        "    def decode_latents(self, latents):\n",
        "        warnings.warn(\n",
        "            \"The decode_latents method is deprecated and will be removed in a future version. Please\"\n",
        "            \" use VaeImageProcessor instead\",\n",
        "            FutureWarning,\n",
        "        )\n",
        "        latents = 1 / self.vae.config.scaling_factor * latents\n",
        "        image = self.vae.decode(latents, return_dict=False)[0]\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
        "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "        return image\n",
        "\n",
        "    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n",
        "    def prepare_extra_step_kwargs(self, generator, eta):\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        # check if the scheduler accepts generator\n",
        "        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        if accepts_generator:\n",
        "            extra_step_kwargs[\"generator\"] = generator\n",
        "        return extra_step_kwargs\n",
        "\n",
        "    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.check_inputs\n",
        "    def check_inputs(\n",
        "        self,\n",
        "        prompt,\n",
        "        height,\n",
        "        width,\n",
        "        callback_steps,\n",
        "        negative_prompt=None,\n",
        "        prompt_embeds=None,\n",
        "        negative_prompt_embeds=None,\n",
        "    ):\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        if (callback_steps is None) or (\n",
        "            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n",
        "                f\" {type(callback_steps)}.\"\n",
        "            )\n",
        "\n",
        "        if prompt is not None and prompt_embeds is not None:\n",
        "            raise ValueError(\n",
        "                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
        "                \" only forward one of the two.\"\n",
        "            )\n",
        "        elif prompt is None and prompt_embeds is None:\n",
        "            raise ValueError(\n",
        "                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n",
        "            )\n",
        "        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if negative_prompt is not None and negative_prompt_embeds is not None:\n",
        "            raise ValueError(\n",
        "                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n",
        "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n",
        "            )\n",
        "\n",
        "        if prompt_embeds is not None and negative_prompt_embeds is not None:\n",
        "            if prompt_embeds.shape != negative_prompt_embeds.shape:\n",
        "                raise ValueError(\n",
        "                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n",
        "                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n",
        "                    f\" {negative_prompt_embeds.shape}.\"\n",
        "                )\n",
        "\n",
        "    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents\n",
        "    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n",
        "        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
        "        if isinstance(generator, list) and len(generator) != batch_size:\n",
        "            raise ValueError(\n",
        "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
        "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
        "            )\n",
        "\n",
        "        if latents is None:\n",
        "            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
        "        else:\n",
        "            latents = latents.to(device)\n",
        "\n",
        "        # scale the initial noise by the standard deviation required by the scheduler\n",
        "        latents = latents * self.scheduler.init_noise_sigma\n",
        "        return latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        guidance_scale: float = 7.5,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: int = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: int = 1,\n",
        "        editing_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        editing_prompt_embeddings: Optional[torch.Tensor] = None,\n",
        "        reverse_editing_direction: Optional[Union[bool, List[bool]]] = False,\n",
        "        edit_guidance_scale: Optional[Union[float, List[float]]] = 5,\n",
        "        edit_warmup_steps: Optional[Union[int, List[int]]] = 10,\n",
        "        edit_cooldown_steps: Optional[Union[int, List[int]]] = None,\n",
        "        edit_threshold: Optional[Union[float, List[float]]] = 0.9,\n",
        "        edit_momentum_scale: Optional[float] = 0.1,\n",
        "        edit_mom_beta: Optional[float] = 0.4,\n",
        "        edit_weights: Optional[List[float]] = None,\n",
        "        sem_guidance: Optional[List[torch.Tensor]] = None,\n",
        "\n",
        "        # DDPM additions\n",
        "        use_ddpm: bool = False,\n",
        "        wts: Optional[List[torch.Tensor]] = None,\n",
        "        zs: Optional[List[torch.Tensor]] = None\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Function invoked when calling the pipeline for generation.\n",
        "\n",
        "        Args:\n",
        "            prompt (`str` or `List[str]`):\n",
        "                The prompt or prompts to guide the image generation.\n",
        "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The height in pixels of the generated image.\n",
        "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The width in pixels of the generated image.\n",
        "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
        "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
        "                expense of slower inference.\n",
        "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
        "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
        "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
        "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
        "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
        "                usually at the expense of lower image quality.\n",
        "            negative_prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n",
        "                if `guidance_scale` is less than `1`).\n",
        "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
        "                The number of images to generate per prompt.\n",
        "            eta (`float`, *optional*, defaults to 0.0):\n",
        "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
        "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
        "            generator (`torch.Generator`, *optional*):\n",
        "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
        "                to make generation deterministic.\n",
        "            latents (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
        "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
        "                tensor will ge generated by sampling using the supplied random `generator`.\n",
        "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
        "                The output format of the generate image. Choose between\n",
        "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
        "                plain tuple.\n",
        "            callback (`Callable`, *optional*):\n",
        "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
        "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
        "            callback_steps (`int`, *optional*, defaults to 1):\n",
        "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
        "                called at every step.\n",
        "            editing_prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to use for Semantic guidance. Semantic guidance is disabled by setting\n",
        "                `editing_prompt = None`. Guidance direction of prompt should be specified via\n",
        "                `reverse_editing_direction`.\n",
        "            editing_prompt_embeddings (`torch.Tensor>`, *optional*):\n",
        "                Pre-computed embeddings to use for semantic guidance. Guidance direction of embedding should be\n",
        "                specified via `reverse_editing_direction`.\n",
        "            reverse_editing_direction (`bool` or `List[bool]`, *optional*, defaults to `False`):\n",
        "                Whether the corresponding prompt in `editing_prompt` should be increased or decreased.\n",
        "            edit_guidance_scale (`float` or `List[float]`, *optional*, defaults to 5):\n",
        "                Guidance scale for semantic guidance. If provided as list values should correspond to `editing_prompt`.\n",
        "                `edit_guidance_scale` is defined as `s_e` of equation 6 of [SEGA\n",
        "                Paper](https://arxiv.org/pdf/2301.12247.pdf).\n",
        "            edit_warmup_steps (`float` or `List[float]`, *optional*, defaults to 10):\n",
        "                Number of diffusion steps (for each prompt) for which semantic guidance will not be applied. Momentum\n",
        "                will still be calculated for those steps and applied once all warmup periods are over.\n",
        "                `edit_warmup_steps` is defined as `delta` (δ) of [SEGA Paper](https://arxiv.org/pdf/2301.12247.pdf).\n",
        "            edit_cooldown_steps (`float` or `List[float]`, *optional*, defaults to `None`):\n",
        "                Number of diffusion steps (for each prompt) after which semantic guidance will no longer be applied.\n",
        "            edit_threshold (`float` or `List[float]`, *optional*, defaults to 0.9):\n",
        "                Threshold of semantic guidance.\n",
        "            edit_momentum_scale (`float`, *optional*, defaults to 0.1):\n",
        "                Scale of the momentum to be added to the semantic guidance at each diffusion step. If set to 0.0\n",
        "                momentum will be disabled. Momentum is already built up during warmup, i.e. for diffusion steps smaller\n",
        "                than `sld_warmup_steps`. Momentum will only be added to latent guidance once all warmup periods are\n",
        "                finished. `edit_momentum_scale` is defined as `s_m` of equation 7 of [SEGA\n",
        "                Paper](https://arxiv.org/pdf/2301.12247.pdf).\n",
        "            edit_mom_beta (`float`, *optional*, defaults to 0.4):\n",
        "                Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous\n",
        "                momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller\n",
        "                than `edit_warmup_steps`. `edit_mom_beta` is defined as `beta_m` (β) of equation 8 of [SEGA\n",
        "                Paper](https://arxiv.org/pdf/2301.12247.pdf).\n",
        "            edit_weights (`List[float]`, *optional*, defaults to `None`):\n",
        "                Indicates how much each individual concept should influence the overall guidance. If no weights are\n",
        "                provided all concepts are applied equally. `edit_mom_beta` is defined as `g_i` of equation 9 of [SEGA\n",
        "                Paper](https://arxiv.org/pdf/2301.12247.pdf).\n",
        "            sem_guidance (`List[torch.Tensor]`, *optional*):\n",
        "                List of pre-generated guidance vectors to be applied at generation. Length of the list has to\n",
        "                correspond to `num_inference_steps`.\n",
        "\n",
        "        Returns:\n",
        "            [`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`] or `tuple`:\n",
        "            [`~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput`] if `return_dict` is True,\n",
        "            otherwise a `tuple. When returning a tuple, the first element is a list with the generated images, and the\n",
        "            second element is a list of `bool`s denoting whether the corresponding generated image likely represents\n",
        "            \"not-safe-for-work\" (nsfw) content, according to the `safety_checker`.\n",
        "        \"\"\"\n",
        "        # 0. Default height and width to unet\n",
        "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
        "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
        "\n",
        "        # 1. Check inputs. Raise error if not correct\n",
        "        self.check_inputs(prompt, height, width, callback_steps)\n",
        "\n",
        "        # 2. Define call parameters\n",
        "        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
        "\n",
        "        if editing_prompt:\n",
        "            enable_edit_guidance = True\n",
        "            if isinstance(editing_prompt, str):\n",
        "                editing_prompt = [editing_prompt]\n",
        "            enabled_editing_prompts = len(editing_prompt)\n",
        "        elif editing_prompt_embeddings is not None:\n",
        "            enable_edit_guidance = True\n",
        "            enabled_editing_prompts = editing_prompt_embeddings.shape[0]\n",
        "        else:\n",
        "            enabled_editing_prompts = 0\n",
        "            enable_edit_guidance = False\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_input_ids = text_inputs.input_ids\n",
        "\n",
        "        if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
        "            removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n",
        "            logger.warning(\n",
        "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
        "                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
        "            )\n",
        "            text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
        "        text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n",
        "\n",
        "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "        bs_embed, seq_len, _ = text_embeddings.shape\n",
        "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
        "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "        if enable_edit_guidance:\n",
        "            # get safety text embeddings\n",
        "            if editing_prompt_embeddings is None:\n",
        "                edit_concepts_input = self.tokenizer(\n",
        "                    [x for item in editing_prompt for x in repeat(item, batch_size)],\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.tokenizer.model_max_length,\n",
        "                    return_tensors=\"pt\",\n",
        "                )\n",
        "\n",
        "                edit_concepts_input_ids = edit_concepts_input.input_ids\n",
        "\n",
        "                if edit_concepts_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
        "                    removed_text = self.tokenizer.batch_decode(\n",
        "                        edit_concepts_input_ids[:, self.tokenizer.model_max_length :]\n",
        "                    )\n",
        "                    logger.warning(\n",
        "                        \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
        "                        f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
        "                    )\n",
        "                    edit_concepts_input_ids = edit_concepts_input_ids[:, : self.tokenizer.model_max_length]\n",
        "                edit_concepts = self.text_encoder(edit_concepts_input_ids.to(self.device))[0]\n",
        "            else:\n",
        "                edit_concepts = editing_prompt_embeddings.to(self.device).repeat(batch_size, 1, 1)\n",
        "\n",
        "            # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
        "            bs_embed_edit, seq_len_edit, _ = edit_concepts.shape\n",
        "            edit_concepts = edit_concepts.repeat(1, num_images_per_prompt, 1)\n",
        "            edit_concepts = edit_concepts.view(bs_embed_edit * num_images_per_prompt, seq_len_edit, -1)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "\n",
        "        if do_classifier_free_guidance:\n",
        "            uncond_tokens: List[str]\n",
        "            if negative_prompt is None:\n",
        "                uncond_tokens = [\"\"]\n",
        "            elif type(prompt) is not type(negative_prompt):\n",
        "                raise TypeError(\n",
        "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
        "                    f\" {type(prompt)}.\"\n",
        "                )\n",
        "            elif isinstance(negative_prompt, str):\n",
        "                uncond_tokens = [negative_prompt]\n",
        "            elif batch_size != len(negative_prompt):\n",
        "                raise ValueError(\n",
        "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
        "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
        "                    \" the batch size of `prompt`.\"\n",
        "                )\n",
        "            else:\n",
        "                uncond_tokens = negative_prompt\n",
        "\n",
        "            max_length = text_input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                uncond_tokens,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
        "            seq_len = uncond_embeddings.shape[1]\n",
        "            uncond_embeddings = uncond_embeddings.repeat(batch_size, num_images_per_prompt, 1)\n",
        "            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            if enable_edit_guidance:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, text_embeddings, edit_concepts])\n",
        "            else:\n",
        "                text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # 4. Prepare timesteps\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=self.device)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "        if use_ddpm:\n",
        "          t_to_idx = {int(v):k for k,v in enumerate(timesteps[-zs.shape[0]:])}\n",
        "          timesteps = timesteps[-zs.shape[0]:]\n",
        "\n",
        "        # 5. Prepare latent variables\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            text_embeddings.dtype,\n",
        "            self.device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        # 6. Prepare extra step kwargs.\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # Initialize edit_momentum to None\n",
        "        edit_momentum = None\n",
        "\n",
        "        self.uncond_estimates = None\n",
        "        self.text_estimates = None\n",
        "        self.edit_estimates = None\n",
        "        self.sem_guidance = None\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = (\n",
        "                torch.cat([latents] * (2 + enabled_editing_prompts)) if do_classifier_free_guidance else latents\n",
        "            )\n",
        "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_out = noise_pred.chunk(2 + enabled_editing_prompts)  # [b,4, 64, 64]\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred_out[0], noise_pred_out[1]\n",
        "                noise_pred_edit_concepts = noise_pred_out[2:]\n",
        "\n",
        "                # default text guidance\n",
        "                noise_guidance = guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "                # noise_guidance = (noise_pred_text - noise_pred_edit_concepts[0])\n",
        "\n",
        "                if self.uncond_estimates is None:\n",
        "                    self.uncond_estimates = torch.zeros((num_inference_steps + 1, *noise_pred_uncond.shape))\n",
        "                self.uncond_estimates[i] = noise_pred_uncond.detach().cpu()\n",
        "\n",
        "                if self.text_estimates is None:\n",
        "                    self.text_estimates = torch.zeros((num_inference_steps + 1, *noise_pred_text.shape))\n",
        "                self.text_estimates[i] = noise_pred_text.detach().cpu()\n",
        "\n",
        "                if self.edit_estimates is None and enable_edit_guidance:\n",
        "                    self.edit_estimates = torch.zeros(\n",
        "                        (num_inference_steps + 1, len(noise_pred_edit_concepts), *noise_pred_edit_concepts[0].shape)\n",
        "                    )\n",
        "\n",
        "                if self.sem_guidance is None:\n",
        "                    self.sem_guidance = torch.zeros((num_inference_steps + 1, *noise_pred_text.shape))\n",
        "\n",
        "                if edit_momentum is None:\n",
        "                    edit_momentum = torch.zeros_like(noise_guidance)\n",
        "\n",
        "                if enable_edit_guidance:\n",
        "                    concept_weights = torch.zeros(\n",
        "                        (len(noise_pred_edit_concepts), noise_guidance.shape[0]),\n",
        "                        device=self.device,\n",
        "                        dtype=noise_guidance.dtype,\n",
        "                    )\n",
        "                    noise_guidance_edit = torch.zeros(\n",
        "                        (len(noise_pred_edit_concepts), *noise_guidance.shape),\n",
        "                        device=self.device,\n",
        "                        dtype=noise_guidance.dtype,\n",
        "                    )\n",
        "                    # noise_guidance_edit = torch.zeros_like(noise_guidance)\n",
        "                    warmup_inds = []\n",
        "                    for c, noise_pred_edit_concept in enumerate(noise_pred_edit_concepts):\n",
        "                        self.edit_estimates[i, c] = noise_pred_edit_concept\n",
        "                        if isinstance(edit_guidance_scale, list):\n",
        "                            edit_guidance_scale_c = edit_guidance_scale[c]\n",
        "                        else:\n",
        "                            edit_guidance_scale_c = edit_guidance_scale\n",
        "\n",
        "                        if isinstance(edit_threshold, list):\n",
        "                            edit_threshold_c = edit_threshold[c]\n",
        "                        else:\n",
        "                            edit_threshold_c = edit_threshold\n",
        "                        if isinstance(reverse_editing_direction, list):\n",
        "                            reverse_editing_direction_c = reverse_editing_direction[c]\n",
        "                        else:\n",
        "                            reverse_editing_direction_c = reverse_editing_direction\n",
        "                        if edit_weights:\n",
        "                            edit_weight_c = edit_weights[c]\n",
        "                        else:\n",
        "                            edit_weight_c = 1.0\n",
        "                        if isinstance(edit_warmup_steps, list):\n",
        "                            edit_warmup_steps_c = edit_warmup_steps[c]\n",
        "                        else:\n",
        "                            edit_warmup_steps_c = edit_warmup_steps\n",
        "\n",
        "                        if isinstance(edit_cooldown_steps, list):\n",
        "                            edit_cooldown_steps_c = edit_cooldown_steps[c]\n",
        "                        elif edit_cooldown_steps is None:\n",
        "                            edit_cooldown_steps_c = i + 1\n",
        "                        else:\n",
        "                            edit_cooldown_steps_c = edit_cooldown_steps\n",
        "                        if i >= edit_warmup_steps_c:\n",
        "                            warmup_inds.append(c)\n",
        "                        if i >= edit_cooldown_steps_c:\n",
        "                            noise_guidance_edit[c, :, :, :, :] = torch.zeros_like(noise_pred_edit_concept)\n",
        "                            continue\n",
        "\n",
        "                        noise_guidance_edit_tmp = noise_pred_edit_concept - noise_pred_uncond\n",
        "                        # tmp_weights = (noise_pred_text - noise_pred_edit_concept).sum(dim=(1, 2, 3))\n",
        "                        tmp_weights = (noise_guidance - noise_pred_edit_concept).sum(dim=(1, 2, 3))\n",
        "\n",
        "                        tmp_weights = torch.full_like(tmp_weights, edit_weight_c)  # * (1 / enabled_editing_prompts)\n",
        "                        if reverse_editing_direction_c:\n",
        "                            noise_guidance_edit_tmp = noise_guidance_edit_tmp * -1\n",
        "                        concept_weights[c, :] = tmp_weights\n",
        "\n",
        "                        noise_guidance_edit_tmp = noise_guidance_edit_tmp * edit_guidance_scale_c\n",
        "\n",
        "                        # torch.quantile function expects float32\n",
        "                        if noise_guidance_edit_tmp.dtype == torch.float32:\n",
        "                            tmp = torch.quantile(\n",
        "                                torch.abs(noise_guidance_edit_tmp).flatten(start_dim=2),\n",
        "                                edit_threshold_c,\n",
        "                                dim=2,\n",
        "                                keepdim=False,\n",
        "                            )\n",
        "                        else:\n",
        "                            tmp = torch.quantile(\n",
        "                                torch.abs(noise_guidance_edit_tmp).flatten(start_dim=2).to(torch.float32),\n",
        "                                edit_threshold_c,\n",
        "                                dim=2,\n",
        "                                keepdim=False,\n",
        "                            ).to(noise_guidance_edit_tmp.dtype)\n",
        "\n",
        "                        noise_guidance_edit_tmp = torch.where(\n",
        "                            torch.abs(noise_guidance_edit_tmp) >= tmp[:, :, None, None],\n",
        "                            noise_guidance_edit_tmp,\n",
        "                            torch.zeros_like(noise_guidance_edit_tmp),\n",
        "                        )\n",
        "                        noise_guidance_edit[c, :, :, :, :] = noise_guidance_edit_tmp\n",
        "\n",
        "                        # noise_guidance_edit = noise_guidance_edit + noise_guidance_edit_tmp\n",
        "\n",
        "                    warmup_inds = torch.tensor(warmup_inds).to(self.device)\n",
        "                    if len(noise_pred_edit_concepts) > warmup_inds.shape[0] > 0:\n",
        "                        concept_weights = concept_weights.to(\"cpu\")  # Offload to cpu\n",
        "                        noise_guidance_edit = noise_guidance_edit.to(\"cpu\")\n",
        "\n",
        "                        concept_weights_tmp = torch.index_select(concept_weights.to(self.device), 0, warmup_inds)\n",
        "                        concept_weights_tmp = torch.where(\n",
        "                            concept_weights_tmp < 0, torch.zeros_like(concept_weights_tmp), concept_weights_tmp\n",
        "                        )\n",
        "                        concept_weights_tmp = concept_weights_tmp / concept_weights_tmp.sum(dim=0)\n",
        "                        # concept_weights_tmp = torch.nan_to_num(concept_weights_tmp)\n",
        "\n",
        "                        noise_guidance_edit_tmp = torch.index_select(\n",
        "                            noise_guidance_edit.to(self.device), 0, warmup_inds\n",
        "                        )\n",
        "                        noise_guidance_edit_tmp = torch.einsum(\n",
        "                            \"cb,cbijk->bijk\", concept_weights_tmp, noise_guidance_edit_tmp\n",
        "                        )\n",
        "                        noise_guidance_edit_tmp = noise_guidance_edit_tmp\n",
        "                        noise_guidance = noise_guidance + noise_guidance_edit_tmp\n",
        "\n",
        "                        self.sem_guidance[i] = noise_guidance_edit_tmp.detach().cpu()\n",
        "\n",
        "                        del noise_guidance_edit_tmp\n",
        "                        del concept_weights_tmp\n",
        "                        concept_weights = concept_weights.to(self.device)\n",
        "                        noise_guidance_edit = noise_guidance_edit.to(self.device)\n",
        "\n",
        "                    concept_weights = torch.where(\n",
        "                        concept_weights < 0, torch.zeros_like(concept_weights), concept_weights\n",
        "                    )\n",
        "\n",
        "                    concept_weights = torch.nan_to_num(concept_weights)\n",
        "\n",
        "                    noise_guidance_edit = torch.einsum(\"cb,cbijk->bijk\", concept_weights, noise_guidance_edit)\n",
        "\n",
        "                    noise_guidance_edit = noise_guidance_edit + edit_momentum_scale * edit_momentum\n",
        "\n",
        "                    edit_momentum = edit_mom_beta * edit_momentum + (1 - edit_mom_beta) * noise_guidance_edit\n",
        "\n",
        "                    if warmup_inds.shape[0] == len(noise_pred_edit_concepts):\n",
        "                        noise_guidance = noise_guidance + noise_guidance_edit\n",
        "                        self.sem_guidance[i] = noise_guidance_edit.detach().cpu()\n",
        "\n",
        "                if sem_guidance is not None:\n",
        "                    edit_guidance = sem_guidance[i].to(self.device)\n",
        "                    noise_guidance = noise_guidance + edit_guidance\n",
        "\n",
        "                noise_pred = noise_pred_uncond + noise_guidance\n",
        "            ## ddpm ###########################################################\n",
        "            if use_ddpm:\n",
        "\n",
        "              idx = t_to_idx[int(t)]\n",
        "              z = zs[idx] if not zs is None else None\n",
        "\n",
        "              # 1. get previous step value (=t-1)\n",
        "              prev_timestep = t - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
        "              # 2. compute alphas, betas\n",
        "              alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
        "              alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
        "              beta_prod_t = 1 - alpha_prod_t\n",
        "\n",
        "              # 3. compute predicted original sample from predicted noise also called\n",
        "              # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "              pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
        "\n",
        "\n",
        "              # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
        "              # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
        "              # variance = self.scheduler._get_variance(timestep, prev_timestep)\n",
        "              # variance = get_variance(model, t) #, prev_timestep)\n",
        "              beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
        "              variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
        "\n",
        "\n",
        "\n",
        "              std_dev_t = eta * variance ** (0.5)\n",
        "              # Take care of asymetric reverse process (asyrp)\n",
        "              noise_pred_direction = noise_pred\n",
        "\n",
        "              # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "              # pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output_direction\n",
        "              pred_sample_direction = (1 - alpha_prod_t_prev - eta * variance) ** (0.5) * noise_pred_direction\n",
        "\n",
        "              # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "              prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
        "              # 8. Add noice if eta > 0\n",
        "              if eta > 0:\n",
        "                  if z is None:\n",
        "                      z = torch.randn(noise_pred.shape, device=self.device)\n",
        "                  sigma_z =  eta * variance ** (0.5) * z\n",
        "                  latents = prev_sample + sigma_z\n",
        "\n",
        "            ## ddpm ##########################################################\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "            else: #if not use_ddpm:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "            # call the callback, if provided\n",
        "            if callback is not None and i % callback_steps == 0:\n",
        "                callback(i, t, latents)\n",
        "\n",
        "        # 8. Post-processing\n",
        "        if not output_type == \"latent\":\n",
        "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
        "            image, has_nsfw_concept = self.run_safety_checker(image, self.device, text_embeddings.dtype)\n",
        "        else:\n",
        "            image = latents\n",
        "            has_nsfw_concept = None\n",
        "\n",
        "        if has_nsfw_concept is None:\n",
        "            do_denormalize = [True] * image.shape[0]\n",
        "        else:\n",
        "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
        "\n",
        "        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, has_nsfw_concept)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
      ],
      "metadata": {
        "id": "0VC8cGXlleHd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pipeline"
      ],
      "metadata": {
        "id": "toBLOQCRlhY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "import random\n",
        "from io import BytesIO\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DDIMScheduler\n",
        "from torch import autocast, inference_mode\n",
        "import re\n",
        "\n",
        "# load pipelines\n",
        "sd_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sd_pipe = StableDiffusionPipeline.from_pretrained(sd_model_id).to(device)\n",
        "sd_pipe.scheduler = DDIMScheduler.from_config(sd_model_id, subfolder = \"scheduler\")\n",
        "sega_pipe = SemanticStableDiffusionPipeline.from_pretrained(sd_model_id).to(device)"
      ],
      "metadata": {
        "id": "oubuHPfwli97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit(wts, zs,\n",
        "            tar_prompt = \"\",\n",
        "            steps = 100,\n",
        "            skip = 36,\n",
        "            tar_cfg_scale =15,\n",
        "            edit_concept = \"\",\n",
        "            guidnace_scale = 7,\n",
        "            warmup = 1,\n",
        "            neg_guidance=False,\n",
        "            threshold=0.95\n",
        "\n",
        "   ):\n",
        "\n",
        "    # SEGA\n",
        "    # parse concepts and neg guidance\n",
        "    editing_args = dict(\n",
        "    editing_prompt = edit_concept,\n",
        "    reverse_editing_direction = neg_guidance,\n",
        "    edit_warmup_steps=warmup,\n",
        "    edit_guidance_scale=guidnace_scale,\n",
        "    edit_threshold=threshold,\n",
        "    edit_momentum_scale=0.5,\n",
        "    edit_mom_beta=0.6,\n",
        "    eta=1,\n",
        "  )\n",
        "    latnets = wts[skip].expand(1, -1, -1, -1)\n",
        "    sega_out = sega_pipe(prompt=tar_prompt, latents=latnets, guidance_scale = tar_cfg_scale,\n",
        "                        num_images_per_prompt=1,\n",
        "                        num_inference_steps=steps,\n",
        "                        use_ddpm=True,  wts=wts, zs=zs[skip:], **editing_args)\n",
        "    return sega_out.images[0]"
      ],
      "metadata": {
        "id": "bPFR3lXSlpP9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare examples"
      ],
      "metadata": {
        "id": "sYjhWAEImPl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://huggingface.co/spaces/editing-images/ledits"
      ],
      "metadata": {
        "id": "wlrPFIummPAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Main"
      ],
      "metadata": {
        "id": "9lq7RL_Qlygz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "input_image = \"/content/ledits/examples/butterfly_input.jpg\" #@param\n",
        "\n",
        "source_prompt  =\"\" #@param\n",
        "target_prompt =\"oil painting\" #@param\n",
        "num_diffusion_steps =100 #@param\n",
        "source_guidance_scale = 3.5 #@param\n",
        "reconstruct = True #@param\n",
        "skip_steps =36 #@param\n",
        "target_guidance_scale=20 #@param\n",
        "\n",
        "# SEGA only params\n",
        "edit_concepts=[\"buttefly\", \"bee\"]#@param\n",
        "edit_guidance_scales=[7,15] #@param\n",
        "warmup_steps=[1,1] #@param\n",
        "reverse_editing=[True, False] #@param\n",
        "thresholds = [ 0.95,0.95] #@param\n",
        "\n",
        "# uncomment for reproducabilty\n",
        "import torch\n",
        "seed = 36478574352 #@param\n",
        "# torch.manual_seed(seed)\n"
      ],
      "metadata": {
        "id": "gFqqlnA9lzsj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invert with ddpm\n",
        "x0 = load_512(input_image, device=device)\n",
        "# noise maps and latents\n",
        "zs, wts = invert(x0 =x0 , prompt_src=source_prompt, num_inference_steps=num_diffusion_steps, cfg_scale_src=source_guidance_scale)\n",
        "if reconstruct:\n",
        "  ddpm_out_img = sample(zs, wts, prompt_tar=target_prompt, skip=skip_steps, cfg_scale_tar=target_guidance_scale)\n",
        "\n",
        "# edit with the pre-computed latents and noise maps\n",
        "sega_ddpm_edited_img =edit(wts, zs,\n",
        "            tar_prompt = target_prompt,\n",
        "            steps = num_diffusion_steps,\n",
        "            skip = skip_steps,\n",
        "            tar_cfg_scale =target_guidance_scale,\n",
        "            edit_concept = edit_concepts,\n",
        "            guidnace_scale = edit_guidance_scales,\n",
        "            warmup = warmup_steps,\n",
        "            neg_guidance=reverse_editing,\n",
        "            threshold=thresholds)"
      ],
      "metadata": {
        "id": "PDkpRzybl4dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show results"
      ],
      "metadata": {
        "id": "DXrtCKqhmC0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show results\n",
        "def display(show_reconstruction):\n",
        "  orig_img_pt = load_512(input_image)\n",
        "  orig_img = tensor_to_pil(orig_img_pt)[0]\n",
        "  if show_reconstruction:\n",
        "    return image_grid([orig_img, ddpm_out_img, sega_ddpm_edited_img], rows=1, cols=3)\n",
        "  else:\n",
        "    return image_grid([orig_img, sega_ddpm_edited_img], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "HGHRrzGWmAX-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(show_reconstruction=True)"
      ],
      "metadata": {
        "id": "cxbmT7ETmD-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}