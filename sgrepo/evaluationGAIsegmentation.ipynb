{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPl3lh2Is1g1scplJgCCtz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/sgrepo/evaluationGAIsegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FoxIPJBMn24"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://img.freepik.com/free-photo/picture-frame-on-a-wall-with-scandinavian-home-interior_53876-139779.jpg -O /content/room001.jpg"
      ],
      "metadata": {
        "id": "GwkUZYTvM5Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py"
      ],
      "metadata": {
        "id": "ybqKZhc-NI0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oneformer inference"
      ],
      "metadata": {
        "id": "yw0gva7BUno8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import OneFormerProcessor, OneFormerModel\n",
        "from torch import autocast\n",
        "import json"
      ],
      "metadata": {
        "id": "RZCNrVbeNYlx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_images_dir = \"/content/objects\"\n",
        "os.makedirs(object_images_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "EYCAj3vBWxiz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load oneformer model\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "processor = OneFormerProcessor.from_pretrained(model_id)\n",
        "model = OneFormerModel.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txQOOQ_nNb4J",
        "outputId": "0bb185f6-3ff7-4f39-ff05-384ab5ebaa4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:427: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = f\"/content/room001.jpg\"\n",
        "image = Image.open(img_path)\n",
        "\n",
        "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# you can pass them to processor for semantic postprocessing\n",
        "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
        "      outputs, target_sizes=[image.size[::-1]]\n",
        ")[0]\n",
        "\n",
        "print(np.unique(predicted_semantic_map))"
      ],
      "metadata": {
        "id": "M2SdOimeNfcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227e43c6-743b-4f0e-e1f1-6cfb3318871a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   3  17  22  23  28  30  36  39  42  64  67 125 135]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_categories = [\"sofa\", \"table\", \"cushion\", \"rug\", \"chair\"]\n",
        "sub_categories = [\"plant\", \"painting\", \"vase\"]\n",
        "item_list = model.config.label2id.keys()\n",
        "include_categories = {}\n",
        "include_categories[\"target\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in target_categories)]\n",
        "include_categories[\"sub\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in sub_categories)]\n",
        "\n",
        "print(include_categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C455nSH_OHw-",
        "outputId": "6a3d59fa-31b0-47bb-fbb8-5b51f5294a6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'target': [30, 19, 64, 39, 56, 28, 23, 75, 15], 'sub': [22, 17, 135]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bounding_box(mask_image):\n",
        "    # Find the indices of non-zero pixels within the mask image\n",
        "    non_zero_pixels = np.transpose(np.nonzero(mask_image))\n",
        "\n",
        "    if non_zero_pixels.size == 0:\n",
        "        # Return an empty Bounding Box if there are no non-zero pixels in the mask\n",
        "        return None\n",
        "\n",
        "    # Get x and y coordinates\n",
        "    x_coords, y_coords = non_zero_pixels[:, 0], non_zero_pixels[:, 1]\n",
        "\n",
        "    # Calculate the Bounding Box coordinates\n",
        "    min_x, min_y = np.min(x_coords), np.min(y_coords)\n",
        "    max_x, max_y = np.max(x_coords), np.max(y_coords)\n",
        "\n",
        "    return (min_x, min_y, max_x, max_y)"
      ],
      "metadata": {
        "id": "r-2-Re9mTUgF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_list = [\n",
        "    [255, 0, 0],     # 赤\n",
        "    [0, 255, 0],     # 緑\n",
        "    [0, 0, 255],     # 青\n",
        "    [255, 255, 0],   # イエロー\n",
        "    [255, 0, 255],   # マゼンタ\n",
        "    [0, 255, 255],   # シアン\n",
        "    [128, 0, 128],   # パープル\n",
        "    [128, 128, 128], # グレー\n",
        "    [0, 128, 0],     # オリーブ\n",
        "    [128, 0, 0]      # マルーン\n",
        "]\n",
        "\n",
        "color_dict = {}\n",
        "for idx, t in enumerate(target_categories):\n",
        "  color_dict[t] = color_list[idx]"
      ],
      "metadata": {
        "id": "ukwdXnJdbT29"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "total_count = np.prod(target_base_mask.shape)\n",
        "source_image = cv2.imread(img_path)\n",
        "bg_image = np.zeros_like(source_image)\n",
        "\n",
        "os.makedirs(object_images_dir + \"/target\", exist_ok=True)\n",
        "\n",
        "label_dict = {}\n",
        "\n",
        "result_dict = {}\n",
        "\n",
        "for target_id in include_categories[\"target\"]:\n",
        "  label_pred_map = np.where(predicted_semantic_map == target_id, 255, 0)\n",
        "  # bboxのリストと領域の計算\n",
        "  count = np.count_nonzero(label_pred_map == 255)\n",
        "  if count > 0:\n",
        "    bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "    mask_ratio = count / total_count\n",
        "    ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "    label = model.config.id2label[target_id].split(\",\")[0].replace(\" \", \"\")\n",
        "    if label in label_dict.keys():\n",
        "      label_dict[label] += 1\n",
        "    else:\n",
        "      label_dict[label] = 0\n",
        "\n",
        "    target_color = None\n",
        "    for cl in color_dict.keys():\n",
        "      if cl in label:\n",
        "        target_color = color_dict[cl]\n",
        "\n",
        "    if target_color is not None:\n",
        "      center = [int(xmin * 0.5 + xmax * 0.5), int(ymin * 0.5 + ymax * 0.5)]\n",
        "      cv2.circle(bg_image,\n",
        "            center=(center[0], center[1]),\n",
        "            radius=20,\n",
        "            color=(target_color[0], target_color[1], target_color[2]),\n",
        "            thickness=-1,\n",
        "            lineType=cv2.LINE_4,\n",
        "            shift=0)\n",
        "\n",
        "      item_dict = {\n",
        "        \"bbox\": [int(xmin), int(ymin), int(xmax), int(ymax)],\n",
        "        \"ratio\": float(mask_ratio),\n",
        "        \"graph\": {\n",
        "            \"label\": label,\n",
        "            \"color\": target_color,\n",
        "            \"center\": center\n",
        "          }\n",
        "      }\n",
        "\n",
        "    result_dict[label] = item_dict\n",
        "\n",
        "    target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "    cv2.imwrite(object_images_dir + f\"/target/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "\n",
        "\n",
        "  target_base_mask += label_pred_map\n",
        "\n",
        "\n",
        "cv2.imwrite(\"/content/room001_graph.jpg\", bg_image)\n",
        "json.dump(result_dict, open(\"/content/room001.json\", \"w\"))\n",
        "display(Image.fromarray(target_base_mask.astype(np.uint8)))\n",
        "display(Image.fromarray(bg_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "f09lbguuNmUH",
        "outputId": "4c1c716a-a5fb-4e35-9d32-6bd5467f84c7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=626x417>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=626x417>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "os.makedirs(object_images_dir + \"/sub\", exist_ok=True)\n",
        "label_dict = {}\n",
        "for sub_id in include_categories[\"sub\"]:\n",
        "  label_pred_map = np.where(predicted_semantic_map == sub_id, 255, 0)\n",
        "  count = np.count_nonzero(label_pred_map == 255)\n",
        "  if count > 0:\n",
        "    bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "    print(count / total_count, bounding_box)\n",
        "    mask_ratio = count / total_count\n",
        "    ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "    label = model.config.id2label[sub_id].split(\",\")[0].replace(\" \", \"\")\n",
        "    if label in label_dict.keys():\n",
        "      label_dict[label] += 1\n",
        "    else:\n",
        "      label_dict[label] = 0\n",
        "\n",
        "    target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "    cv2.imwrite(object_images_dir + f\"/sub/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "  sub_base_mask += label_pred_map\n",
        "\n",
        "display(Image.fromarray(sub_base_mask.astype(np.uint8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "FLX6teWeQXzX",
        "outputId": "9a417412-c91c-4ab6-f215-fc774a0f3d06"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09913347277449606 (21, 220, 182, 380)\n",
            "0.0017928149493185005 (216, 35, 266, 98)\n",
            "0.00332896621999525 (248, 84, 278, 117)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=626x417>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAGhCAAAAADABwZVAAAEaElEQVR4nO3d0VajSBRAUTNr/v+XmQfbaZMBJW1xwhR7v2gbO2I861YQJG9vAAAAAAAAAAAAAAAAwC+3V2/ACyzv3/by4s244kP/9jb/9/3qrr40+4O/7q9Xb8CxTl3cRc2d3MmLO/nmHWTq5K75Iz27qZPjjCRHTHLEJEdMcsQkR0xyxCRHbOrkrnkM8+ymTo4zkhwxyRGTHDHJEZMcsamTc77cGU2dHGckOWKSIyY5YpIjJjliUyfnTJIzmjo5zkhyxCRHTHLEJEdMcsSmTs6ZJGc0dXKckeSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSISY6Y5IhJjpjkiEmOmOSITZ3c7dUbwIqpk+OMJEdMcsQkR0xyxCRHTHLEJEdMcsQkR0xyxCRHTHLEJEdMcsQkR0xyxCRHTHLEJEdMcsQkR0xyxCRHTHLEJEdMcsQkR0xyxCRHTHLEJEdMcsQkR0xyxCRHTHLEJEds6uSWV28AK6ZOzus+nNHUyXFGkiMmOWKSIyY5YpIjJjlikiMmOWKSIyY5YpIjJjlikiMmOWKSIyY5YpIjJjlikiMmOWKSIyY5YpIjJjlikiMmOWKSIyY5YpIjJjlikiMmOWKSIyY5YpIjJjlikiMmOWKSIzZ3cl5r5IRm/6GMfk2l28Pd/unjt0z/yAMAAAAAAAAAAAAAAAAAAAAAAAAAnNToC1Yyp1GXD13G3h3zGnV5aq2x09xXROeEhiVnzLHPsOTsPLDPqOSWuzewyXM5YoOSM93Ya8yz/uXf+1qG3SfstJh4xCQHAENZWgFgLGsrGxzwInZYcn4jzLoDp5zmWHPkwqo5Vhx1CP4jN4f4eXBQEp8GnOi48/TC+vRqaXnlzrPJLV8ltH6T5vhs77K3vL3dvv776OXuJisrG3b2cD+p1sJ6uMnuAxv2JfGQ1kZXy+e7ux968OEHyS33/1z5P4rjv3btPqzuAHy/V6A4Vjyxx6ogRvh7x+d8/KHgbXn44LuvUrS68mhPcn8azXuVi+b4bE9ya74bcsvqu5ufznU8dfRhfy7b+xbOo7u6p6fcz6eUlfbanj3GOiIXc+7Snkzu9vud2+1295EnaO7KjlnkvmvK0nphT0y5/U/8TTG27U9ukRIj7E5OboyxN7mxxen3wnYm9/vKrPAz+5IzlRjmmd/LGXIM4Hw5YvuTUxxD7E5OcYyxN7mxxen3wvadvCQRhnEVTWKSI3ZIctZhth1/fbnsa/L/YGEldlByXw0yQ+7ajppy210p7uIOW1i3ylLc1dXP5RR3eUcmsH2NTS7s0AicSwwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAND7B+EUVKHHMQyHAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}