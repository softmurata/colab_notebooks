{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNA3SAhZ5urtzlXqK0+rZOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/sgrepo/PeRFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgkSV4y95wZo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/magic-research/piecewise-rectified-flow.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers accelerate transformers\n",
        "!pip install -q controlnet_aux"
      ],
      "metadata": {
        "id": "MW2KwYrQ6BjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/piecewise-rectified-flow\n",
        "\n",
        "import random, argparse, os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch, torchvision\n",
        "from diffusers import UNet2DConditionModel, ControlNetModel, StableDiffusionControlNetPipeline, StableDiffusionControlNetImg2ImgPipeline\n",
        "from diffusers.utils import make_image_grid, load_image\n",
        "\n",
        "from src.utils_perflow import merge_delta_weights_into_unet\n",
        "from src.scheduler_perflow import PeRFlowScheduler\n",
        "\n",
        "def setup_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "PROMPT_LIST_DEPTH = [[\"assets/others/control/depth/husky.jpg\", [\"A husky dog, lying on his stomach in the snow, looking away, blue sky\",\"A cute cat, lying on the ground, raining outside\"]],]\n",
        "PROMPT_LIST_TILE = [\n",
        "    [\"assets/others/control/dog.png\", [\"a dog sitting\",]],\n",
        "    [\"assets/others/control/fruits.png\", [\"a plate of fruits\",]],\n",
        "]\n",
        "\n",
        "def main(args):\n",
        "    save_dir = os.path.join(args.save_dir + f\"_{args.num_inference_steps}\")\n",
        "    Path(os.path.join(save_dir, \"images\")).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ## controlnet pipeline\n",
        "    from scripts.controlnet_preprocessor import Preprocessor\n",
        "    controlnet_preprocessor = Preprocessor(control_type=args.control_type, path=\"lllyasviel/Annotators\")\n",
        "    if args.control_type == \"openpose\":\n",
        "        controlnet_model_path = \"lllyasviel/control_v11p_sd15_openpose\"\n",
        "        controlnet_preprocessor.preprocessor.to(device='cuda')\n",
        "    elif args.control_type == \"midas\":\n",
        "        controlnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
        "        controlnet_preprocessor.preprocessor.to(device='cuda')\n",
        "    elif args.control_type == \"canny\":\n",
        "        controlnet_model_path = \"lllyasviel/control_v11p_sd15_canny\"\n",
        "    elif args.control_type == 'tile':\n",
        "        controlnet_model_path = \"lllyasviel/control_v11f1e_sd15_tile\"\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
        "    if args.control_type == 'tile':\n",
        "        pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(args.sd_base, controlnet=controlnet, torch_dtype=torch.float16,)\n",
        "    else:\n",
        "        pipe = StableDiffusionControlNetPipeline.from_pretrained(args.sd_base, controlnet = controlnet, torch_dtype=torch.float16,)\n",
        "\n",
        "    delta_weights = UNet2DConditionModel.from_pretrained(\"hansyan/perflow-sd15-delta-weights\", torch_dtype=torch.float16, variant=\"v0-1\",).state_dict()\n",
        "    pipe = merge_delta_weights_into_unet(pipe, delta_weights)\n",
        "    pipe.scheduler = PeRFlowScheduler.from_config(pipe.scheduler.config, prediction_type=\"epsilon\", num_time_windows=4,)\n",
        "\n",
        "    pipe.to(\"cuda\", torch.float16)\n",
        "\n",
        "\n",
        "    ## sampling\n",
        "    if args.control_type == \"midas\":\n",
        "        prompts_list = PROMPT_LIST_DEPTH\n",
        "    elif args.control_type == 'tile':\n",
        "        prompts_list = PROMPT_LIST_TILE\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    prompt_prefix = \"raw photo, 8k uhd, dslr, high quality, hyper detailed masterpiece; \"\n",
        "    negative_prompt = \"distorted, blur, smooth, low-quality, warm, haze, over-saturated, high-contrast\"\n",
        "\n",
        "    assert args.guidance_scale is not None\n",
        "    if \"-\" in args.guidance_scale:\n",
        "        cfg_scale_list = [float(x) for x in args.guidance_scale.split(\"-\")]\n",
        "    else:\n",
        "        cfg_scale_list = [float(args.guidance_scale)]\n",
        "\n",
        "    for cfg_scale in cfg_scale_list:\n",
        "        for i, prompt_ctrl in enumerate(prompts_list):\n",
        "            setup_seed(args.seed)\n",
        "            ctrl, prompts = prompt_ctrl[0], prompt_ctrl[1]\n",
        "            prompts = [prompt_prefix + p for p in prompts]\n",
        "\n",
        "            if args.control_type == 'tile':\n",
        "                ctrl = load_image(ctrl)\n",
        "                print(f\"low res input: {ctrl.size}, upsampling to ---> {args.size}\")\n",
        "                ctrl = controlnet_preprocessor(np.array(ctrl), image_resolution=args.size)\n",
        "                ctrl = Image.fromarray(ctrl)\n",
        "                samples = pipe(\n",
        "                            image               = ctrl,\n",
        "                            control_image       = ctrl,\n",
        "                            strength            = 1.0,\n",
        "                            prompt              = prompts,\n",
        "                            negative_prompt     = [negative_prompt] * len(prompts),\n",
        "                            height              = args.size,\n",
        "                            width               = args.size,\n",
        "                            num_inference_steps = args.num_inference_steps,\n",
        "                            guidance_scale      = cfg_scale,\n",
        "                            output_type         = 'pt',\n",
        "                            ).images\n",
        "            else:\n",
        "                ctrl = load_image(ctrl)\n",
        "                ctrl = controlnet_preprocessor(np.array(ctrl), image_resolution=args.size)\n",
        "                ctrl = Image.fromarray(ctrl)\n",
        "                samples = pipe(\n",
        "                    image               = ctrl,\n",
        "                    prompt              = prompts,\n",
        "                    negative_prompt     = [negative_prompt] * len(prompts),\n",
        "                    height              = args.size,\n",
        "                    width               = args.size,\n",
        "                    num_inference_steps = args.num_inference_steps,\n",
        "                    guidance_scale      = cfg_scale,\n",
        "                    output_type         = 'pt',\n",
        "                ).images\n",
        "\n",
        "            cfg_int = int(cfg_scale); cfg_float = int(cfg_scale*10 - cfg_int*10)\n",
        "            save_name = f'ctrl{i+1}_cfg{cfg_int}-{cfg_float}.png'\n",
        "            torchvision.utils.save_image(\n",
        "                torchvision.utils.make_grid(samples, nrow = 4),\n",
        "                os.path.join(save_dir, save_name)\n",
        "            )\n",
        "            ctrl.save(os.path.join(save_dir, f'ctrl{i+1}.png'))\n",
        "\n",
        "class Arguments:\n",
        "  save_dir: str = \"demo\"\n",
        "  control_type: str = \"tile\"\n",
        "  sd_base: str = \"Lykon/dreamshaper-8\"\n",
        "  size: int = 1024\n",
        "  num_inference_steps: int = 4\n",
        "  guidance_scale: str = \"4.5\"\n",
        "  seed: int = 42\n",
        "\n",
        "args = Arguments()\n",
        "main(args)"
      ],
      "metadata": {
        "id": "0ve2sPXL6JDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "display(Image.open(\"/content/piecewise-rectified-flow/demo_4/ctrl1_cfg4-5.png\"))"
      ],
      "metadata": {
        "id": "ei4EWAuS8XVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonder3d"
      ],
      "metadata": {
        "id": "_-tNb9CL8w0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers==0.19.3"
      ],
      "metadata": {
        "id": "xOhK3ZOe8yAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rembg segment_anything\n",
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "f0H7C1q99JAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xformers"
      ],
      "metadata": {
        "id": "H3aGtpc_-MNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fire streamlit"
      ],
      "metadata": {
        "id": "mEtrViY9A8Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q omegaconf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_KXbz6VB8QL",
        "outputId": "3775ed0e-efd9-45f6-f9c7-7611c0bf9dd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P /content/piecewise-rectified-flow/Wonder3D/sam_pt/"
      ],
      "metadata": {
        "id": "h3xORe5KC1Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/piecewise-rectified-flow/Wonder3D/\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import fire\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "from rembg import remove\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy\n",
        "import torch\n",
        "import rembg\n",
        "import threading\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from typing import Dict, Optional, Tuple, List\n",
        "from dataclasses import dataclass\n",
        "import streamlit as st\n",
        "import huggingface_hub\n",
        "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
        "from mvdiffusion.models.unet_mv2d_condition import UNetMV2DConditionModel\n",
        "from mvdiffusion.data.single_image_dataset import SingleImageDataset as MVDiffusionDataset\n",
        "from mvdiffusion.pipelines.pipeline_mvdiffusion_image import MVDiffusionImagePipeline\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "def save_image(tensor):\n",
        "    ndarr = tensor.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
        "    # pdb.set_trace()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    return ndarr\n",
        "\n",
        "\n",
        "def save_image_to_disk(tensor, fp):\n",
        "    ndarr = tensor.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
        "    # pdb.set_trace()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(fp)\n",
        "    return ndarr\n",
        "\n",
        "\n",
        "def save_image_numpy(ndarr, fp):\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(fp)\n",
        "\n",
        "\n",
        "weight_dtype = torch.float16\n",
        "\n",
        "_TITLE = '''Wonder3D: Single Image to 3D using Cross-Domain Diffusion'''\n",
        "_DESCRIPTION = '''\n",
        "<div>\n",
        "Generate consistent multi-view normals maps and color images.\n",
        "<a style=\"display:inline-block; margin-left: .5em\" href='https://github.com/xxlong0/Wonder3D/'><img src='https://img.shields.io/github/stars/xxlong0/Wonder3D?style=social' /></a>\n",
        "</div>\n",
        "<div>\n",
        "The demo does not include the mesh reconstruction part, please visit <a href=\"https://github.com/xxlong0/Wonder3D/\">our github repo</a> to get a textured mesh.\n",
        "</div>\n",
        "'''\n",
        "_GPU_ID = 0\n",
        "\n",
        "\n",
        "if not hasattr(Image, 'Resampling'):\n",
        "    Image.Resampling = Image\n",
        "\n",
        "\n",
        "def sam_init():\n",
        "    sam_checkpoint = os.path.join(\"/content/piecewise-rectified-flow/Wonder3D/\", \"sam_pt\", \"sam_vit_h_4b8939.pth\")\n",
        "    model_type = \"vit_h\"\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device=f\"cuda:{_GPU_ID}\")\n",
        "    predictor = SamPredictor(sam)\n",
        "    return predictor\n",
        "\n",
        "\n",
        "def sam_segment(predictor, input_image, *bbox_coords):\n",
        "    bbox = np.array(bbox_coords)\n",
        "    image = np.asarray(input_image)\n",
        "\n",
        "    start_time = time.time()\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    masks_bbox, scores_bbox, logits_bbox = predictor.predict(box=bbox, multimask_output=True)\n",
        "\n",
        "    print(f\"SAM Time: {time.time() - start_time:.3f}s\")\n",
        "    out_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
        "    out_image[:, :, :3] = image\n",
        "    out_image_bbox = out_image.copy()\n",
        "    out_image_bbox[:, :, 3] = masks_bbox[-1].astype(np.uint8) * 255\n",
        "    torch.cuda.empty_cache()\n",
        "    return Image.fromarray(out_image_bbox, mode='RGBA')\n",
        "\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width - height) // 2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, ((height - width) // 2, 0))\n",
        "        return result\n",
        "\n",
        "\n",
        "def preprocess(predictor, input_image, chk_group=None, segment=True, rescale=False):\n",
        "    RES = 1024\n",
        "    input_image.thumbnail([RES, RES], Image.Resampling.LANCZOS)\n",
        "    if chk_group is not None:\n",
        "        segment = \"Background Removal\" in chk_group\n",
        "        rescale = \"Rescale\" in chk_group\n",
        "    if segment:\n",
        "        image_rem = input_image.convert('RGBA')\n",
        "        image_nobg = remove(image_rem, alpha_matting=True)\n",
        "        arr = np.asarray(image_nobg)[:, :, -1]\n",
        "        x_nonzero = np.nonzero(arr.sum(axis=0))\n",
        "        y_nonzero = np.nonzero(arr.sum(axis=1))\n",
        "        x_min = int(x_nonzero[0].min())\n",
        "        y_min = int(y_nonzero[0].min())\n",
        "        x_max = int(x_nonzero[0].max())\n",
        "        y_max = int(y_nonzero[0].max())\n",
        "        input_image = sam_segment(predictor, input_image.convert('RGB'), x_min, y_min, x_max, y_max)\n",
        "    # Rescale and recenter\n",
        "    if rescale:\n",
        "        image_arr = np.array(input_image)\n",
        "        in_w, in_h = image_arr.shape[:2]\n",
        "        out_res = min(RES, max(in_w, in_h))\n",
        "        ret, mask = cv2.threshold(np.array(input_image.split()[-1]), 0, 255, cv2.THRESH_BINARY)\n",
        "        x, y, w, h = cv2.boundingRect(mask)\n",
        "        max_size = max(w, h)\n",
        "        ratio = 0.75\n",
        "        side_len = int(max_size / ratio)\n",
        "        padded_image = np.zeros((side_len, side_len, 4), dtype=np.uint8)\n",
        "        center = side_len // 2\n",
        "        padded_image[center - h // 2 : center - h // 2 + h, center - w // 2 : center - w // 2 + w] = image_arr[y : y + h, x : x + w]\n",
        "        rgba = Image.fromarray(padded_image).resize((out_res, out_res), Image.LANCZOS)\n",
        "\n",
        "        rgba_arr = np.array(rgba) / 255.0\n",
        "        rgb = rgba_arr[..., :3] * rgba_arr[..., -1:] + (1 - rgba_arr[..., -1:])\n",
        "        input_image = Image.fromarray((rgb * 255).astype(np.uint8))\n",
        "    else:\n",
        "        input_image = expand2square(input_image, (127, 127, 127, 0))\n",
        "    return input_image, input_image.resize((320, 320), Image.Resampling.LANCZOS)\n",
        "\n",
        "\n",
        "def load_wonder3d_pipeline(cfg):\n",
        "\n",
        "    pipeline = MVDiffusionImagePipeline.from_pretrained(\n",
        "    cfg.pretrained_model_name_or_path,\n",
        "    torch_dtype=weight_dtype\n",
        "    )\n",
        "\n",
        "    # pipeline.to('cuda:0')\n",
        "    pipeline.unet.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        pipeline.to('cuda:0')\n",
        "    # sys.main_lock = threading.Lock()\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "from mvdiffusion.data.single_image_dataset import SingleImageDataset\n",
        "\n",
        "\n",
        "def prepare_data(single_image, crop_size):\n",
        "    dataset = SingleImageDataset(root_dir='', num_views=6, img_wh=[256, 256], bg_color='white', crop_size=crop_size, single_image=single_image)\n",
        "    return dataset[0]\n",
        "\n",
        "scene = 'scene'\n",
        "\n",
        "def run_pipeline(pipeline, cfg, single_image, guidance_scale, steps, seed, crop_size, chk_group=None):\n",
        "    import pdb\n",
        "    global scene\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    if chk_group is not None:\n",
        "        write_image = \"Write Results\" in chk_group\n",
        "\n",
        "    batch = prepare_data(single_image, crop_size)\n",
        "\n",
        "    pipeline.set_progress_bar_config(disable=True)\n",
        "    seed = int(seed)\n",
        "    generator = torch.Generator(device=pipeline.unet.device).manual_seed(seed)\n",
        "\n",
        "    # repeat  (2B, Nv, 3, H, W)\n",
        "    imgs_in = torch.cat([batch['imgs_in']] * 2, dim=0).to(weight_dtype)\n",
        "\n",
        "    # (2B, Nv, Nce)\n",
        "    camera_embeddings = torch.cat([batch['camera_embeddings']] * 2, dim=0).to(weight_dtype)\n",
        "\n",
        "    task_embeddings = torch.cat([batch['normal_task_embeddings'], batch['color_task_embeddings']], dim=0).to(weight_dtype)\n",
        "\n",
        "    camera_embeddings = torch.cat([camera_embeddings, task_embeddings], dim=-1).to(weight_dtype)\n",
        "\n",
        "    # (B*Nv, 3, H, W)\n",
        "    imgs_in = rearrange(imgs_in, \"Nv C H W -> (Nv) C H W\")\n",
        "    # (B*Nv, Nce)\n",
        "    # camera_embeddings = rearrange(camera_embeddings, \"B Nv Nce -> (B Nv) Nce\")\n",
        "\n",
        "    out = pipeline(\n",
        "        imgs_in,\n",
        "        # camera_embeddings,\n",
        "        generator=generator,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=steps,\n",
        "        output_type='pt',\n",
        "        num_images_per_prompt=1,\n",
        "        **cfg.pipe_validation_kwargs,\n",
        "    ).images\n",
        "\n",
        "    bsz = out.shape[0] // 2\n",
        "    normals_pred = out[:bsz]\n",
        "    images_pred = out[bsz:]\n",
        "    num_views = 6\n",
        "    if write_image:\n",
        "        VIEWS = ['front', 'front_right', 'right', 'back', 'left', 'front_left']\n",
        "        cur_dir = os.path.join(\"./outputs\", f\"cropsize-{int(crop_size)}-cfg{guidance_scale:.1f}\")\n",
        "\n",
        "        scene = 'scene'+datetime.now().strftime('@%Y%m%d-%H%M%S')\n",
        "        scene_dir = os.path.join(cur_dir, scene)\n",
        "        normal_dir = os.path.join(scene_dir, \"normals\")\n",
        "        masked_colors_dir = os.path.join(scene_dir, \"masked_colors\")\n",
        "        os.makedirs(normal_dir, exist_ok=True)\n",
        "        os.makedirs(masked_colors_dir, exist_ok=True)\n",
        "        for j in range(num_views):\n",
        "            view = VIEWS[j]\n",
        "            normal = normals_pred[j]\n",
        "            color = images_pred[j]\n",
        "\n",
        "            normal_filename = f\"normals_000_{view}.png\"\n",
        "            rgb_filename = f\"rgb_000_{view}.png\"\n",
        "            normal = save_image_to_disk(normal, os.path.join(normal_dir, normal_filename))\n",
        "            color = save_image_to_disk(color, os.path.join(scene_dir, rgb_filename))\n",
        "\n",
        "            # rm_normal = remove(normal)\n",
        "            # rm_color = remove(color)\n",
        "\n",
        "            # save_image_numpy(rm_normal, os.path.join(scene_dir, normal_filename))\n",
        "            # save_image_numpy(rm_color, os.path.join(masked_colors_dir, rgb_filename))\n",
        "\n",
        "    normals_pred = [save_image(normals_pred[i]) for i in range(bsz)]\n",
        "    images_pred = [save_image(images_pred[i]) for i in range(bsz)]\n",
        "\n",
        "    out = images_pred + normals_pred\n",
        "    return out\n",
        "\n",
        "\n",
        "def process_3d(mode, data_dir, guidance_scale, crop_size):\n",
        "    dir = None\n",
        "    global scene\n",
        "\n",
        "    cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    subprocess.run(\n",
        "        f'cd instant-nsr-pl && python launch.py --config configs/neuralangelo-ortho-wmask.yaml --gpu 0 --train dataset.root_dir=../{data_dir}/cropsize-{crop_size:.1f}-cfg{guidance_scale:.1f}/ dataset.scene={scene} && cd ..',\n",
        "        shell=True,\n",
        "    )\n",
        "    import glob\n",
        "    # import pdb\n",
        "\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    obj_files = glob.glob(f'{cur_dir}/instant-nsr-pl/exp/{scene}/*/save/*.obj', recursive=True)\n",
        "    print(obj_files)\n",
        "    if obj_files:\n",
        "        dir = obj_files[0]\n",
        "    return dir\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TestConfig:\n",
        "    pretrained_model_name_or_path: str\n",
        "    pretrained_unet_path: str\n",
        "    revision: Optional[str]\n",
        "    validation_dataset: Dict\n",
        "    save_dir: str\n",
        "    seed: Optional[int]\n",
        "    validation_batch_size: int\n",
        "    dataloader_num_workers: int\n",
        "\n",
        "    local_rank: int\n",
        "\n",
        "    pipe_kwargs: Dict\n",
        "    pipe_validation_kwargs: Dict\n",
        "    unet_from_pretrained_kwargs: Dict\n",
        "    validation_guidance_scales: List[float]\n",
        "    validation_grid_nrow: int\n",
        "    camera_embedding_lr_mult: float\n",
        "\n",
        "    num_views: int\n",
        "    camera_embedding_type: str\n",
        "\n",
        "    pred_type: str  # joint, or ablation\n",
        "\n",
        "    enable_xformers_memory_efficient_attention: bool\n",
        "\n",
        "    cond_on_normals: bool\n",
        "    cond_on_colors: bool\n",
        "\n",
        "\n",
        "def run_demo():\n",
        "    from utils.misc import load_config\n",
        "    from omegaconf import OmegaConf\n",
        "\n",
        "    # parse YAML config to OmegaConf\n",
        "    cfg = load_config(\"./configs/mvdiffusion-joint-ortho-6views.yaml\")\n",
        "    # print(cfg)\n",
        "    schema = OmegaConf.structured(TestConfig)\n",
        "    cfg = OmegaConf.merge(schema, cfg)\n",
        "\n",
        "    pipeline = load_wonder3d_pipeline(cfg)\n",
        "    torch.set_grad_enabled(False)\n",
        "    pipeline.to(f'cuda:{_GPU_ID}')\n",
        "\n",
        "    predictor = sam_init()\n",
        "\n",
        "    custom_theme = gr.themes.Soft(primary_hue=\"blue\").set(\n",
        "        button_secondary_background_fill=\"*neutral_100\", button_secondary_background_fill_hover=\"*neutral_200\"\n",
        "    )\n",
        "    custom_css = '''#disp_image {\n",
        "        text-align: center; /* Horizontally center the content */\n",
        "    }'''\n",
        "\n",
        "    with gr.Blocks(title=_TITLE, theme=custom_theme, css=custom_css) as demo:\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown('# ' + _TITLE)\n",
        "        gr.Markdown(_DESCRIPTION)\n",
        "        with gr.Row(variant='panel'):\n",
        "            with gr.Column(scale=1):\n",
        "                input_image = gr.Image(type='pil', image_mode='RGBA', height=320, label='Input image')\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                processed_image = gr.Image(\n",
        "                    type='pil',\n",
        "                    label=\"Processed Image\",\n",
        "                    interactive=False,\n",
        "                    height=320,\n",
        "                    image_mode='RGBA',\n",
        "                    elem_id=\"disp_image\",\n",
        "                    visible=True,\n",
        "                )\n",
        "            # with gr.Column(scale=1):\n",
        "            #     ## add 3D Model\n",
        "            #     obj_3d = gr.Model3D(\n",
        "            #                         # clear_color=[0.0, 0.0, 0.0, 0.0],\n",
        "            #                         label=\"3D Model\", height=320,\n",
        "            #                         # camera_position=[0,0,2.0]\n",
        "            #                         )\n",
        "                processed_image_highres = gr.Image(type='pil', image_mode='RGBA', visible=False)\n",
        "        with gr.Row(variant='panel'):\n",
        "            with gr.Column(scale=1):\n",
        "                example_folder = \"./example_images\"\n",
        "                example_fns = [os.path.join(example_folder, example) for example in os.listdir(example_folder)]\n",
        "                gr.Examples(\n",
        "                    examples=example_fns,\n",
        "                    inputs=[input_image],\n",
        "                    outputs=[input_image],\n",
        "                    cache_examples=False,\n",
        "                    label='Examples (click one of the images below to start)',\n",
        "                    examples_per_page=30,\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Accordion('Advanced options', open=True):\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            input_processing = gr.CheckboxGroup(\n",
        "                                ['Background Removal'],\n",
        "                                label='Input Image Preprocessing',\n",
        "                                value=['Background Removal'],\n",
        "                                info='untick this, if masked image with alpha channel',\n",
        "                            )\n",
        "                        with gr.Column():\n",
        "                            output_processing = gr.CheckboxGroup(\n",
        "                                ['Write Results'], label='write the results in ./outputs folder', value=['Write Results']\n",
        "                            )\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            scale_slider = gr.Slider(1, 5, value=1, step=1, label='Classifier Free Guidance Scale')\n",
        "                        with gr.Column():\n",
        "                            steps_slider = gr.Slider(15, 100, value=50, step=1, label='Number of Diffusion Inference Steps')\n",
        "                    with gr.Row():\n",
        "                        with gr.Column():\n",
        "                            seed = gr.Number(42, label='Seed')\n",
        "                        with gr.Column():\n",
        "                            crop_size = gr.Number(192, label='Crop size')\n",
        "\n",
        "                        mode = gr.Textbox('train', visible=False)\n",
        "                        data_dir = gr.Textbox('outputs', visible=False)\n",
        "                    # crop_size = 192\n",
        "                    # with gr.Row():\n",
        "                    #     method = gr.Radio(choices=['instant-nsr-pl', 'NeuS'], label='Method (Default: instant-nsr-pl)', value='instant-nsr-pl')\n",
        "                run_btn = gr.Button('Generate Normals and Colors', variant='primary', interactive=True)\n",
        "                # recon_btn = gr.Button('Reconstruct 3D model', variant='primary', interactive=True)\n",
        "                # gr.Markdown(\"<span style='color:red'>First click Generate button, then click Reconstruct button. Reconstruction may cost several minutes.</span>\")\n",
        "\n",
        "        with gr.Row():\n",
        "            view_1 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            view_2 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            view_3 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            view_4 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            view_5 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            view_6 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "        with gr.Row():\n",
        "            normal_1 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            normal_2 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            normal_3 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            normal_4 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            normal_5 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "            normal_6 = gr.Image(interactive=False, height=240, show_label=False)\n",
        "\n",
        "        run_btn.click(\n",
        "            fn=partial(preprocess, predictor), inputs=[input_image, input_processing], outputs=[processed_image_highres, processed_image], queue=True\n",
        "        ).success(\n",
        "            fn=partial(run_pipeline, pipeline, cfg),\n",
        "            inputs=[processed_image_highres, scale_slider, steps_slider, seed, crop_size, output_processing],\n",
        "            outputs=[view_1, view_2, view_3, view_4, view_5, view_6, normal_1, normal_2, normal_3, normal_4, normal_5, normal_6],\n",
        "        )\n",
        "        # recon_btn.click(\n",
        "        #     process_3d, inputs=[mode, data_dir, scale_slider, crop_size], outputs=[obj_3d]\n",
        "        # )\n",
        "\n",
        "        demo.queue().launch(share=True, max_threads=80)\n",
        "\n",
        "\n",
        "# import fire\n",
        "# fire.Fire(run_demo)\n",
        "\n",
        "run_demo()\n"
      ],
      "metadata": {
        "id": "Wtg_Sb6g9CLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}