{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8/TBBoitsOJ2w0wUyrXBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/sports/soccerXAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/yt-dlp/yt-dlp/releases/download/2023.03.04/yt-dlp\n",
        "!chmod +x yt-dlp"
      ],
      "metadata": {
        "id": "rGKOJRjws8tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./yt-dlp https://www.youtube.com/watch?v=mqdt2FEE22o -o test.mp4 -f bestvideo[ext=mp4]+bestaudio[ext=m4a] -S vcodec:h264"
      ],
      "metadata": {
        "id": "fLZaW6EwsibI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "cap = cv2.VideoCapture(\"test.mp4\")\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 2200)\n",
        "ret, frame = cap.read()\n",
        "cv2.imwrite(\"test.jpg\", frame)\n",
        "display(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))"
      ],
      "metadata": {
        "id": "Y9aQV5mStTcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groudig DINO installation"
      ],
      "metadata": {
        "id": "0my3M7RXu5fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd /content/GroundingDINO\n",
        "!pip install -q -e .\n",
        "!pip install -q roboflow"
      ],
      "metadata": {
        "id": "tVRA3llGuyqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "0BwERrZcvfkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from functools import partial\n",
        "import cv2\n",
        "import requests\n",
        "\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "from groundingdino.models import build_model\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.util.utils import clean_state_dict\n",
        "from groundingdino.util.inference import annotate, load_image, predict\n",
        "import groundingdino.datasets.transforms as T\n",
        "\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "qRwH6uWxvg-0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils function\n",
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file) \n",
        "    model = build_model(args)\n",
        "    args.device = device\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    return model  "
      ],
      "metadata": {
        "id": "zDguH0tnvkx9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load detection model\n",
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filenmae = \"groundingdino_swint_ogc.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinT_OGC.cfg.py\"\n",
        "dino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGvx6XTOvmTX",
        "outputId": "39fe1bcb-008e-4588-abd9-03ad84897220"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swint_ogc.pth \n",
            " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "CBECWAxcy1x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "local_image_path = \"/content/test.jpg\"\n",
        "\n",
        "TEXT_PROMPT = \"player\"\n",
        "BOX_TRESHOLD = 0.2\n",
        "TEXT_TRESHOLD = 0.2\n",
        "\n",
        "image_source, image = load_image(local_image_path)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=dino_model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "i66ASTBBvyaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "imgh, imgw = image_source.shape[:2]\n",
        "\n",
        "player_dir = \"/content/playerimages\"\n",
        "os.makedirs(player_dir, exist_ok=True)\n",
        "\n",
        "for idx, bbox in enumerate(boxes.numpy()):\n",
        "  xc, yc, w, h = bbox\n",
        "  xmin = int((xc - w * 0.5) * imgw)\n",
        "  ymin = int((yc - h * 0.5) * imgh)\n",
        "  xmax = int((xc + w * 0.5) * imgw)\n",
        "  ymax = int((yc + h * 0.5) * imgh)\n",
        "  crop_img = image_source[ymin:ymax, xmin:xmax, :]\n",
        "  cv2.imwrite(player_dir + \"/{}.jpg\".format(str(idx).zfill(3)), cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n"
      ],
      "metadata": {
        "id": "Z3smQl4Wgihr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Player classification"
      ],
      "metadata": {
        "id": "59pc5sfGeAP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation transformers"
      ],
      "metadata": {
        "id": "MDIZQ1geeHqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "7oDSEn3neEiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions"
      ],
      "metadata": {
        "id": "eu0KYkFieJRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  \n",
        "\n",
        "def show_boxes_on_image(raw_image, boxes):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(raw_image)\n",
        "    for box in boxes:\n",
        "      show_box(box, plt.gca())\n",
        "    plt.axis('on')\n",
        "    plt.show()\n",
        "\n",
        "def show_points_on_image(raw_image, input_points, input_labels=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(raw_image)\n",
        "    input_points = np.array(input_points)\n",
        "    if input_labels is None:\n",
        "      labels = np.ones_like(input_points[:, 0])\n",
        "    else:\n",
        "      labels = np.array(input_labels)\n",
        "    show_points(input_points, labels, plt.gca())\n",
        "    plt.axis('on')\n",
        "    plt.show()\n",
        "\n",
        "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(raw_image)\n",
        "    input_points = np.array(input_points)\n",
        "    if input_labels is None:\n",
        "      labels = np.ones_like(input_points[:, 0])\n",
        "    else:\n",
        "      labels = np.array(input_labels)\n",
        "    show_points(input_points, labels, plt.gca())\n",
        "    for box in boxes:\n",
        "      show_box(box, plt.gca())\n",
        "    plt.axis('on')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(raw_image)\n",
        "    input_points = np.array(input_points)\n",
        "    if input_labels is None:\n",
        "      labels = np.ones_like(input_points[:, 0])\n",
        "    else:\n",
        "      labels = np.array(input_labels)\n",
        "    show_points(input_points, labels, plt.gca())\n",
        "    for box in boxes:\n",
        "      show_box(box, plt.gca())\n",
        "    plt.axis('on')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "\n",
        "def show_masks_on_image(raw_image, masks, scores):\n",
        "    if len(masks.shape) == 4:\n",
        "      masks = masks.squeeze()\n",
        "    if scores.shape[0] == 1:\n",
        "      scores = scores.squeeze()\n",
        "\n",
        "    nb_predictions = scores.shape[-1]\n",
        "    fig, axes = plt.subplots(1, nb_predictions, figsize=(15, 15))\n",
        "\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "      mask = mask.cpu().detach()\n",
        "      axes[i].imshow(np.array(raw_image))\n",
        "      show_mask(mask, axes[i])\n",
        "      axes[i].title.set_text(f\"Mask {i+1}, Score: {score.item():.3f}\")\n",
        "      axes[i].axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "r4-hITMZeKaR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segment Anything"
      ],
      "metadata": {
        "id": "3KH9M2rq5m5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import SamModel, SamProcessor"
      ],
      "metadata": {
        "id": "uBaDSFlr5pg0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "TXqydbyn6JJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
      ],
      "metadata": {
        "id": "GDMy3Nk25yen"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input player image\n",
        "img_path = \"/content/playerimages/011.jpg\"\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "display(img)"
      ],
      "metadata": {
        "id": "uclfzm0K6W9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, h = img.size\n",
        "input_points = [[[int(w * 0.5), int(h * 0.5)]]]  # 2D location of a window in the image\n",
        "\n",
        "inputs = processor(img, input_points=input_points, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "masks = processor.image_processor.post_process_masks(\n",
        "    outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n",
        ")\n",
        "scores = outputs.iou_scores"
      ],
      "metadata": {
        "id": "hReXtoX86gl9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks_on_image(img, masks[0], scores)"
      ],
      "metadata": {
        "id": "7pqK2A-X6v3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_list = scores.cpu().numpy()[0][0]\n",
        "for idx in range(len(scores_list)):\n",
        "  sum = np.sum(masks[0].cpu().numpy().squeeze(0)[idx])\n",
        "  prod = np.prod(masks[0].cpu().numpy().squeeze(0)[idx].shape)\n",
        "  if sum > prod * 0.8:\n",
        "    scores_list[idx] = 0\n",
        "  if sum < prod * 0.05:\n",
        "    scores_list[idx] = 0\n",
        "\n",
        "print(scores_list)"
      ],
      "metadata": {
        "id": "Hk8x07TL7Yuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13eca4f-b16e-48b7-b561-df2fff68eb0b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.93344164 0.94545054 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "midx = np.argmax(scores_list)\n",
        "pred_mask = masks[0].cpu().numpy().squeeze(0)[midx]\n",
        "cv2.imwrite(\"mask.jpg\", pred_mask.astype(np.uint8) * 255)\n",
        "rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "mask = cv2.imread(\"mask.jpg\")\n",
        "rgb_and = cv2.bitwise_and(rgb, mask)\n",
        "rh, rw = rgb_and.shape[:2]\n",
        "# center crop\n",
        "center_rgb_and = rgb_and[int(0.1 * rh):int(0.5 * rh), int(0.3 * rw):int(0.7 * rw), :]\n",
        "from PIL import Image\n",
        "Image.fromarray(center_rgb_and)"
      ],
      "metadata": {
        "id": "QLDrup3I7jnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hsv = cv2.cvtColor(center_rgb_and, cv2.COLOR_RGB2HSV)\n",
        "red = cv2.inRange(hsv, np.array([145, 70, 0]), np.array([180, 255, 255]))\n",
        "yellow = cv2.inRange(hsv, np.array([10, 80, 0]), np.array([50, 255, 255]))\n",
        "blue = cv2.inRange(hsv, np.array([108, 121, 0]), np.array([120, 255, 255]))"
      ],
      "metadata": {
        "id": "Yl5Wjx2Z7zDq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_imgs = {'red': red, 'yellow': yellow, 'blue': blue}\n",
        "\n",
        "# 2値化結果を可視化する。\n",
        "fig, axes_list = plt.subplots(3, 1, figsize=(10, 18))\n",
        "\n",
        "player_color = \"\"\n",
        "max_sum = 0\n",
        "for ax, (label, bin_img) in zip(axes_list.ravel(), bin_imgs.items()):\n",
        "    ax.axis('off')\n",
        "    ax.set_title(label)\n",
        "    ax.imshow(bin_img, cmap=plt.cm.gray)\n",
        "    sum = np.sum(bin_img)\n",
        "    if sum > max_sum:\n",
        "      player_color = label\n",
        "      max_sum = sum\n",
        "    \n",
        "print(player_color)\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-t1MJQq7YL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips"
      ],
      "metadata": {
        "id": "itHKIvhF5jUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "NSmgmX1bePAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# player classification\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"mask-generation\", model=\"facebook/sam-vit-huge\", device=0)"
      ],
      "metadata": {
        "id": "rM5g4Dxkd1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img_path = \"/content/playerimages/000.jpg\"\n",
        "img = Image.open(img_path).convert(\"RGB\")"
      ],
      "metadata": {
        "id": "mOaGsa9Ieljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "Vvv9ZhckeZIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generator(img, points_per_batch=64)"
      ],
      "metadata": {
        "id": "gDRd-6_PeOrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display image\n",
        "masks = outputs[\"masks\"]\n",
        "show_masks_on_image(img, masks)"
      ],
      "metadata": {
        "id": "qLr-yj-_eeIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(masks[1].astype(np.uint8))"
      ],
      "metadata": {
        "id": "0v7ytkCumk5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "cv2_img = cv2.imread(img_path)\n",
        "h, w = cv2_img.shape[:2]\n",
        "base_img = np.zeros((h, w))\n",
        "for mask in masks:\n",
        "  h, w = mask.shape[-2:]\n",
        "  mask_image = mask.reshape(h, w).astype(np.uint8)\n",
        "  base_img += mask_image\n",
        "print(base_img)"
      ],
      "metadata": {
        "id": "53E4S27tkbpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動画切り出し"
      ],
      "metadata": {
        "id": "5QWJU9QTd0a2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4KhSmytse0x"
      },
      "outputs": [],
      "source": [
        "# ２分から10秒だけ切り出す。\n",
        "!ffmpeg -ss 00:02:00 -i test.mp4 -ss 0 -t 10 -c:v copy -c:a copy -async 1 -strict -2 output.mp4"
      ]
    }
  ]
}