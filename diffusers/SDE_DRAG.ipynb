{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9oB5+be5b4O5fdLf1KLw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/diffusers/SDE_DRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z13RIL1hLnJg"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/diffusers.git"
      ],
      "metadata": {
        "id": "6C1X4R4WMPVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://static2.flymee.jp/product_images/c74a-110961/202206091032086835.jpg -O /content/input.jpg"
      ],
      "metadata": {
        "id": "bQ7y3jUpMMXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import torch\n",
        "from diffusers import DDIMScheduler, DiffusionPipeline\n",
        "\n",
        "# Load the pipeline\n",
        "model_path = \"SG161222/Realistic_Vision_V6.0_B1_noVAE\"\n",
        "scheduler = DDIMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
        "pipe = DiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, custom_pipeline=\"sde_drag\", torch_dtype=torch.float16)\n",
        "pipe.to('cuda')\n",
        "\n",
        "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
        "# If not training LoRA, please avoid using torch.float16\n",
        "# pipe.to(torch.float16)"
      ],
      "metadata": {
        "id": "80H2lTlxLt5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForUniversalSegmentation\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForUniversalSegmentation.from_pretrained(model_id).to(\"cuda\")"
      ],
      "metadata": {
        "id": "RhI6RHvWM4sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bounding_box(mask_image):\n",
        "    # 二値化処理\n",
        "    _, binary_mask = cv2.threshold(mask_image, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # 輪郭を検出\n",
        "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        return None  # 輪郭が見つからない場合\n",
        "\n",
        "    # 最大の輪郭を取得\n",
        "    max_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # 輪郭を囲む矩形を計算\n",
        "    x, y, w, h = cv2.boundingRect(max_contour)\n",
        "\n",
        "    return x, y, x + w, y + h  # 左上の座標と右下の座標を返す"
      ],
      "metadata": {
        "id": "6MPDMb-mAs9Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "img_path = '/content/input.jpg'\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
        "    outputs, target_sizes=[image.size[::-1]]\n",
        ")[0]\n",
        "\n",
        "predicted_semantic_map = predicted_semantic_map.detach().cpu().numpy()\n",
        "target_label_id = model.config.label2id[\"chair\"]\n",
        "target_label_map = np.where(predicted_semantic_map == target_label_id, 255, 0).astype(np.uint8)\n",
        "display(Image.fromarray(target_label_map))"
      ],
      "metadata": {
        "id": "HP-aNQ1_NobG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xmin, ymin, xmax, ymax = calculate_bounding_box(target_label_map)\n",
        "display(image.crop((xmin, ymin, xmax, ymax)))"
      ],
      "metadata": {
        "id": "TvNLf84NA0Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def draw_point(image, point, color=(0, 255, 0), radius=5, thickness=-1):\n",
        "    \"\"\"\n",
        "    画像に点を描画する関数\n",
        "\n",
        "    Parameters:\n",
        "    - image: 描画対象の画像\n",
        "    - point: 描画する点の座標 (x, y)\n",
        "    - color: 点の色 (B, G, R)\n",
        "    - radius: 点の半径\n",
        "    - thickness: 点の輪郭の太さ（負の値の場合、塗りつぶし）\n",
        "    \"\"\"\n",
        "    cv2.circle(image, point, radius, color, thickness)\n",
        "\n",
        "\n",
        "base = cv2.imread('/content/input.jpg')\n",
        "draw_point(base, (570, 600))\n",
        "\n",
        "display(Image.fromarray(cv2.cvtColor(base, cv2.COLOR_BGR2RGB)))\n"
      ],
      "metadata": {
        "id": "uQcfosuWZe1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide prompt, image, mask image, and the starting and target points for drag editing.\n",
        "prompt = \"chair\"\n",
        "image = Image.open('/content/input.jpg')\n",
        "mask_image = np.zeros_like(target_label_map)\n",
        "mask_image[ymin:ymax, xmin:xmax] = 255\n",
        "mask_image = Image.fromarray(mask_image)\n",
        "source_points = [[400, 400], [340, 600], [570, 600]]\n",
        "target_points = [[250, 400], [300, 600], [530, 600]]\n",
        "\n",
        "# train_lora is optional, and in most cases, using train_lora can better preserve consistency with the original image.\n",
        "# pipe.train_lora(prompt, image)\n",
        "\n",
        "output = pipe(prompt, image, mask_image, source_points, target_points)\n",
        "output_image = Image.fromarray(output)\n",
        "display(output_image)"
      ],
      "metadata": {
        "id": "QimHesiqL6cl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}