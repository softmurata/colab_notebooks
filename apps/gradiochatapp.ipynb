{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBLDSERmN4LVIi5zZZ0yta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/colab_notebooks/blob/main/apps/gradiochatapp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxHlBGC9mcvM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors"
      ],
      "metadata": {
        "id": "A7tx0vSOqYQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title chat\n",
        "# google colab„Åß„ÅØÂ§ßË¶èÊ®°„É¢„Éá„É´„ÅåÂãï„Åã„Åõ„Åö„ÇØ„É©„ÉÉ„Ç∑„É•„Åó„Å¶„Åó„Åæ„ÅÜ„ÄÇ"
      ],
      "metadata": {
        "id": "crPPpA5InGs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "6Ir1l1xHmpMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
        "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "question = \"how many dogs are in the picture?\"\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "-e3Zouyemwn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title image captioning\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import torch"
      ],
      "metadata": {
        "id": "2q0pfgfInICA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "eFqOhlYPoadn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model.generate(**inputs)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "hgWg9AIkocSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kD8Px6hts0nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "RQ_Xsv26s1Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "\n",
        "import string\n",
        "import gradio as gr\n",
        "import requests"
      ],
      "metadata": {
        "id": "GMZLnBMDtCSi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"\"\"<h1 align=\"center\">chatZumen</h1>\"\"\"\n",
        "description = \"\"\"Gradio demo for chatZumen, image-to-text generation from soccer R&D. To use it, simply upload your image, or click one of the examples to load them.\n",
        "<br> <strong>Disclaimer</strong>: This is a research prototype and is not intended for production use. No data including but not restricted to text and images is collected.\"\"\"\n",
        "article = \"\"\"<strong>Paper</strong>: <a href='https://arxiv.org/abs/2301.12597' target='_blank'>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>\n",
        "<br> <strong>Code</strong>: BLIP2 is now integrated into GitHub repo: <a href='https://github.com/salesforce/LAVIS' target='_blank'>LAVIS: a One-stop Library for Language and Vision</a>\n",
        "<br> <strong>ü§ó `transformers` integration</strong>: You can now use `transformers` to use our BLIP-2 models! Check out the <a href='https://huggingface.co/docs/transformers/main/en/model_doc/blip-2' target='_blank'> official docs </a>\n",
        "<p> <strong>Project Page</strong>: <a href='https://github.com/salesforce/LAVIS/tree/main/projects/blip2' target='_blank'> BLIP2 on LAVIS</a>\n",
        "<br> <strong>Description</strong>: Captioning results from <strong>BLIP2_OPT_6.7B</strong>. Chat results from <strong>BLIP2_FlanT5xxl</strong>.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4Wec3aZ6tGw0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_output(output):\n",
        "    # if last character is not a punctuation, add a full stop\n",
        "    if not output[0][-1] in string.punctuation:\n",
        "        output[0] += \".\"\n",
        "\n",
        "    return output\n",
        "\n",
        "def inference_chat(\n",
        "    image,\n",
        "    text_input,\n",
        "    decoding_method,\n",
        "    temperature,\n",
        "    length_penalty,\n",
        "    repetition_penalty,\n",
        "    history=[],\n",
        "):\n",
        "    text_input = text_input\n",
        "    history.append(text_input)\n",
        "\n",
        "    prompt = \" \".join(history)\n",
        "    \n",
        "    # ToDo: GPT_index\n",
        "    # output = query_chat_api(\n",
        "    #    image, prompt, decoding_method, temperature, length_penalty, repetition_penalty\n",
        "    #)\n",
        "    # output = postprocess_output(output)\n",
        "    output = [\"hello world\"]\n",
        "    history += output\n",
        "\n",
        "    chat = [\n",
        "        (history[i], history[i + 1]) for i in range(0, len(history) - 1, 2)\n",
        "    ]  # convert to tuples of list\n",
        "\n",
        "    return {chatbot: chat, state: history}\n",
        "\n",
        "def inference_caption(\n",
        "    image,\n",
        "    decoding_method,\n",
        "    temperature,\n",
        "    length_penalty,\n",
        "    repetition_penalty,\n",
        "):\n",
        "    # ToDo: yolov8 detection\n",
        "    # output = query_caption_api(\n",
        "    #    image, decoding_method, temperature, length_penalty, repetition_penalty\n",
        "    # )\n",
        "    # return output[0]\n",
        "    return \"great cat\""
      ],
      "metadata": {
        "id": "IhGHx8ZStbKT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(\n",
        "    css=\"\"\"\n",
        "    .message.svelte-w6rprc.svelte-w6rprc.svelte-w6rprc {font-size: 20px; margin-top: 20px}\n",
        "    #component-21 > div.wrap.svelte-w6rprc {height: 600px;}\n",
        "    \"\"\"\n",
        ") as iface:\n",
        "    state = gr.State([])\n",
        "\n",
        "    gr.Markdown(title)\n",
        "    gr.Markdown(description)\n",
        "    gr.Markdown(article)\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(type=\"pil\")\n",
        "\n",
        "            # with gr.Row():\n",
        "            sampling = gr.Radio(\n",
        "                choices=[\"Beam search\", \"Nucleus sampling\"],\n",
        "                value=\"Beam search\",\n",
        "                label=\"Text Decoding Method\",\n",
        "                interactive=True,\n",
        "            )\n",
        "\n",
        "            temperature = gr.Slider(\n",
        "                minimum=0.5,\n",
        "                maximum=1.0,\n",
        "                value=1.0,\n",
        "                step=0.1,\n",
        "                interactive=True,\n",
        "                label=\"Temperature (used with nucleus sampling)\",\n",
        "            )\n",
        "\n",
        "            len_penalty = gr.Slider(\n",
        "                minimum=-1.0,\n",
        "                maximum=2.0,\n",
        "                value=1.0,\n",
        "                step=0.2,\n",
        "                interactive=True,\n",
        "                label=\"Length Penalty (set to larger for longer sequence, used with beam search)\",\n",
        "            )\n",
        "\n",
        "            rep_penalty = gr.Slider(\n",
        "                minimum=1.0,\n",
        "                maximum=5.0,\n",
        "                value=1.5,\n",
        "                step=0.5,\n",
        "                interactive=True,\n",
        "                label=\"Repeat Penalty (larger value prevents repetition)\",\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1.8):\n",
        "\n",
        "            with gr.Column():\n",
        "                caption_output = gr.Textbox(lines=1, label=\"Caption Output\")\n",
        "                caption_button = gr.Button(\n",
        "                    value=\"Caption it!\", interactive=True, variant=\"primary\"\n",
        "                )\n",
        "                caption_button.click(\n",
        "                    inference_caption,\n",
        "                    [\n",
        "                        image_input,\n",
        "                        sampling,\n",
        "                        temperature,\n",
        "                        len_penalty,\n",
        "                        rep_penalty,\n",
        "                    ],\n",
        "                    [caption_output],\n",
        "                )\n",
        "\n",
        "            gr.Markdown(\"\"\"Trying prompting your input for chat; e.g. example prompt for QA, \\\"Question: {} Answer:\\\" Use proper punctuation (e.g., question mark).\"\"\")\n",
        "            with gr.Row():\n",
        "                with gr.Column(\n",
        "                    scale=1.5, \n",
        "                ):\n",
        "                    chatbot = gr.Chatbot(\n",
        "                        label=\"Chat Output (from FlanT5)\",\n",
        "                    )\n",
        "\n",
        "                # with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    chat_input = gr.Textbox(lines=1, label=\"Chat Input\")\n",
        "                    chat_input.submit(\n",
        "                        inference_chat,\n",
        "                        [\n",
        "                            image_input,\n",
        "                            chat_input,\n",
        "                            sampling,\n",
        "                            temperature,\n",
        "                            len_penalty,\n",
        "                            rep_penalty,\n",
        "                            state,\n",
        "                        ],\n",
        "                        [chatbot, state],\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        clear_button = gr.Button(value=\"Clear\", interactive=True)\n",
        "                        clear_button.click(\n",
        "                            lambda: (\"\", [], []),\n",
        "                            [],\n",
        "                            [chat_input, chatbot, state],\n",
        "                            queue=False,\n",
        "                        )\n",
        "\n",
        "                        submit_button = gr.Button(\n",
        "                            value=\"Submit\", interactive=True, variant=\"primary\"\n",
        "                        )\n",
        "                        submit_button.click(\n",
        "                            inference_chat,\n",
        "                            [\n",
        "                                image_input,\n",
        "                                chat_input,\n",
        "                                sampling,\n",
        "                                temperature,\n",
        "                                len_penalty,\n",
        "                                rep_penalty,\n",
        "                                state,\n",
        "                            ],\n",
        "                            [chatbot, state],\n",
        "                        )\n",
        "\n",
        "            image_input.change(\n",
        "                lambda: (\"\", \"\", []),\n",
        "                [],\n",
        "                [chatbot, caption_output, state],\n",
        "                queue=False,\n",
        "            )\n",
        "\n",
        "iface.queue(concurrency_count=1, api_open=False, max_size=10)\n",
        "iface.launch(enable_queue=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "37SByPKitLAq",
        "outputId": "db196ef9-434d-4133-9dbe-77b3b1a25b5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1121abc5a93b34a137.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1121abc5a93b34a137.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}